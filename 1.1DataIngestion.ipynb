{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5733a272",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langchain_community.document_loaders.text.TextLoader at 0x1d22fd7dab0>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_community.document_loaders import TextLoader\n",
    "loader=TextLoader('speech.txt')\n",
    "loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4e8d937d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'speech.txt'}, page_content='hi my name is shafqat and i am from india\\ni am a python developer\\ni am a data scientist\\ni am a machine learning expert\\ni am a deep learning expert\\ni am a natural language processing expert\\ni am a computer vision expert\\ni am a computer graphics expert\\ni am a computer audio expert\\ni am a computer vision expert')]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "df30abe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "loader=PyPDFLoader('pdf_file.pdf')\n",
    "docs=loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "aa94fd66",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langchain_community.document_loaders.pdf.PyPDFLoader at 0x1d23111c8e0>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cf1f2399",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'producer': 'Nitro Pro 13 (13.42.1.855)', 'creator': 'Nitro Pro 13 (13.42.1.855)', 'creationdate': '2023-09-22T23:58:25+05:30', 'moddate': '2023-09-22T23:58:25+05:30', 'author': 'Sarim Moin', 'title': 'Basic Details of the Team and Problem Statement', 'source': 'pdf_file.pdf', 'total_pages': 5, 'page': 0, 'page_label': '1'}, page_content='Basic Details of the Team and \\nProblem Statement \\nMinistry/Organization Name/Student Innovation: \\nPS Code:\\n626126 \\nProblem Statement Title: Chicken disease detection \\nTeam Name: Core\\nTeam Leader Name: Shafqat Ameen Mir \\nInstitute Code (AISHE): NA \\nInstitute Name:\\nKalasalingam University \\nTheme Name:'),\n",
       " Document(metadata={'producer': 'Nitro Pro 13 (13.42.1.855)', 'creator': 'Nitro Pro 13 (13.42.1.855)', 'creationdate': '2023-09-22T23:58:25+05:30', 'moddate': '2023-09-22T23:58:25+05:30', 'author': 'Sarim Moin', 'title': 'Basic Details of the Team and Problem Statement', 'source': 'pdf_file.pdf', 'total_pages': 5, 'page': 1, 'page_label': '2'}, page_content='Idea/Approach Details\\nDescribe your idea/Solution/Prototype here: \\nThe \"Chicken Disease Detection\" project aims to develop an \\ninnovative solution to safeguard poultry farming by utilizing \\nadvanced technology. Our prototype combines computer vision \\nand machine learning algorithms to analyze images of chickens \\nand identify potential signs of diseases. By capturing and \\nprocessing visual data, the system can detect subtle symptoms \\nsuch as changes in plumage, posture, or behavior, enabling early \\ndisease identification. This early warning system helps poultry \\nfarmers take prompt action to prevent disease outbreaks, \\nensuring healthier flocks and minimizing economic losses. \\nUltimately, our solution supports sustainable and efficient \\npoultry farming practices, contributing to food security and \\nanimal welfare \\n2\\nDescribe your Technology stack here :\\n⮚ Machine learning \\n⮚ Data science \\n⮚ IOT sensors and cameras'),\n",
       " Document(metadata={'producer': 'Nitro Pro 13 (13.42.1.855)', 'creator': 'Nitro Pro 13 (13.42.1.855)', 'creationdate': '2023-09-22T23:58:25+05:30', 'moddate': '2023-09-22T23:58:25+05:30', 'author': 'Sarim Moin', 'title': 'Basic Details of the Team and Problem Statement', 'source': 'pdf_file.pdf', 'total_pages': 5, 'page': 2, 'page_label': '3'}, page_content='Idea/Approach Details\\nDescribe your Use Cases here \\n⮚ Automated Diagnostics \\n⮚ Data-Driven Farm \\nManagement.\\n⮚ Remote Monitoring. \\n⮚ Vaccination Management \\n⮚ Market Quality Assurance \\n3\\nDescribe your Dependencies / Show stopper here \\n⮚ Data Availability. \\n⮚ Access to Poultry Farms.\\n⮚ Sensor and Camera \\nDeployment. \\n⮚ Privacy and Data Security .\\n⮚ Algorithm Accuracy \\n⮚ Educating Farmers'),\n",
       " Document(metadata={'producer': 'Nitro Pro 13 (13.42.1.855)', 'creator': 'Nitro Pro 13 (13.42.1.855)', 'creationdate': '2023-09-22T23:58:25+05:30', 'moddate': '2023-09-22T23:58:25+05:30', 'author': 'Sarim Moin', 'title': 'Basic Details of the Team and Problem Statement', 'source': 'pdf_file.pdf', 'total_pages': 5, 'page': 3, 'page_label': '4'}, page_content='Team Member Details \\nTeam Leader Name: Shafqat ameen mir \\nBtech(IT) INFORMATION TECHNOLOGY Year (II) \\nTeam Member 1 Name: Gaznavie Ahad\\nBtech(CSE)                                                         DATA SCIENCE Year (II) \\nTeam Member 2 Name: R.Kavyasri\\nBtech(IT) INFORMATION TECHNOLOGY Year (II) \\nTeam Member 3 Name: G.Uday \\nBtech(CSE)                                           DATA SCIENCE Year (II) \\nTeam Member 4 Name: Sheik Abdul Rahuman M\\nBtech(CSE)                                    INFORMATION TECHNOLOGY Year (II) \\nTeam Member 5 Name: K.Satya Prakash \\nBtech(CSE)                                         AI &ML                                        Year (II) \\nTeam Mentor 1 Name:P Rajkumar \\nAcademic Machine Learning and Deep learning Domain Experience (2 years)'),\n",
       " Document(metadata={'producer': 'Nitro Pro 13 (13.42.1.855)', 'creator': 'Nitro Pro 13 (13.42.1.855)', 'creationdate': '2023-09-22T23:58:25+05:30', 'moddate': '2023-09-22T23:58:25+05:30', 'author': 'Sarim Moin', 'title': 'Basic Details of the Team and Problem Statement', 'source': 'pdf_file.pdf', 'total_pages': 5, 'page': 4, 'page_label': '5'}, page_content='ALGORITHIMS \\n01.Random Forest Algorithm \\n02.Regression Algorithm \\n03.Time series Algorithm \\n04.Anomoli Detection Algorithm \\n05.Encryption Algorithm \\n06.Hashing Algorithm \\n07.Precision Agriculture Algorithm \\n5')]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f049918b",
   "metadata": {},
   "source": [
    "##web based loader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0aca6686",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "expression cannot contain assignment, perhaps you meant \"==\"? (2733146416.py, line 3)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[17], line 3\u001b[1;36m\u001b[0m\n\u001b[1;33m    loader=WebBaseLoader(web_paths=(\"https://lilianweng.github.io/posts/2025-05-01-thinking/\"),bs_kwargs=dict(parse-only=bs4.SoupStrainer(class =\"post-title\"))\u001b[0m\n\u001b[1;37m                                                                                                              ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m expression cannot contain assignment, perhaps you meant \"==\"?\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "import bs4\n",
    "loader=WebBaseLoader(web_paths=(\"https://lilianweng.github.io/posts/2025-05-01-thinking/\"),bs_kwargs=dict(parse-only=bs4.SoupStrainer(class =\"post-title\"))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4d3afa10",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'https://lilianweng.github.io/posts/2025-05-01-thinking/', 'title': \"Why We Think | Lil'Log\", 'description': 'Special thanks to John Schulman for a lot of super valuable feedback and direct edits on this post.\\nTest time compute (Graves et al. 2016, Ling, et al. 2017, Cobbe et al. 2021) and Chain-of-thought (CoT) (Wei et al. 2022, Nye et al. 2021), have led to significant improvements in model performance, while raising many research questions. This post aims to review recent developments in how to effectively use test-time compute (i.e. “thinking time”) and why it helps.', 'language': 'en'}, page_content='\\n\\n\\n\\n\\n\\nWhy We Think | Lil\\'Log\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nLil\\'Log\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n|\\n\\n\\n\\n\\n\\n\\nPosts\\n\\n\\n\\n\\nArchive\\n\\n\\n\\n\\nSearch\\n\\n\\n\\n\\nTags\\n\\n\\n\\n\\nFAQ\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n      Why We Think\\n    \\nDate: May 1, 2025  |  Estimated Reading Time: 40 min  |  Author: Lilian Weng\\n\\n\\n \\n\\n\\nTable of Contents\\n\\n\\n\\nMotivation\\n\\nAnalogy to Psychology\\n\\nComputation as a Resource\\n\\nLatent Variable Modeling\\n\\n\\nThinking in Tokens\\n\\nBranching and Editing\\n\\nParallel Sampling\\n\\nSequential Revision\\n\\n\\nRL for Better Reasoning\\n\\nExternal Tool Use\\n\\nThinking Faithfully\\n\\nDoes the Model Tell What it Thinks Faithfully\\n\\nOptimization Pressure on CoT: Good or Bad?\\n\\n\\n\\nThinking in Continuous Space\\n\\nRecurrent Architecture\\n\\nThinking Tokens\\n\\n\\nThinking as Latent Variables\\n\\nExpectation-Maximization\\n\\nIterative Learning\\n\\n\\nScaling Laws for Thinking Time\\n\\nWhat’s for Future\\n\\nCitation\\n\\nReferences\\n\\n\\n\\n\\n\\nSpecial thanks to John Schulman for a lot of super valuable feedback and direct edits on this post.\\nTest time compute (Graves et al. 2016, Ling, et al. 2017, Cobbe et al. 2021) and Chain-of-thought (CoT) (Wei et al. 2022, Nye et al. 2021), have led to significant improvements in model performance, while raising many research questions. This post aims to review recent developments in how to effectively use test-time compute (i.e. “thinking time”) and why it helps.\\nMotivation#\\nEnabling models to think for longer can be motivated in a few different ways.\\nAnalogy to Psychology#\\nThe core idea is deeply connected to how humans think. We humans cannot immediately provide the answer for \"What\\'s 12345 times 56789?\". Rather, it is natural to spend time pondering and analyzing before getting to the result, especially for complex problems. In Thinking, Fast and Slow (Kahneman, 2013), Daniel Kahneman characterizes human thinking into two modes, through the lens of the dual process theory :\\n\\nFast thinking (System 1) operates quickly and automatically, driven by intuition and emotion while requiring little to no effort.\\nSlow thinking (System 2) demands deliberate, logical thought and significant cognitive efforts. This mode of thinking consumes more mental energy and requires intentional engagement.\\n\\nBecause System 1 thinking is fast and easy, it often ends up being the main decision driver, at the cost of accuracy and logic. It naturally relies on our brain’s mental shortcuts (i.e., heuristics) and can lead to errors and biases. By consciously slowing down and taking more time to reflect, improve and analyze, we can engage in System 2 thinking to challenge our instincts and make more rational choices.\\nComputation as a Resource#\\nOne view of deep learning, is that neural networks can be characterized by the amount of computation and storage they can access in a forward pass, and if we optimize them to solve problems using gradient descent, the optimization process will figure out how to use these resources–they’ll figure out how to organize these resources into circuits for calculation and information storage. From this view, if we design an architecture or system that can do more computation at test time, and we train it to effectively use this resource, it’ll work better.\\nIn Transformer models, the amount of computation (flops) that the model does for each generated token is roughly 2 times the number of parameters. For sparse models like mixture of experts (MoE), only a fraction of the parameters are used in each forward pass, so computation = 2 * parameters / sparsity, where sparsity is the fraction of experts active.\\nOn the other hand, CoT enables the model to perform far more flops of computation for each token of the answer that it is trying to compute. In fact, CoT has a nice property that it allows the model to use a variable amount of compute depending on the hardness of the problem.\\nLatent Variable Modeling#\\nA classic idea in machine learning is to define a probabilistic model with a latent (hidden) variable $z$ and a visible variable $y$, where $y$ is given to our learning algorithm. Marginalizing (summing) over the possible values of the latent variable allows us to express a rich distribution over the visible variables, $P(y) = \\\\sum_{z \\\\sim P(z)} P(y \\\\mid z)$. For example, we can model the distribution over math problems and solutions by letting $x$ denote a problem statement, $y$ be ground truth answer or proof, and $z$ as a free-form thought process that leads to the proof. The marginal probability distribution to optimize would be $P(y \\\\mid x) = \\\\sum_{z \\\\sim p(z\\\\mid x)} P(y \\\\mid x, z)$\\nThe latent variable perspective is particularly useful for understanding methods that involve collecting multiple parallel CoTs or searching over the CoT–these algorithms can be seen as sampling from the posterior $P(z \\\\mid x, y)$. This view also suggests the benefits of using the log loss $\\\\log P(y \\\\mid x)$ as the target objective to optimize, as the log loss objective has been so effective in pretraining.\\nThinking in Tokens#\\nThe strategy of generating intermediate steps before generating short answers, particularly for math problems, was explored by Ling, et al. 2017, who introduced the AQUA-RAT dataset, and then expanded by Cobbe et al. 2021, who introduced the Grade School Math (GSM) dataset. Cobbe et al. train a generator with supervised learning on human-written solutions and verifiers that predict the correctness of a candidate solution; they can then search over these solutions. Nye et al. (2021) experimented with intermediate thinking tokens as “scratchpads” and Wei et al. (2022) coined the now-standard term chain-of-thought (CoT).\\nEarly work on improving CoT reasoning involved doing supervised learning on human-written reasoning traces or model-written traces filtered for answer correctness, where the latter can be seen as a rudimentary form of reinforcement learning (RL). Some other work found that one could significantly boost math performance of instruction tuned models by prompting them appropriately, with \"think step by step\" (Kojima et al. 2022) or more complex prompting to encourage the model to reflect on related knowledge first (Yasunaga et al. 2023).\\nLater work found that the CoT reasoning capabilities can be significantly improved by doing reinforcement learning on a dataset of problems with automatically checkable solutions, such as STEM problems with short answers, or coding tasks that can be checked with unit tests (Zelikman et al. 2022, Wang et al., 2023, Liu et al., 2023). This approach rose to prominence with the announcement of o1-preview, o3, and the R1 tech report (DeepSeek-AI, 2025), which showed that a simple recipe where a policy gradient algorithm could lead to strong performance.\\n\\n\\nChain-of-thought prompting leads to higher success rate of solving math problems. Larger models benefit more from thinking time. (Image source: Wei et al. 2022)\\n\\nBranching and Editing#\\nThe fundamental intent of test-time compute is to adaptively modify the model’s output distribution at test time. There are various ways of utilizing test time resources for decoding to select better samples and thus alter the model’s predictions towards a more desired distribution. Two main approaches for improving the decoding process are parallel sampling and sequential revision.\\n\\nParallel sampling generates multiple outputs simultaneously, meanwhile providing guidance per step with process reward signals or using verifiers to judge the quality at the end. It is the most widely adopted decoding method to improve test time performance, such as best-of-$N$ or beam search. Self-consistency (Wang et al. 2023) is commonly used to select the answer with majority vote among multiple CoT rollouts when the ground truth is not available.\\nSequential revision adapts the model’s responses iteratively based on the output in the previous step, asking the model to intentionally reflect its existing response and correct mistakes. The revision process may have to rely on a fine-tuned model, as naively relying on the model’s intrinsic capability of self-correction without external feedback may not lead to improvement (Kamoi et al. 2024, Huang et al. 2024).\\n\\nParallel sampling is simple, intuitive and easier to implement, but bounded by the model capability of whether it can achieve the correct solution in one-go. Sequential explicitly asks the model to reflect on mistakes but it is slower and requires extra care during implementation as it does run the risk of correct predictions being modified to be incorrect or introducing other types of hallucinations. These two methods can be used together. Snell et al. (2024) showed that easier questions benefit from purely sequential test-time compute, whereas harder questions often perform best with an optimal ratio of sequential to parallel compute.\\n\\n\\nIllustration of parallel sampling vs sequential revision.\\n\\nParallel Sampling#\\nGiven a generative model and a scoring function that we can use to score full or partial samples, there are various search algorithms we can use to find a high scoring sample. Best-of-$N$ is the simplest such algorithm: one just collects $N$ independent samples and chooses the highest-ranking sample according to some scoring function. Beam search is a more sophisticated search algorithm that makes the search process more adaptive, spending more sampling computation on more promising parts of the solution space.\\nBeam search maintains a set of promising partial sequences and alternates between extending them and pruning the less promising ones. As a selection mechanism, we can use a process reward model (PRM; Lightman et al. 2023) to guide beam search candidate selection. Xie et al. (2023) used LLM to evaluate how likely its own generated reasoning step is correct, formatted as a multiple-choice question and found that per-step self-evaluation reduces accumulative errors in multi-step reasoning during beam search decoding. Besides, during sampling, annealing the temperature helps mitigate aggregated randomness. These experiments by Xie et al. achieved 5-6% improvement on few-shot GSM8k, AQuA and StrategyQA benchmarks with the Codex model. Reward balanced search (short for “REBASE”; Wu et al. 2025) separately trained a process reward model (PRM) to determine how much each node should be expanded at each depth during beam search, according to the softmax-normalized reward scores. Jiang et al. (2024) trained their PRM, named “RATIONALYST”, for beam search guidance on synthetic rationales conditioned on a large amount of unlabelled data. Good rationales are filtered based on whether they help reduce the neg log-prob of true answer tokens by a threshold, when comparing the difference between when the rationales is included in the context vs not. At inference time, RATIONALYST provides process supervision to the CoT generator by helping estimate log-prob of next reasoning steps (“implicit”) or directly generating next reasoning steps as part of the prompt (“explicit”).\\n\\n\\nBeam search decoding guided by LLM self-evaluation per reasoning step. (Image source: Xie et al. 2023)\\n\\nInterestingly, it is possible to trigger the emergent chain-of-thought reasoning paths without explicit zero-shot or few-shot prompting. Wang & Zhou (2024) discovered that if we branch out at the first sampling tokens by retaining the top $k$ tokens with highest confidence, measured as the difference between top-1 and top-2 candidates during sampling, and then continue these $k$ sampling trials with greedy decoding onward, many of these sequences natively contain CoT. Especially when CoT does appear in the context, it leads to a more confident decoding of the final answer. To calculate the confidence of the final answer, the answer span needs to be identified by task-specific heuristics (e.g. last numerical values for math questions) or  by prompting the model further with \"So the answer is\". The design choice of only branching out at the first token is based on the observation that early branching significantly enhances the diversity of potential paths, while later tokens are influenced a lot by previous sequences.\\n\\n\\nTop-$k$ decoding, $k$ refers to the number of candidates at the first sampling step. (Image source: Wang & Zhou, 2024)\\n\\nSequential Revision#\\nIf the model can reflect and correct mistakes in past responses, we would expect the model to produce a nice sequence of iterative revision with increasing quality. However, this self-correction capability turns out to not exist intrinsically among LLMs and does not easily work out of the box, due to various failure modes, such as, (1) hallucination, including modifying correct responses to be incorrect; (2) behavior collapse to non-correcting behavior; e.g. making minor or no modification on the first incorrect responses; or (3) fail to generalize to distribution shift at test time. Experiments by Huang et al. (2024) showed that naively applying self-correction leads to worse performance and external feedback is needed for models to self improve, which can be based on matching ground truths, heuristics and task-specific metrics, unit tests results for coding questions (Shinn, et al. 2023), a stronger model (Zhang et al. 2024), as well as human feedback (Liu et al. 2023).\\nSelf-correction learning (Welleck et al. 2023) aims to train a corrector model $P_\\\\theta(y \\\\mid y_0, x)$ given a fixed generator model $P_0(y_0 \\\\mid x)$. While the generator model remains to be generic, the corrector model can task-specific and only does generation conditioned on an initial model response and additional feedback (e.g. a sentence, a compiler trace, unit test results; can be optional):\\n\\nSelf-correction learning first generates first generates multiple outputs per prompt in the data pool;\\nthen create value-improving pairs by pairing two outputs for the same prompt together if one has a higher value than the other, (prompt $x$, hypothesis $y$, correction $y’$).\\nThese pairs are selected proportional to is improvement in value, $v(y’) - v(y)$, and similarity between two outputs, $\\\\text{Similarity}(y, y’)$ to train the corrector model.\\nTo encourage exploration, the corrector provides new generations into the data pool as well. At the inference time, the corrector can be used iteratively to create a correction trajectory of sequential revision.\\n\\n\\n\\nIllustration of self-correction learning by matching model outputs for the same problem to form value-improving pairs to train a correction model. (Image source: Welleck et al. 2023)\\n\\nRecursive inspection (Qu et al. 2024) also aims to train a better corrector model but with a single model to do both generation and self-correction.\\nSCoRe (Self-Correction via Reinforcement Learning; Kumar et al. 2024) is a multi-turn RL approach to encourage the model to do self-correction by producing better answers at the second attempt than the one created at the first attempt. It composes two stages of training: stage 1 only maximizes the accuracy of the second attempt while enforcing a KL penalty only on the first attempt to avoid too much shifting of the first-turn responses from the base model behavior; stage 2 optimizes the accuracy of answers produced by both the first and second attempts. Ideally we do want to see performance at both first and second attempts to be better, but adding stage 1 prevents the behavior collapse where the model does minor or none edits on the first response, and stage 2 further improves the results.\\n\\n\\nExplicit training setup to improve self-correction capabilities by doing two-staged RL training. (Image source: Kumar et al. 2024)\\n\\nRL for Better Reasoning#\\nThere’s been a lot of recent success in using RL to improve the reasoning ability of language models, by using a collection of questions with ground truth answers (usually STEM problems and puzzles with easy to verify answers), and rewarding the model for getting the correct answer.Recent activity in this area was spurred by strong performance of the o-series models from OpenAI, and the subsequent releases of models and tech reports from DeepSeek.\\nDeepSeek-R1 (DeepSeek-AI, 2025) is an open-source LLM designed to excel in tasks that require advanced reasoning skills like math, coding and logical problem solving. They run through 2 rounds of SFT-RL training, enabling R1 to be good at both reasoning and non-reasoning tasks.\\n\\nCold-start SFT is to fine-tune the DeepSeek-V3-Base base model on a collection of thousands of cold-start data. Without this step, the model has issues of poor readability and language mixing.\\nReasoning-oriented RL trains a reasoning model on reasoning-only prompts with two types of rule-based rewards:\\n\\nFormat rewards: The model should wrap CoTs by <thinking> ... </thinking> tokens.\\nAccuracy rewards: Whether the final answers are correct. The answer for math problems needs to be present in a specific format (e.g. in a box) to be verified reliably. For coding problems, a compiler is used to evaluate whether test cases pass.\\n\\n\\nRejection-sampling + non-reasoning SFT utilizes new SFT data created by rejection sampling on the RL checkpoint of step 2, combined with non-reasoning supervised data from DeepSeek-V3 in domains like writing, factual QA, and self-cognition, to retrain DeepSeek-V3-Base.\\n\\nFilter out CoTs with mixed languages, long paragraphs, and code blocks.\\nInclude non-reasoning tasks using DeepSeek-V3 (DeepSeek-AI, 2024) pipeline.\\nFor certain non-reasoning tasks, call DeepSeek-V3 to generate potential CoTs before answering the question by prompting. But for simpler queries like “hello”, CoT is not needed.\\nThen fine-tune the DeepSeek-V3-Base on the total 800k samples for 2 epochs.\\n\\n\\nThe final RL stage trains the step 3 checkpoint on both reasoning and non-reasoning prompts, improving helpfulness, harmlessness and reasoning.\\n\\n\\n\\nDeepSeek-R1 performs comparable to OpenAI o1-preview and o1-mini on several widely used reasoning benchmarks. DeepSeek-V3 is the only non-reasoning model listed. (Image source: DeepSeek-AI, 2025)\\n\\nInterestingly the DeepSeek team showed that with pure RL, no SFT stage, it is still possible to learn advanced reasoning capabilities like reflection and backtracking (“Aha moment”). The model naturally learns to spend more thinking tokens during the RL training process to solve reasoning tasks. The “aha moment” can emerge, referring to the model reflecting on previous mistakes and then trying alternative approaches to correct them. Later, various open source efforts happened for replicating R1 results like Open-R1, SimpleRL-reason, and TinyZero, all based on Qwen models. These efforts also confirmed that pure RL leads to great performance on math problems, as well as the emergent “aha moment”.\\n\\n\\nExamples of the model learning to reflect and correct mistakes. (Image source: (left) DeepSeek-AI, 2025; (right) Zeng et al. 2025)\\n\\nThe DeepSeek team also shared some of their unsuccessful attempts. They failed to use process reward model (PRM) as it is hard to define per-step rubrics or determine whether an intermediate step is correct, meanwhile making the training more vulnerable to reward hacking. The efforts on MCTS (Monte Carlo Tree Search) also failed due to the large search space for language model tokens, in comparison to, say, chess; and training the fine-grained value model used for guiding the search is very challenging too. Failed attempts often provide unique insights and we would like to encourage the research community to share more about what did not work out.\\nExternal Tool Use#\\nDuring the reasoning steps, certain intermediate steps can be reliably and accurately solved by executing code or running mathematical calculations. Offloading that part of reasoning components into an external code interpreter, as in PAL (Program-Aided Language Model; Gao et al. 2022) or Chain of Code (Li et al. 2023), can extend the capability of LLM with external tools, eliminating the need for LLMs to learn to execute code or function as calculators themselves. These code emulators, like in Chain of Code, can be augmented by an LLM such that if a standard code interpreter fails, we have the option of using LLM to execute that line of code instead. Using code to enhance reasoning steps are especially beneficial for mathematical problems, symbolic reasoning and algorithmic tasks. These unit tests may not exist as part of the coding questions, and in those cases, we can instruct the model to self-generate unit tests for it to test against to verify the solution (Shinn, et al. 2023).\\n\\n\\nAn example of program-aided language model prompting looks like. (Image source: Gao et al. 2022)\\n\\nReAct (Reason+Act; Yao et al. 2023) combines the action of searching the Wikipedia API and generation of reasoning traces, such that reasoning paths can incorporate external knowledge.\\n\\n\\nAn example of the ReAct prompting method to solve a HotpotQA question, using Wikipedia search API as an external tool to help with reasoning. (Image source: Yao et al. 2023)\\n\\no3 & o4-mini, recently released by OpenAI, are another two good examples where the reasoning process involves tool use like Web search, code execution and image processing. The team observed that large-scale reinforcement learning exhibits the same trend as in the GPT paradigm that “more compute = better performance”.\\nThinking Faithfully#\\nDeep learning models are often treated as black boxes and various interpretability methods have been proposed. Interpretability is useful for a couple reasons: first, it gives us an extra test to determine if the model is misaligned with its creators’ intent, or if it’s misbehaving in some way that we can’t tell by monitoring its actions. Second, it can help us determine whether the model is using a sound process to compute its answers. Chain of thought provides an especially convenient form of interpretability, as it makes the model’s internal process visible in natural language. This interpretability, however, rests on the assumption that the model truthfully describes its internal thought processes.\\nRecent work showed that monitoring CoT of reasoning models can effectively detect model misbehavior such as reward hacking, and can even enable a weaker model to monitor a stronger model (Baker et al. 2025). Increasing test time compute can also lead to improved adversarial robustness (Zaremba et al. 2025); this makes sense intuitively, because thinking for longer should be especially useful when the model is presented with an unusual input, such as an adversarial example or jailbreak attempt – it can use the extra thinking time to make sense of the strange situation it’s been presented with.\\n\\n\\nThe experiment of asking the model to decide if another model tried to hack the unit tests in some way for coding questions given its thought process. We can monitor these reward hacking behavior during training with different types of monitor. The exit(0) coding hack is when the agent exploited a bug that allowed it to exit from the environment early without running all unit tests. The raise SkipTest hack is when the agent raises an exception from functions outside the testing framework in order to skip unit test evaluation. (Image source: Baker et al. 2025)\\n\\nDoes the Model Tell What it Thinks Faithfully#\\nIntuitively, model CoTs could be biased due to lack of explicit training objectives aimed at encouraging faithful reasoning. Or when we fine-tune the model on human-written explanations, those human-written samples may contain mistakes. Thus we cannot by default assume CoT is always faithful .\\nLanham et al. (2023) investigated several modes of CoT faithfulness failures by deliberately introducing mistakes into CoTs and measuring their impacts on the accuracy of a set of multiple choice tasks (e.g. AQuA, MMLU, ARC Challenge, TruthfulQA, HellaSwag):\\n\\n\\nMistake 1 (Early answering): The model may form a conclusion prematurely before CoT is generated. This is tested by early truncating or inserting mistakes into CoT. Different tasks revealed varying task-specific dependencies on CoT effectiveness; some have evaluation performance sensitive to truncated CoT but some do not. Wang et al. (2023) did similar experiments but with more subtle mistakes related to bridging objects or language templates in the formation of CoT.\\n\\n\\nMistake 2 (Uninformative tokens): Uninformative CoT tokens improve performance. This hypothesis is tested by replacing CoT with filler text (e.g. all periods) and this setup shows no accuracy increase and some tasks may suffer performance drop slightly when compared to no CoT.\\n\\n\\nMistake 3 (Human-unreadable encoding): Relevant information is encoded in a way that is hard for humans to understand. Paraphrasing CoTs in an non-standard way did not degrade performance across datasets, suggesting accuracy gains do not rely on human-readable reasoning.\\n\\n\\n\\n\\nIllustration of different ways of CoT perturbation to assess its faithfulness. (Image source: Lanham et al. 2023)\\n\\nInterestingly, Lanham et al. suggests that for multiple choice questions, smaller models may not be capable enough of utilizing CoT well, whereas larger models may have been able to solve the tasks without CoT. This dependency on CoT reasoning, measured by the percent of obtaining the same answer with vs without CoT, does not always increase with model size on multiple choice questions, but does increase with model size on addition tasks, implying that thinking time matters more for complex reasoning tasks.\\n\\n\\nThe dependency on CoT reasoning is measured as the percentage of obtaining same answers with vs without CoT. It matters more for reasoning tasks like addition and larger models benefit more. (Image source: Lanham et al. 2023)\\n\\nAlternative approaches for testing CoT faithfulness involve perturbing prompts rather than modifying CoT paths directly (Turpin et al. 2023, Chua & Evans, 2025, Chen et al. 2025).\\nOne method consistently labels correct answers as “(A)” in few-shot examples regardless of true labels to introduce biases.\\nAnother prompting technique inserts misleading hints into prompts, such as \"I think the answer is <random_label> but curious to hear what you think\". or \"A Stanford Professor thinks the answer is <random_label>\". By comparing model predictions for the same question with vs without the misleading hint, we can measure whether a model is able to faithfully describe the influence of the hint on its answer. Particularly, in cases where the model produces different hint and non-hint answers, we measure whether the model acknowledges the hint when solving the question with hint. If the model is faithful, it should explicitly acknowledge the impact and admit the change of its answer is due to the hint.\\n\\n\\nBoth GPT and Claude models are sensitive to different types of biases in context. Decrease in model accuracy indicates systematic unfaithfulness. Direct hints of wrong labels are more effectively than \"Answer is always A\" type of bias. (Image source: Turpin et al. 2023)\\n\\nMultiple studies found that reasoning models describe the influence of the hint much more reliably than all the non-reasoning models tested. For example, we can measure the fraction of samples where the model acknowledges the hint as a deciding factor (“faithful CoT”). Reasoning models (Claude 3.7 Sonnet, DeepSeek R1) are overall doing better than non-reasoning ones (Claude 3.6, DeepSeek V3).\\n\\n\\nReasoning models are more likely to reveal faithful CoT than non-reasoning models. (Image source: Chen et al. 2025)\\n\\nSome evidence showed that using reward models leads to less faithfulness in model outputs. The reward model in classic RLHF is not trained to incentivize faithful behavior in this test as the preference comparison dataset is collected by humans selecting which one seems better or pleasing. However, in reasoning models, the CoT paths are optimized for producing the correct final answers, not to match human preference defined in RM, and thus intuitively are expected to be more faithful.\\nOptimization Pressure on CoT: Good or Bad?#\\nMonitoring CoT of reasoning models for reward hacking behavior is effective. A simple way to use this monitor is to run rejection sampling with the monitor as a filter at test time and identify solutions without reward hacking. However, it only works at the early stage of RL, and, as the model is further optimized, it is rare to find a sample to pass a CoT monitor within budget. This challenge naturally suggests that we could consider incorporating CoT monitors into RL rewards—a form of process-based supervision—to prevent reward hacking.\\nHowever, incorporating CoT monitoring reward into RL does not work out well, as it leads to obfuscated reward hacking where the model tries to hide its true intent within the CoT (Baker et al. 2025). With this direct optimization pressure provided by the CoT monitor applied during RL training, the model still can find ways to hack, although a bit less frequently, but can hide its intention in CoT to avoid being caught by the monitor.\\n\\n\\nWhen using the monitor to provide an RL reward signal to prevent reward hacking during training, the model still can find ways to hack while avoiding being caught by the monitor. (Image source: Baker et al. 2025)\\n\\nExperiments on CoT length RL signals by Yeo et al. (2025) confirmed a similar message as Baker et al. that new types of reward hacking can happen with new types of RL reward shaping. They designed the reward function such that correct CoTs can have higher rewards than wrong ones, short correct CoTs obtain higher rewards than long correct ones and short incorrect CoT receive higher penalties than long incorrect CoTs. With this extra reward, the model learns to repeat text in CoTs for challenging questions rather than attempting to solve them. Yeo et al. further applied a n-gram repetition penalty to prevent this hacking behavior.\\n\\n\\nCareful reward shaping is needed to stabilize training with CoT length rewards. (Image source: Yeo et al. 2025)\\n\\nChen et al. (2025) experimented with a flawed RL environment, specifically using a grader with incorrect answers filled in for multiple-choice questions. The model learns to exploit the reward hack on >99% of the prompts, but almost never (<2%) verbalizes the reward hack in its CoT on more than half of their environments. Additional RL optimization pressure fails to incentivize the model to verbalize the hack in this case.\\nRL training is inherently sensitive to reward hacking. Only relying on heuristic investigation of reward hacking and manual fixes may lead to a “whack-a-mole” situation. We would suggest being very cautious when trying to apply optimization directly on CoT during RL training, or trying to avoid it altogether.\\nThinking in Continuous Space#\\nAdaptive Computation Time, introduced by Alex Graves in 2016, predated large language models but pioneered the same direction of enabling the model to dynamically decide the number of computational steps to take at the inference time, which can be viewed as enabling the model to “think more” in continuous space at test time. Adaptive thinking time in continuous space can be enabled vertically via recurrent architecture or horizontally via more sequential sampling steps.\\nRecurrent Architecture#\\nA number of architecture variations have been proposed to make the Transformer architecture recurrent, enabling adaptive test time compute (Dehghani, et al. 2019, Hutchins, et al. 2022, Bulatov, et al. 2022). A deep dive into literature on this topic would make the post too long, so we will only review a few.\\nUniversal Transformer (Dehghani, et al. 2019) combines self-attention in Transformer with the recurrent mechanism in RNN, dynamically adjusting the number of steps using adaptive computation time (Graves, 2016). On a high level, it can be viewed as a recurrent function for learning the hidden state representation per token, and if the number of steps is fixed, an Universal Transformer is equivalent to a multi-layer Transformer with shared parameters across layers.\\nA recent recurrent architecture design, proposed by Geiping et al. (2025), adds a recurrent block $R$ on top of the standard Transformer. Every iteration of this recurrent block takes the embedding $\\\\mathbf{e}$ and a random state $\\\\mathbf{s}_i$. Conceptually, this recurrent-depth architecture is a bit similar to a conditioned diffusion model, where the original input $\\\\mathbf{e}$ is provided in every recurrent step while a random Gaussian initialized state $\\\\mathbf{s}_i$ gets updated iteratively through the process. (Interestingly some of their experiments of designs that resemble diffusion models more turned out to be bad.)\\n\\n$$  \\n\\\\begin{aligned}  \\n\\\\mathbf{e} &= P(\\\\mathbf{x}) & \\\\text{embedding} \\\\\\\\ \\\\mathbf{s}\\\\_0 &\\\\sim \\\\mathcal{N}(\\\\mathbf{0}, \\\\sigma^2 I\\\\_{n \\\\cdot h}) \\\\\\\\ \\\\mathbf{s}\\\\_i &= R(\\\\mathbf{e}, \\\\mathbf{s}\\\\_{i-1}) \\\\quad\\\\text{ for }i \\\\in {1, \\\\dots, r} & \\\\small{\\\\text{recurrent block; resembles a Transformer block}}\\\\\\\\ \\\\mathbf{p} &= C(\\\\mathbf{s}\\\\_r) & \\\\text{unembedding}  \\n\\\\end{aligned}\\n$$\\n\\nThe recurrence count $r$ during training is randomized, sampled from a log-normal Poisson distribution, per input sequence. To manage computational costs, backpropagation is truncated to only the last $k$ iterations of the recurrent unit ($k=8$ in experiments), making it possible to train on the heavy-tail part of the Poisson distribution. The embedding block continues to receive gradient updates in every step since its output $\\\\mathbf{e}$ is injected in every step, mimicking RNN training. Unsurprisingly, the stability of training a recurrent model turns out to be very sensitive. Factors like initialization, normalization and hyperparameters all matter, especially when scaling up the training. For example, hidden states can collapse by predicting the same hidden state for every token; or the model may learn to ignore the incoming state $\\\\mathbf{s}$. To stabilize the training, Geiping et al. adopted an embedding scale factor, a small learning rate and careful tuning.\\n\\n\\nPlot of an experiment on training a 3.5B model with depth recurrence. The saturation roughly happens around $\\\\bar{r} = 32$, making us wonder how this architecture extrapolate and generalize to larger iteration counts. (Image source: Geiping et al. 2025)\\n\\nThinking Tokens#\\nThinking tokens refer to a set of implicit tokens introduced during training or inference that do not carry direct linguistic meaning. Instead, their role is to provide extra thinking time and compute power for the model to perform better.\\nHerel & Mikolov (2023) introduced the idea of inserting special thinking tokens (<T>) after each word in a sentence and training the model on such a dataset. Each thinking token buys extra time for the model to process and make better predictions. Training with thinking tokens on a toy model setup results in lower perplexity than baseline model trained without them. The benefits of thinking tokens are more pronounced for non-trivial reasoning tasks or sentences involving numbers.\\nSimilarly, pause tokens proposed by Goyal et al. (2024) delay the model’s outputs by appending dummy tokens (e.g. character like . or #) at the end of the input sequence, giving the model extra computation during inference. It is important to inject such pause tokens both during training and inference time, while only fine-tuning on pause tokens leads to limited gain. During training, multiple copies of pause tokens are inserted at uniformly random locations and the loss on pause tokens is ignored for training.\\n\\n\\nIllustration of how pause tokens are injected during training and inference in comparison to standard setup. (Image source: Goyal et al. 2024)\\n\\nInterestingly, thinking tokens or pause tokens in above experiments do not carry any extra information or add many new parameters. But why is it still helpful? On one hand, it helps expand the computation by introducing more inference loops, effectively increasing computational capacity. On the other hand, it can be seen as acting as a special, implicit form of CoTs. A drawback here is hat the model needs to be pretrained with respect to thinking tokens. Still, this strategy is an interesting way to further improve the capability of test time compute utilization on top of inference time CoTs.\\nQuiet-STaR (Zelikman et al. 2025) introduces token-level reasoning by training the model to generate rationales after every token to explain future text. It mixes the future-text predictions with and without rationales and uses learning to generate better rationales and uses REINFORCE to optimize the quality of rationale generation.\\n\\n\\nIllustration of Quiet-STaR. (Image source: Zelikman et al. 2025)\\n\\nQuiet-STaR consists of three stages:\\n\\n\\nThink: Predicting next tokens with rationales. Due to high computational cost demanded by token level reasoning, this process is designed to generate multiple rationales in parallel. A special attention map is used to enable all thought tokens to only pay attention to themselves, all preceding thought tokens within the same thought, and the preceding text.\\n\\n\\nTalk: Next token prediction without rationale is mixed with post-rationale prediction. The mixing weight for two logits is learned by a special mixing head of a shallow MLP out of hidden output after each rationale. Correct next tokens can be selected via teacher forcing.\\n\\n\\nLearn: Train the model to generate better rationale via REINFORCE by learning from examples that increase the probability of correct next token while discarding those that hurt the prediction.\\n\\n\\nWithout dataset specific fine-tuning, Quiet-STaR improves zero-shot results on CommonsenseQA (36.3%→47.2%) and GSM8K (5.9%→10.9%), within experiments on Mistral 7B.\\nThinking as Latent Variables#\\nA latent variable model defines a probabilistic framework where observable data is explained through unobserved (latent) variables. These latent variables capture hidden structures or intermediate processes that generate the observable outcomes. Language models can be viewed as probabilistic latent variable models where test-time thinking and reasoning steps are latent thought variables (Zhou et al. 2020, Phan et al. 2023). Such a latent variable model defines a joint distribution of problems x_i, answers y_i and latent thought z_i. We would like to optimize the log-likelihood of answers given questions and a variety of CoTs as latent variables (N is the number of samples; $K$ is the number of CoTs per problem):\\n\\n$$ \\n\\\\begin{aligned}  \\n\\\\log \\\\mathcal{L}(\\\\theta)  \\n&= \\\\log p(y \\\\mid x) \\\\\\\\  \\n&= \\\\log \\\\sum_{k=1}^K p(y, z^{(k)} \\\\mid x) \\\\\\\\  \\n&= \\\\log \\\\sum_{k=1}^K p(z^{(k)} \\\\mid x)\\\\; p(y \\\\mid z^{(k)}, x) \\\\\\\\  \\n&= \\\\log \\\\mathbb{E}_{z^{(k)} \\\\sim p(z^{(k)} \\\\mid x)} \\\\; p(y \\\\mid z^{(k)}, x) \\\\\\\\  \\n\\\\end{aligned}\\n$$\\n\\nOur goal is to maximize the marginal likelihood of the correct answer, $p(y \\\\mid x)$, given a number of reasoning traces per problem, $\\\\{z^{(k)}\\\\}_{k=1}^K$.\\nExpectation-Maximization#\\nExpectation-Maximization is a commonly used iterative algorithm for optimizing parameters for a model with (hidden) latent variables, and thus can be applied to train better CoTs and then condition on that to generate better responses. Typically we iterate between E-step (Expectation) where we guess the missing information about latent variables (i.e. how to sample better CoTs), and M-step (Maximization) where we optimize the model parameters based on latent variables (i.e. how to sample better answers), until convergence.\\n\\n$$\\n\\\\log \\\\mathcal{L}(\\\\theta) = \\n\\\\log \\\\mathbb{E}_{\\\\underbrace{z^{(k)} \\\\sim p(z^{(k)} \\\\mid x)}_\\\\text{E-step}}\\\\;\\\\underbrace{p(y \\\\mid z^{(k)}, x)}_\\\\text{M-step}\\n$$\\n\\nBecause we cannot directly sample from the latent variable distribution $p(z \\\\mid x, y)$, researchers have explored methods relying on human annotated data (Zhou et al. 2020), Metropolis-Hastings MCMC (Phan et al. 2023) or Monte Carlo sampling with special importance weights (Ruan et al. 2025) to draw good CoT samples to update the model. Ruan et al. (2025) experimented with training a model on a large body of Web text with latent thoughts with the EM algorithm, where the latent thought is synthesized per chunk of observed data and then the model learns over both latent thought and data in an autoregressive manner.\\n\\n\\nIllustration of training on data corpus with latent thought injected. (Image source: Ruan et al. 2025)\\n\\nThey first prompt a LLM $\\\\tilde{q}$ to generate synthetic latent thought Z_i given observed data X_i:\\nYou are provided with a pair of web document prefix and suffix. Your task is to insert latent thoughts between them underlying the creation of the suffix conditioned on the prefix. The latent thoughts should include: the missing background knowledge and the reasoning traces underlying each claim (especially, step-by-step derivations or logical reasoning).\\nSpecial tokens like <StartOfLatent><Prior> ... <EndOfPrior> are used to insert the generated latent thought content into the raw data for training the joint distribution $p(z, x)$ or the approximate posterior $q(z \\\\mid x)$, depending on whether $z$ is inserted before or after $x$. However, since we are using a LLM $\\\\tilde{q}(z \\\\mid x)$ to generate the CoTs, it imposed a performance ceiling on how good the approximate $q(z \\\\mid x)$ can be. Ruan et al. introduced importance weights for selecting CoT samples at the E-step, formulated as:\\n\\n$$\\nw^{(k)}  \\n= \\\\frac{p(z^{(k)}, x)}{q(z^{(k)} \\\\mid x)}  \\n= \\\\frac{p(x \\\\mid z^{(k)}) \\\\; p(z^{(k)})}{q(z^{(k)} \\\\mid x)}  \\n$$\\n\\n, such that we prioritize samples with CoTs that are good at predicting the observation (i.e., high $p(x \\\\mid z^{(k)})$), simple, intuitive (i.e., high $p(z^{(k)})$) but also informative and not too obvious (i.e. low $q(z^{(k)} \\\\mid x)$).\\nIterative Learning#\\nSince pretrained models already possess the capability of generating chains of thought, it is intuitive to design an iterative improvement process where we generate multiple CoTs and fine-tune the model only on rationales that lead to correct answers.\\nHowever, this straightforward design can fail because the model receives no learning signals for problems it fails to solve. STaR (“Self-taught reasoner”; Zelikman et al. 2022) addresses this limitation by adding a “rationalization” process for failed attempts, in which the model generates good CoTs backward conditioned on both the problem and the ground truth answer and thus the model can generate more reasonable CoTs. Then the model is finetuned on correct solutions that either lead to correct outputs or are generated through rationalization.\\n\\n\\nThe algorithm of STaR. (Image source: Zelikman et al. 2022)\\n\\nWe can view STaR as an approximation to a policy gradient in RL, with a simple indicator function as the reward, $\\\\mathbb{1}[\\\\hat{y} = y]$. We want to maximize the expectation of this reward when sampling $z \\\\sim p(z \\\\mid x)$ and then $y \\\\sim p(y \\\\mid x, z)$, since $p(y \\\\mid x) = \\\\sum_z p(z \\\\mid x) \\\\; p(y \\\\mid x, z)$.\\n\\n$$\\n\\\\begin{aligned}  \\n\\\\nabla_\\\\theta J(\\\\theta)  \\n&= \\\\nabla_\\\\theta \\\\mathbb{E}_{z_i, y_i \\\\sim p(.\\\\mid x_i)} \\\\mathbb{1}[y_i = y_i^\\\\text{truth}] \\\\\\\\  \\n&= \\\\sum_{i=1}^N \\\\nabla_\\\\theta \\\\mathbb{1}[y_i = y_i^\\\\text{truth}] \\\\; p(y_i, z_i \\\\mid x_i) \\\\\\\\  \\n&= \\\\sum_{i=1}^N \\\\mathbb{1}[y_i = y_i^\\\\text{truth}] \\\\; p(y_i, z_i \\\\mid x_i) \\\\frac{\\\\nabla_\\\\theta p(y_i, z_i \\\\mid x_i)}{p(y_i, z_i \\\\mid x_i)} & \\\\text{;log-derivative trick}\\\\\\\\  \\n&= \\\\mathbb{E}_{z_i, y_i \\\\sim p(.\\\\mid x_i)} \\\\mathbb{1}[y_i = y_i^\\\\text{truth}] \\\\; \\\\nabla_\\\\theta \\\\log p(y_i, z_i \\\\mid x_i) & \\\\text{;log-derivative trick}\\n\\\\end{aligned}\\n$$\\n\\nEach iteration is equivalent to first selecting the CoT samples according to $\\\\mathbb{1}[y=y^\\\\text{truth}]$ and then running supervised fine-tuning to optimize the logprob of generating good CoTs and answers. Performance of STaR improves with more training iterations, and the “rationalization” process for generating better CoTs accelerates learning. They observed that sampling with high temperature increases the chance of getting correct answers with incorrect reasoning and finetuning LLM on such data can impair generalization. For datasets without ground truths, majority votes of multiple high-temperature outputs can serve as a proxy of ground truth answers (Wang et al. 2022), making it possible to use synthetic samples for training.\\n\\n\\nA comparison of accuracy of two $n$-digit numbers summation. With rationalization (CoT generation conditioned on ground truth), the model can learn complex arithmetic tasks like $5$-digit summation pretty early on. (Image source: Zelikman et al. 2022)\\n\\nScaling Laws for Thinking Time#\\nSo far we have seen much evidence that allowing models to spend additional compute on reasoning before producing final answers at inference time can significantly improve performance. Techniques like prompting the model to generate intermediate reasoning steps before the answers, or training the model to pause and reflect before predicting next tokens, have been found to boost the model performance beyond the capability limit obtained during training. This essentially introduces a new dimension to tinker with for improving model intelligence, complementing established factors such as model size, training compute and data quantity, as defined in scaling laws (Kaplan et al. 2020).\\nRecent studies demonstrated that optimizing LLM test-time compute could be more effective than scaling up model parameters (Snell et al. 2024, Wu et al. 2025). Smaller models combined with advanced inference algorithms can offer Pareto-optimal trade-offs in cost and performance.\\nSnell et al. (2024) evaluated and compared test-time and pretraining compute, and found that they are not 1:1 exchangeable. Test-time compute can cover up the gap easily on easy and medium questions when there is only a small gap in model capability, but proves less effective for hard problems. The ratio between token budgets for pretraining and inference matters a lot. Test-time compute is only preferable when inference tokens are substantially fewer than pretraining ones. This indicates that developing a capable base model with enough pretraining data and compute is still very critical, as test-time compute cannot solve everything or fill in big model capability gaps.\\n\\n\\n(Left) Evaluation accuracy as a function of test time compute budgets, via iterative revisions or parallel decoding. (Right) Comparing a small model with test-time compute sampling tricks and a 14x larger model with only greedy decoding. We can control the token budgets used at test time such that the ratio of inference to pretraining tokens is << 1, ~=1 or >> 1 and the benefit of test time compute is clear only the ratio << 1. (Image source: Snell et al. 2024)\\n\\ns1 models (Muennighoff & Yang, et al. 2025) experimented with scaling up the CoT reasoning path length via budget forcing technique (i.e. forcefully lengthen it by appending the word \"wait\", or shorten it by terminating the model’s thinking process by appending end-of-thinking token or \"Final Answer:\"). They observed a clear positive correlation between the average thinking time measured in tokens and the downstream evaluation accuracy.\\n\\n\\nBoth parallel and sequential scaling methods of test-time compute shows positive correlation with the evaluation performance in s1 experiments. (Image Muennighoff & Yang, et al. 2025)\\n\\nWhen comparing this budget forcing technique with other decoding methods of controlling reasoning trace length, it is quite surprising that simple rejection sampling (i.e. sampling generation until the lengths fits a token budget) leads to reversed scaling, meaning that longer CoTs lead to worse performance.\\n\\n\\n(Left) Longer CoT path length is positively correlated with eval accuracy. (Right) Rejection sampling for controlling the generated CoT path length shows a negative scaling where longer CoTs lead to worse eval accuracy. (Image source: Muennighoff & Yang et al. 2025)\\n\\nWhat’s for Future#\\nThe exploration of test-time compute and chain-of-thought reasoning presents new opportunities for enhancing model capabilities. More interestingly, via test-time thinking, we are moving towards building future AI systems that mirror the best practices of how humans think, incorporating adaptability, flexibility, critical reflection, and error correction. Excitement with current progress invites us for more future research to improve and understand deeply not just how but why we—and our models—think.\\nAt the end, I would like to call for more research for the following open research questions on test time compute and chain-of-thought reasoning.\\n\\nCan we incentivize the model to produce human-readable, faithful reasoning paths during RL training while avoiding reward hacking behavior?\\nHow to define reward hacking? Can we capture reward hacking during RL training or inference without human intervention? How to prevent “whack-a-mole” style of fixes for reward hacking during RL training?\\nSelf-correction can happen within chain-of-thought or can be encouraged to happen explicitly during multi-turn RL. How can we train the model to correct itself without hallucination or regression when ground truth is not available?\\nHow to run RL training with CoT rollout for tasks that are highly contextualized, personalized and hard to grade, such as creative writing, coaching, brainstorming?\\nWhen we deploy the model in reality, we cannot grow test time thinking forever and how can we smoothly translate the performance gain back into the base model with reduced inference time cost (e.g. via distillation)?\\nHow to make test time spending more adaptive according to the difficulty of the problem in hand?\\n\\nCitation#\\nPlease cite this work as:\\nWeng, Lilian. \"Why We Think\". Lil\\'Log (May 2025). https://lilianweng.github.io/posts/2025-05-01-thinking/\\nOr use the BibTex citation:\\n@article{weng2025think,\\n  title = {Why We Think},\\n  author = {Weng, Lilian},\\n  journal = {lilianweng.github.io},\\n  year = {2025},\\n  month = {May},\\n  url = \"https://lilianweng.github.io/posts/2025-05-01-thinking/\"\\n}\\nReferences#\\n[1] Alex Graves. “Adaptive Computation Time for Recurrent Neural Networks.”. arXiv preprint arXiv:1603.08983 (2016).\\n[2] Wang Ling, et al. “Program Induction by Rationale Generation: Learning to Solve and Explain Algebraic Word Problems.”. arXiv preprint arXiv:1705.04146 (2017).\\n[3] Karl Cobbe, et al. “Training Verifiers to Solve Math Word Problems.”. arXiv preprint arXiv:2110.14168 (2021).\\n[4] Jason Wei, et al. “Chain of Thought Prompting Elicits Reasoning in Large Language Models.”. NeurIPS 2022.\\n[5] Maxwell Nye, et al. “Show Your Work: Scratchpads for Intermediate Computation with Language Models.”. arXiv preprint arXiv:2112.00114 (2021).\\n[6] Daniel Kahneman. Thinking, Fast and Slow. Farrar, Straus and Giroux (2013).\\n[7] Takeshi Kojima, et al. “Large Language Models are Zero-Shot Reasoners.”. NeurIPS 2022.\\n[8] Michihiro Yasunaga, et al. “Large Language Models as Analogical Reasoners”. arXiv preprint arXiv:2310.01714 (2023).\\n[9] Eric Zelikman, et al. “STaR: Bootstrapping Reasoning With Reasoning.”. NeurIPS 2022.\\n[10] Xuezhi Wang, et al. “Self-consistency Improves Chain of Thought Reasoning in Language Models.”. ACL 2023.\\n[11] Ryo Kamoi, et al. “When Can LLMs Actually Correct Their Own Mistakes? A Critical Survey of Self-Correction of LLMs.”. TACL 2024.\\n[12] Jie Huang, et al. “Large Language Models Cannot Self-Correct Reasoning Yet.”. ICLR 2024.\\n[13] Noah Shinn, et al. “Reflexion: Language Agents with Verbal Reinforcement Learning.”. arXiv preprint arXiv:2303.11366 (2023).\\n[14] Yunxiang Zhang, et al. “Small Language Models Need Strong Verifiers to Self-Correct Reasoning.”. ACL Findings 2024.\\n[15] Hao Liu, et al. “Chain of Hindsight Aligns Language Models with Feedback.”. arXiv preprint arXiv:2302.02676 (2023).\\n[16] Sean Welleck, et al. “Generating Sequences by Learning to Self-Correct.”. arXiv preprint arXiv:2211.00053 (2023).\\n[17] Yuxiao Qu, et al. “Recursive Introspection: Teaching Language Model Agents How to Self-Improve.”. arXiv preprint arXiv:2407.18219 (2024).\\n[18] Aviral Kumar, et al. “Training Language Models to Self-Correct via Reinforcement Learning.”. arXiv preprint arXiv:2409.12917 (2024).\\n[19] Hunter Lightman, et al. “Let’s Verify Step by Step.”. arXiv preprint arXiv:2305.20050 (2023).\\n[20] Yuxi Xie, et al. “Self-Evaluation Guided Beam Search for Reasoning.”. NeurIPS 2023.\\n[21] Yangzhen Wu, et al. “Inference Scaling Laws: An Empirical Analysis of Compute-Optimal Inference for Problem-Solving with Language Models”. ICLR 2025.\\n[22] Dongwei Jiang, et al. “RATIONALYST: Pre-training Process-Supervision for Improving Reasoning”. arXiv preprint arXiv:2410.01044 (2024).\\n[23] Xuezhi Wang and Denny Zhou. “Chain-of-Thought Reasoning Without Prompting.”. arXiv preprint arXiv:2402.10200 (2024).\\n[24] DeepSeek-AI. “DeepSeek-V3 Technical Report.” arXiv preprint arXiv:2412.19437 (2024).\\n[25] DeepSeek-AI. “DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning.”. arXiv preprint arXiv:2501.12948 (2025).\\n[26] Luyu Gao, Aman Madaan & Shuyan Zhou, et al. “PAL: Program-aided Language Models.”. ICML 2023.\\n[27] Shunyu Yao, et al. “ReAct: Synergizing Reasoning and Acting in Language Models.”. ICLR 2023.\\n[29] Bowen Baker, et al. “Monitoring Reasoning Models for Misbehavior and the Risks of Promoting Obfuscation.”. arXiv preprint arXiv:2503.11926 (2025).\\n[30] Wojciech Zaremba, et al. “Trading Inference-Time Compute for Adversarial Robustness.”. arXiv preprint arXiv:2501.18841 (2025).\\n[31] Tamera Lanham, et al. “Measuring Faithfulness in Chain-of-Thought Reasoning”. arXiv preprint arXiv:2307.13702 (2023).\\n[32] Boshi Wang, et al. “Towards Understanding Chain-of-Thought Prompting: An Empirical Study of What Matters.”. ACL 2023.\\n[33] Miles Turpin, et al. “Language Models Don’t Always Say What They Think: Unfaithful Explanations in Chain-of-Thought Prompting.”. NeuriPS 2023.\\n[34] James Chua & Owain Evans. “Are DeepSeek R1 And Other Reasoning Models More Faithful?”. arXiv preprint arXiv:2501.08156 (2025).\\n[35] Yanda Chen et al. “Reasoning Models Don’t Always Say What They Think”. arXiv preprint arXiv:2505.05410 (2025).\\n[36] Edward Yeo, et al. “Demystifying Long Chain-of-Thought Reasoning in LLMs.”. arXiv preprint arXiv:2502.03373 (2025).\\n[37] Mostafa Dehghani, et al. “Universal Transformers.”. ICLR 2019.\\n[38] DeLesley Hutchins, et al. “Block-Recurrent Transformers.”. NeurIPS 2022.\\n[39] Aydar Bulatov, et al. “Recurrent Memory Transformers.”. NeuriPS 2022.\\n[40] Jonas Geiping, et al. “Scaling up Test-Time Compute with Latent Reasoning: A Recurrent Depth Approach.”. arXiv preprint arXiv:2502.05171 (2025).\\n[41] Herel & Mikolov. “Thinking Tokens for Language Modeling.”. AITP 2023.\\n[42] Sachin Goyal et al. “Think before you speak: Training Language Models With Pause Tokens.”. ICLR 2024.\\n[43] Eric Zelikman, et al. “Quiet-STaR: Language Models Can Teach Themselves to Think Before Speaking.”. arXiv preprint arXiv:2403.09629 (2025).\\n[44] Wangchunshu Zhou et al. “Towards Interpretable Natural Language Understanding with Explanations as Latent Variables.”. NeurIPS 2020.\\n[45] Du Phan et al. “Training Chain-of-Thought via Latent-Variable Inference.”. NeurIPS 2023.\\n[46] Yangjun Ruan et al. “Reasoning to Learn from Latent Thoughts.”. arXiv preprint arXiv:2503.18866 (2025).\\n[47] Xuezhi Wang et al. “Rationale-Augmented Ensembles in Language Models.”. arXiv preprint arXiv:2207.00747 (2022).\\n[48] Jared Kaplan, et al. “Scaling Laws for Neural Language Models.”. arXiv preprint arXiv:2001.08361 (2020).\\n[49] Niklas Muennighoff & Zitong Yang, et al. “s1: Simple test-time scaling.”. arXiv preprint arXiv:2501.19393 (2025).\\n[50] Peiyi Wang, et al. “Math-Shepherd: Verify and Reinforce LLMs Step-by-step without Human Annotations” arXiv preprint arXiv:2312.08935 (2023).\\n[51] Yixin Liu, et al. “Improving Large Language Model Fine-tuning for Solving Math Problems.” arXiv preprint arXiv:2310.10047 (2023).\\n[52] Charlie Snell, et al. “Scaling LLM Test-Time Compute Optimally can be More Effective than Scaling Model Parameters.”. arXiv preprint arXiv:2408.03314 (2024).\\n[53] OpenAI. o1-preview: “Learning to reason with LLMs.” Sep 12, 2024.\\n[54] OpenAI. o3: “Introducing OpenAI o3 and o4-mini.” Apr 16, 2025.\\n\\n\\n\\nLanguage-Model\\nReinforcement-Learning\\nReasoning\\nLong-Read\\n\\n\\n\\n »\\n\\nReward Hacking in Reinforcement Learning\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n© 2025 Lil\\'Log\\n\\n        Powered by\\n        Hugo &\\n        PaperMod\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n')]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "7a129047",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import SoupStrainer\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "loader = WebBaseLoader(\n",
    "    web_paths=['https://lilianweng.github.io/posts/2025-05-01-thinking/'],\n",
    "    bs_kwargs={\n",
    "        'parse_only': SoupStrainer(class_=[  'post-summary'])\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dc4fbc3",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d90d2cd9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'https://lilianweng.github.io/posts/2025-05-01-thinking/'}, page_content='')]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "233daca6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import ArxivLoader\n",
    "loader=ArxivLoader(query=\"\",max_results=5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7191e73",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'Published': '2023-06-07', 'Title': 'Changing Data Sources in the Age of Machine Learning for Official Statistics', 'Authors': 'Cedric De Boom, Michael Reusens', 'Summary': 'Data science has become increasingly essential for the production of official statistics, as it enables the automated collection, processing, and analysis of large amounts of data. With such data science practices in place, it enables more timely, more insightful and more flexible reporting. However, the quality and integrity of data-science-driven statistics rely on the accuracy and reliability of the data sources and the machine learning techniques that support them. In particular, changes in data sources are inevitable to occur and pose significant risks that are crucial to address in the context of machine learning for official statistics.\\n  This paper gives an overview of the main risks, liabilities, and uncertainties associated with changing data sources in the context of machine learning for official statistics. We provide a checklist of the most prevalent origins and causes of changing data sources; not only on a technical level but also regarding ownership, ethics, regulation, and public perception. Next, we highlight the repercussions of changing data sources on statistical reporting. These include technical effects such as concept drift, bias, availability, validity, accuracy and completeness, but also the neutrality and potential discontinuation of the statistical offering. We offer a few important precautionary measures, such as enhancing robustness in both data sourcing and statistical techniques, and thorough monitoring. In doing so, machine learning-based official statistics can maintain integrity, reliability, consistency, and relevance in policy-making, decision-making, and public discourse.'}, page_content='CHANGING DATA SOURCES IN THE AGE OF\\nMACHINE LEARNING\\nFOR OFFICIAL STATISTICS\\nPRESENTED AT UNECE MACHINE LEARNING FOR OFFICIAL STATISTICS WORKSHOP 2023\\nCedric De Boom\\nStatistics Flanders\\nBelgium\\ncedric.deboom@vlaanderen.be\\nMichael Reusens\\nStatistics Flanders\\nBelgium\\nmichael.reusens@vlaanderen.be\\nJune 6, 2023\\nABSTRACT\\nData science has become increasingly essential for the production of official statistics, as it enables\\nthe automated collection, processing, and analysis of large amounts of data. With such data science\\npractices in place, it enables more timely, more insightful and more flexible reporting. However, the\\nquality and integrity of data-science-driven statistics rely on the accuracy and reliability of the data\\nsources and the machine learning techniques that support them. In particular, changes in data sources\\nare inevitable to occur and pose significant risks that are crucial to address in the context of machine\\nlearning for official statistics.\\nThis paper gives an overview of the main risks, liabilities, and uncertainties associated with changing\\ndata sources in the context of machine learning for official statistics. We provide a checklist of the\\nmost prevalent origins and causes of changing data sources; not only on a technical level but also\\nregarding ownership, ethics, regulation, and public perception. Next, we highlight the repercussions\\nof changing data sources on statistical reporting. These include technical effects such as concept\\ndrift, bias, availability, validity, accuracy and completeness, but also the neutrality and potential\\ndiscontinuation of the statistical offering. We offer a few important precautionary measures, such as\\nenhancing robustness in both data sourcing and statistical techniques, and thorough monitoring. In\\ndoing so, machine learning-based official statistics can maintain integrity, reliability, consistency, and\\nrelevance in policy-making, decision-making, and public discourse.\\n1\\nIntroduction\\nThe field of statistics has long played a critical role in informing policy decisions, driving innovation, and advancing\\nscientific knowledge. Traditional statistical methods such as surveys and censuses have provided valuable insights into\\na wide range of topics, from population demographics to economic trends and public opinion. However, in recent years,\\nthe increasing availability of open and large data sources has opened up new opportunities for statistical analysis. In\\nparticular, the rise of machine learning has transformed the field of statistics, enabling the analysis of massive datasets,\\nthe identification of complex patterns and relationships, non-linear forecasting, etc. [1, 2]. Machine learning algorithms\\ncan be used to analyze data from a wide range of sources, providing insights that traditional survey methods may not\\ncapture.\\nThe use of machine learning for official statistics has the potential to provide more timely, accurate and comprehensive\\ninsights into a wide range of societal topics [3]. By leveraging the vast amounts of data that are generated by individuals\\nand entities on a daily basis, statistical agencies can gain a more nuanced understanding of trends and patterns, and\\nrespond more quickly to emerging issues.\\nHowever, this shift towards machine learning also presents a number of challenges. In particular, there are concerns\\nabout data quality, privacy, and security, as well as the need for appropriate technical skills and infrastructure [4, 5], as\\narXiv:2306.04338v1  [stat.ML]  7 Jun 2023\\nwell as challenges related to explainability, accuracy, reproducibility, timeliness, and cost effectiveness [6]. As statistical\\nagencies grapple with these challenges, it is essential to ensure that the benefits of machine learning are balanced\\nagainst the risks and that the resulting insights are both accurate and representative. In this paper, we explore the\\nchanging data sources in the age of machine learning for official statistics, as we believe that this pervasive issue largely\\nremains underexposed, as we will explain in Section 2.2. In that respect, we highlight some of the key considerations\\nfor statistical agencies looking to incorporate machine learning into their workflows in Section 3, by zooming in on the\\ncauses and risks associated with using external data sources, the consequences on using such sources for statistical\\nproduction, and, finally, a set of mitigations that should ideally be incorporated in any genuine deployment of machine\\nlearning for official statistics.\\n2\\nMachine learning for official statistics\\nThe data abundance in governmental, corporate, social and personal contexts, both online and offline, becomes a\\ntantalizing source and opportunity for the improvement and expansion of official statistics. For example, to inquire\\nabout the overall satisfaction with life of its citizens, a nation could organize periodic surveys. But when this nation has\\naccess to its citizens’ social media posts, likes, reader’s letters, media consumption, ticket sales, (online) shopping carts,\\netc. it could use all of these data as a proxy to extract novel and innovative statistical insights [7]. Typically, the end\\nresult is either a new statistic, a statistic that complements an existing one, or an ersatz statistic that aims to replace\\none or more existing statistics. We will briefly zoom in on the different components of which such a novel statistic is\\ngenerally comprised.\\n2.1\\nMachine learning\\nTo derive novel insights and innovative statistics from data, data scientists and statistical researchers often use a wide\\nvariety of powerful tools that are in the realm of machine learning1. In this paper we will not go into too much detail\\nabout machine learning. However, the use of data sources together with machine learning models can cause unwanted\\neffects regarding statistical production, see further in Section 3. So, we deem it useful to provide a high-level overview\\nof the typical process that involves machine learning for official statistics.\\nMachine learning is a subdiscipline of artificial intelligence that enables machines to learn from data and improve\\ntheir performance over time without being explicitly programmed. This approach involves building algorithms that\\nautomatically learn from data to identify patterns, relationships, and structures that may be difficult or impossible for\\nhumans to discern. The typical process of designing a machine learning algorithm consists of two main phases: training\\nand inference. During the training phase, the parameters of a machine learning model are tuned to solve a specific task.\\nFor this, a wide variety of data sources can be used, or even outputs from existing or pre-trained machine learning\\nmodels. After the model is trained, its parameters are kept fixed so that the model can be used to predict outcomes or\\nidentify patterns in new or previously unseen data. This process is called inference. It is important to keep in mind the\\ndistinction between training and inference. After training, the model remains unchanged, and it remains unchanged\\nuntil it is retrained again. Later, in Section 3, we will focus on the disparities that this can cause w.r.t. the inference\\nphase and, in consequence, official statistics production.\\nSupervised learning is one of the most common types of machine learning used for official statistics. This method\\ninvolves training a model on a labeled dataset, where each data point has a known outcome or target variable. The\\nmodel learns to associate features in the data with the target variable, enabling it to make predictions on new data with\\nsimilar features. In the context of official statistics, supervised learning can for example be used to predict the happiness\\nof an individual based on their Twitter profile [8]. Unsupervised learning, on the other hand, is used when the target\\nvariable is unknown or the goal is to identify patterns or relationships within the data. In this approach, the machine\\nlearning model learns to recognize similarities and differences among input data without explicit guidance from labeled\\ndata. In the context of official statistics, unsupervised learning can for example be used to identify citizens, companies\\nor events that are similar to each other on one or more aspects that could be hidden from plain sight [9].\\nMachine learning can be used to complement or even replace official statistics, and its ability to nowcast and forecast is\\nan extremely valuable addition. Modern machine learning models, tools and hardware can analyze vast amounts of data\\nin real-time or near-real-time, providing more up-to-date and precise estimates of e.g. economic and social trends. By\\n1Although originally (and technically) they imply different methods and techniques, the terms machine learning, data science,\\nartificial intelligence, deep learning... are nowadays considered interchangeable. In this paper we consistently use the term machine\\nlearning to denote the scientific discipline concerned with learning the most optimal model parameters based on data. It is a\\nsubdiscipline of artificial intelligence, while deep learning is a subdiscipline of machine learning. Data science encompasses both\\nmachine learning as well as data preparation, analytics and visualization.\\n2\\nincorporating machine learning into official statistical production, one can benefit from the strengths of both approaches\\nand make more informed decisions based on the most current and accurate data [10].\\n2.2\\nExternal data sources\\nLet’s focus on the data sources that will power such machine learning models.\\nTheir nature, size, structure,\\nfrequency... can be vastly different, they must typically be gathered ‘in the wild’ and should often be combined with\\neach other to extract meaningful insights. Compared to more traditional data sources for official statistics, they may\\npresent unique and appealing characteristics such as:\\nBroad-spectrum – Covers a wide variety of topics.\\nDiversity – A large variety of sources to cover different perspectives.\\nAvailability – Lots of data is freely and easily accessible.\\nSize – Some datasets can be enormous, sometimes even complete.\\nStructure – Not only tabular data, but also images, video, text, audio, etc.\\nTimeliness – (Near) up-to-date and real-time information.\\nFrequency – Raw data on various, even very fine-grained time scales.\\nGranularity – Raw data on various, even fine-grained levels of detail.\\nCoverage – Various locations and regions can be filtered and covered.\\nOn the other hand, before all this data is ready to be exerted for machine learning and official statistical production, a\\nfew challenges need to be overcome, such as:\\nData quality – Data may contain errors, biases, or missing values that need to be addressed to ensure accuracy and\\nreliability.\\nData interpretation – Understanding the context and meaning of data can be difficult, especially when dealing with\\nunstructured data such as text or images.\\nData integration – Combining data from different sources with varying structures and formats can be challenging and\\ntime-consuming.\\nSelection bias – Proper randomization or compiling representative population samples can be challenging, and it\\ngreatly depends on the underlying data origins.\\nOperationalization bias – Reproducibility can be difficult as it depends on many implicit, hidden, and/or production-\\nspecific design choices [11, 12].\\nComputational resources – Processing and analyzing large amounts of data may require significant computational\\nresources.\\nPrivacy and security – Sensitive data may need to be protected and anonymized to ensure privacy and security.\\nData ethics – Data collection and use should adhere to ethical principles.\\nFairness and justness – The end solution should ideally be as neutral as possible and should not discriminate [13].\\nCost – All of the above requires resources, budgets and a talented workforce. In addition, the data source itself might\\nneed to be purchased. In 2016, McKinsey reported that many companies have started to specialize in acquiring and\\nselling data [14].\\n3\\nWith the right tools, workforce, technological advances, mindset, and legislative support, these challenges can and\\nshould be manageable. The most challenging piece of the puzzle, however – and one that is more than often ignored –\\nis the lack of control you can exert over the data sources that are externally gathered. As a national statistics agency,\\ntraditionally, survey data and administrative records that power official statistics are completely under your own control.\\nBut once you start exploiting external data sources to power novel, innovative, complimentary or ersatz statistics, this\\nlack of control of your data should never be ignored, and if possible, should be front and center on your agenda early on\\nin the process.\\nAs the popular saying goes: “With great power comes great responsibility” (from Spider-Man, 2002). Having control\\nand power over your data is essential to fulfilling your responsibilities as a statistics agency. However, in the world of\\ndata, the opposite is often true: with great amounts of external data comes great powerlessness. Therefore, it is crucial\\nto prioritize the issue of data control when incorporating external sources into official statistics. Taking the time to\\nestablish proper protocols and procedures for external data management can prevent a multitude of issues down the line\\nand ensure that the data you rely on remain accurate and trustworthy.\\nThis paper delves into the pervasive problem of powerlessness and lack of control, unraveling the multifaceted aspects,\\nrisks, and pitfalls that arise from utilizing external data sources for machine learning in official statistics. We will\\nexplore the concepts of ‘change’ and ‘consequence’ in their most expansive interpretations to comprehensively tackle\\nthis question.\\n3\\nThe challenge of changing data sources\\nRelying heavily on external data sources for machine learning in official statistics comes with significant risks. Such a\\ndependence can leave statistical agencies vulnerable since they have limited control over these sources. This situation\\nis similar to how our global economy, mobility, and prosperity were once highly dependent on the availability of oil.\\nSince the prices and availability of these precious resources are often beyond our control, countries can do nothing but\\nendure price fluctuations and shortages. Clive Humby proclaimed in 2006 that “data is the new oil”, given its powerful\\nintrinsic value. However, his statement keeps holding true in terms of vulnerability, powerlessness, and lack of control\\nover external providers.\\nIn the following paragraphs, we will delve into the various types and causes of data changes. We will then discuss the\\nramifications of changing data sources for machine learning in official statistics. Finally, we will provide a list of best\\npractices and tips, although it is important to remember that there is no free lunch: whenever we incorporate external\\ndata, we expose ourselves to the risk of future changes in these data sources.\\n3.1\\nTypes and Causes of Changing Data Sources\\n3.1.1\\nData types and schemas\\nA change in data types or schemas refers to modifications made to the data formats or the structure in which the data\\nare stored and offered. These types of changes may arise due to a need to accommodate future use cases or business\\nrequirements, to eliminate technical debt, or to improve data storage and retrieval efficiency. Even the most innocent\\nchanges – e.g. integers becoming floats, data columns that are added or removed... – can break entire pipelines. In\\nthe most fortunate of cases the runtime environment will throw errors that reveal the cause of these data changes. In\\nother cases, however, the data changes remain undetected and secretly wreak havoc in the pipeline. If the pipeline\\ncontains machine learning components, data type changes can e.g. induce feature mismatches – discrepancies between\\nthe feature distributions at train and inference time – that lead to unreliable predictions.\\nIt is important to be vigilant about changes in data types or schemas, as even seemingly minor adjustments can have\\nsignificant impacts further down the data pipeline. To mitigate these risks, it is advisable to stay informed about data\\nchange announcements from providers and implement robust data checks during data ingestion, ranging from simple\\ndata (type) validation to full-blown automated feature analysis, outlier detection, etc. Additionally, the deployment of\\neffective monitoring systems can help catch machine learning failures quickly and prevent potentially costly errors.\\n3.1.2\\nSharing and collection technology\\nData can be shared and collected using many different technologies, such as APIs, queues, network drives, external\\ndrives, e-mail... but also web scraping, online analytics tools, sensor networks... Changes in these technologies inevitably\\noccur from time to time. For example, API endpoints often need to be updated to improve functionality and performance.\\nChanges may be made to the API’s data structures or methods, to provide more efficient or comprehensive data access.\\nIn addition, changes may be made to the API to address security vulnerabilities or to ensure compliance with new\\n4\\nregulations or standards. Furthermore, changes in business requirements or strategy may also lead to changes in API\\nendpoints. For instance, a company may introduce new products or services, modify their existing offerings, or change\\ntheir pricing.\\nA recent, telling example is the Twitter API. In 2021, Twitter launched version 2 of its popular API that introduced\\nmany changes in endpoints, data fields, pricing... compared to version 1.1. Twitter encouraged developers to migrate to\\nthis new API offering, but for many use cases such a migration would introduce breaking changes that, in their turn,\\nwould impact entire data processing pipelines, statistics production, etc. For the time being, Twitter offered both version\\n1.1 and version 2 of their API in parallel, which caused many to bury their heads in the sand. The situation got even\\nworse when Elon Musk acquired Twitter in 2022 and decided to suspend all existing API offerings. Instead, in 2023 a\\nnew enterprise tier was introduced that put a price of more than 40 thousand USD per month on any reasonably effective\\nuse of the API. This caused great dissatisfaction in the development and research community and many initiatives were\\nabandoned.\\n3.1.3\\nConcept drift\\nConcept drift is related to changes in the data distribution between train and test time, which can have multiple causes\\n[15, 16]. Changes in business logic can induce information shifts, for example, when categorical variables are expanded\\nwith additional categories or when the meaning of certain data fields is altered. A particular pervasive issue is the\\ncalculation of derived data fields, especially when those calculations are not transparent or proprietary. In the age of\\nmachine learning, you should always assume that derived data fields can be the result of a model prediction; when\\nthis model is updated without your knowing, the derived data fields will have a (slightly) different data distribution,\\nwhich will cause issues in dependent machine learning models. But even when data fields are not the result of a model’s\\nprediction, it is important to periodically reevaluate and retrain models, since many sociological and economic processes\\nare naturally prone to concept drift themselves.\\n3.1.4\\nFrequency and interruptions\\nA change in data frequency refers to modifications made to the rate at which data is collected or updated, which can\\nhappen deliberately or randomly. Deliberate changes may arise due to shifts in business requirements or technological\\nchoices. Random shifts are most often attributed to noisy factors such as network issues, component failures, down-\\ntime... or (human) errors. Such changes in frequency can impact machine learning components dramatically. E.g. when\\nperiodical data is sampled every minute instead of every second, the data distribution changes on which the model was\\ntrained. To mitigate these risks, data pipelines should be designed to monitor changes in incoming data frequency.\\n3.1.5\\nOwnership and discontinuation\\nWorse than interruptions is downright discontinuation of the data source, which has immediate consequences on the\\nfuture existence of the statistic. Also, a change of ownership of the data source – e.g. when acquired by another company\\n– is not a fictional scenario, and it can trigger any of the risks that are discussed in this section. Building redundancy by\\ndiversifying data sources is a useful mitigating strategy to avoid single points of failures.\\n3.1.6\\nLegal properties\\nLegal changes refer to modifications made to the legal landscape that governs the collection, storage, and use of data.\\nThis type of change may arise due to new privacy laws, contractual obligations, or changes in the cost of data access or\\nstorage. One cause of this type of change is the adoption of new regulations, such as GDPR, which require companies\\nto comply with stricter rules for collecting and processing data. Additionally, changes in the cost of data access or\\nstorage may require companies to modify their data sources or methods to reduce costs, which can have contractual\\nconsequences. If possible, negotiate airtight SLAs with the data provider and make sure to attribute enough attention to\\nfuture data changes.\\n3.1.7\\nEthics and public perception\\nEthical considerations and public perceptions can affect data collection methods and sources. If certain data sources\\nor variables are considered controversial or intrusive, there may be a shift towards alternative sources, which may\\nrequire a refresh of the used machine learning models. It can also impact the way machine learning models are designed\\nand trained. If certain variables or factors are considered discriminatory or unethical, there may be a push towards\\neliminating or adjusting them to reduce algorithmic bias. Changes in ethics or public perceptions can also result in\\ngreater accountability and the need for transparency. Stakeholders may demand more openness and clarity around\\nthe use of algorithms, data sources, and decision-making processes. This can lead to greater scrutiny and oversight of\\n5\\nmachine learning models, which may impact their performance if not adequately addressed, especially when black-box\\nmodels need to be replaced by more interpretative variants [17, 18, 19]. Finally, public trust can be significantly affected.\\nIf stakeholders perceive that machine learning is being used inappropriately or unethically, they may lose faith in the\\nintegrity and reliability of official statistics. This can have significant consequences for public policy and decision\\nmaking.\\n3.2\\nConsequences of Changing Data Sources\\nWhen data sources change, there will be consequences for official statistics production, especially if there are machine\\nlearning components involved. We will broadly but briefly cover a variety of areas that can be impacted, some of which\\nhave already been mentioned above.\\nConcept drift – Concept drift means that the underlying patterns and relationships in the data may change over time,\\nwhich can lead to model deterioration or loss of accuracy. This issue can be particularly relevant when dealing with\\nlong-term trends, as changes in societal norms, technology, or other external factors can influence data over time [16].\\nModel staleness – When a model becomes outdated, it no longer reflects current trends or patterns in the data. This can\\noccur if the machine learning model is not updated frequently enough to keep pace with changing data sources. As a\\nresult, the model may not perform as well as it once did, leading to less accurate official statistics.\\nBias and neutrality – Changing data sources can also introduce bias or incorrect data, which can impact the neutrality\\nof the statistics produced, or which can lead to the phenomenon “garbage in, garbage out” [20, 21]. Since it is essential\\nthat official statistics remain neutral and objective, this will negatively impact the accuracy and validity of these statistics.\\nAvailability – If data become unavailable (for a certain period in time or indefinitely) or are limited in scope, this may\\nimpact the ability to produce accurate and timely official statistics.\\nIntegration – A change in data sources can cause a domino effect when multiple statistics or models rely on this data\\nsource. Especially be mindful when the output of machine learning models is used as input for other machine learning\\nmodels, either directly or indirectly as part of a larger data pipeline. Since the predictions of a machine learning model\\ncan become unreliable when the input data change, this prediction shift itself is a changing data source for other models.\\nExtra labor – The risk of changing data sources requires additional resources and labor to mitigate the effects of such\\nchanges, monitor the occurrence of changes, and ensure that the new data are properly integrated into existing machine\\nlearning models. This has tremendous impacts on the costs and timeline of the produced statistics, and it may also\\nrequire a significant team expansion.\\nBreaking changes or discontinuation – In some cases, changing data sources may cause the impossibility of\\nproducing a statistic any further. If this is the case, it may be necessary to stop offering the statistic altogether or find\\nalternative data sources that can produce accurate and reliable official statistics. When alternative data sources are\\nfound, there will almost always be a mismatch with the original data source that has an impact on the resulting statistic,\\nresulting in a breaking change. In that case, it is important to overcome the mismatches as best as possible – e.g. in\\nterms of statistical properties – and, certainly, to be transparent about the breaking change, e.g. by indicating on a graph\\nwhen exactly the data source was changed.\\nQuality metrics – Finally, changing data sources imply changes in timeliness, validity, accuracy, completeness,\\nconsistency and other quality metrics w.r.t. the produced official statistics [6]. Ensuring that resulting statistic continues\\nto meet these quality metrics remains critical.\\n3.3\\nMitigating Changing Data Sources\\nAs has been illustrated above, changes in data sources can significantly impact the performance of a machine learning\\nmodel. The effects can be diverse, ranging from introducing biases in the data to producing incorrect results. This can\\nhave serious implications, especially when the model is being used for official statistics, where accuracy and reliability\\nare of paramount importance. Therefore, it is essential to take measures to prevent and mitigate such changes. This is\\nnot an easy task, as the consequences can be diverse, and the required efforts to mitigate them are often time-consuming\\nand not straightforward. We do not claim to have definite answers. However, we will propose several recommendations\\nand best practices, including performing a risk analysis, monitoring, diversifying data sources, building technical\\nrobustness, using data normalization techniques, and incorporating data validation processes.\\n6\\nRisk analysis – Performing a risk analysis before incorporating a new data source is an essential step in mitigating\\nthe impact of changes in data sources. This analysis involves identifying the potential risks associated with the data\\nsource, which we have covered in Section 3.1. The analysis should be comprehensive, considering both technical and\\nnon-technical aspects of the data source, and should ideally include potential solutions for the identified risks. This will\\noften force you to face the hard truth and will lead you to decide that the candidate data sources are not adequate or\\nreliable enough. Trade-offs will nevertheless need to be considered, depending on the use case at hand.\\nMonitoring – Monitoring everything that is relevant is another crucial step in mitigating the impact of changing data\\nsources. It involves tracking various aspects of the data sources, the machine learning models, and their outputs to\\ndetect and respond to changes promptly. Draft a list of variables and quantities that must be continuously tracked to\\nensure that the models remain reliable and accurate over time. For this, inspiration can be drawn from the discussed\\ntopics in Section 3.1, but it will vary from use case to use case, as well as the nature of the models that have been used.\\nSupervised models, for example, can be tested against a reference test set or a historical reference model; if the accuracy,\\nprecision or recall starts to deviate significantly from this reference set, it should be flagged. On the other hand,\\nmonitoring the performance of unsupervised models can be more challenging, because there is no clear performance\\nmeasure that can be directly computed. One approach is to monitor the model’s ability to detect patterns and clusters in\\nthe data. It is possible to use a reference test set or reference model for this, but the informative metrics – e.g. cluster\\nsimilarity, homogeneity, separation... – are more abstract and somewhat harder to interpret. Another approach is to\\nvisualize projections of certain interesting data points in the learned latent spaces or preferably a reduction thereof,\\nwhich greatly benefits interpretability but makes it harder to convert it into hard numbers. As a suggestion, a good\\nbalance between interpretability and hard performance metrics is found when clusters are tested against pre-existing\\ndomain knowledge, e.g. by listing similar data points for given queries. Simply monitoring whether expected similarities\\nemerge or not can provide powerful signals about model and data performance. Another effective approach is to create\\nproxy supervised tasks that rely on the output of the unsupervised model. Monitoring the model’s performance on such\\nproxy tasks can provide insights into the quality and usefulness of the unsupervised model’s output.\\nDiversification – Diversifying data sources is another important measure, but is easier said than done. One challenge of\\nusing multiple data sources is the potential for conflicts or inconsistencies between the sources. Different data sources\\nmay have different formats, schemas, and levels of quality, which can create discrepancies and inconsistencies that must\\nbe resolved before the data can be used in the model. Therefore, data normalization is key. Additionally, integrating\\nmultiple data sources can be a complex and time-consuming process. It can also create additional computational\\noverhead, which may impact the model’s scalability and portability. Finally, finding relevant and reliable data sources\\ncan be a challenging task, particularly for specialized or niche domains. It may require extensive research and\\ncommunication with data providers to retrieve relevant data. Again, this story is about economical, technical and\\npractical trade-offs, and is of course highly use-case-dependent.\\nTechnical robustness – Building technical robustness is paramount and requires significant engineering efforts.\\nBuilding an automated, data-driven statistic that is resistant to changing data sources such as errors, outliers, outages,\\ntime-dependent variability, etc. ensures consistency in the statistical offering. Using data normalization techniques and\\nincorporating data validation into the pipeline are essential measures, but robust technical implementations also require\\nthorough unit and integration testing, failover and deduplication, scalability solutions, security measures, etc. Of course,\\nthis is an entire field of study on its own.\\nLegal robustness – Finally, we believe that agreeing on clear legal guidelines is the best mitigation strategy to counter\\nthe risk of changing data sources, for example, by closing formal data sharing agreements or SLAa with data providers.\\nSuch agreements should specify the terms and conditions under which the data can be shared, as well as the legal\\nresponsibilities of each party. In particular, the agreements should specify the legal consequences of non-compliance.\\n4\\nConclusion\\nIn this paper we have investigated the risks and consequences of changing data sources when using machine learning\\nfor official statistics. The list is long and covers many different aspects, ranging from statistical issues and model\\ninconsistencies to technical problems and ethical considerations. We have also looked at a few potential mitigation\\nstrategies. However, we admit that these strategies do not provide all the adequate answers and might leave the reader\\nunsatisfied or, worse still, beguiled, as the solutions require many additional resources and efforts. As we have stressed\\na couple of times in this paper, this is a story of trade-offs. Depending on the use case at hand, some trade-offs might be\\neasier to handle than other ones. However, in the context of official statistics, our advice is to not tread lightly on these\\nmatters and to minimize the risk of losing control over your data sources as much as possible. This takes time, effort\\n7\\nand careful planning with a horizon of multiple years. To end on a positive note, despite the challenges associated with\\nchanging data sources, machine learning offers many opportunities for official statistics. By being aware of the risks\\nand taking necessary precautions, statistical agencies can leverage these opportunities while maintaining the integrity\\nand reliability of their data-driven products. We hope that our checklist of risks and mitigation strategies provides a\\nuseful starting point for statistical agencies and practitioners to ensure the robustness of their machine learning-based\\nstatistical reporting.\\nReferences\\n[1] Stuart J. Russell and Peter Norvig. Artificial Intelligence. Pearson Education, 2009.\\n[2] Trevor Hastie, Jerome Friedman, and Robert Tisbshirani. The elements of Statistical Learning. Springer, 2017.\\n[3] UNECE. Machine learning for official statistics. Technical report, UNECE, 2022.\\n[4] Hossein Hassani, Gilbert Saporta, and Emmanuel Sirimal Silva. Data mining and official statistics: The past, the\\npresent and the future. Big Data, 2(1):34–43, March 2014.\\n[5] Marco Puts and Piet Daas. Machine learning from the perspective of official statistic. The Survey Statistician,\\n84:12–17, July 2021.\\n[6] Wesley Yung, Siu-Ming Tam, Bart Buelens, Hugh Chipman, Florian Dumpert, Gabriele Ascari, Fabiana Rocci,\\nJoep Burger, and InKyung Choi. A quality framework for statistical algorithms. Statistical Journal of the IAOS,\\n38(1):291–308, March 2022.\\n[7] Wesley Yung. The Evolution of Official Statistics in a Changing World. Harvard Data Science Review, 3(4), oct\\n28 2021.\\n[8] Manon Reusens, Michael Reusens, Marc Callens, Bart Baesens, et al. Benchmark study for flemish twitter\\nsentiment analysis. Social Science Research Network, 2022.\\n[9] Annelien Crijns, Victor Vanhullebusch, Manon Reusens, Michael Reusens, and Bart Baesens. Topic modelling\\napplied on innovation studies of flemish companies. Journal of Business Analytics, pages 1–12, 2023.\\n[10] Sevgui Erman, Eric Rancourt, Yanick Beaucage, and Andre Loranger. The Use of Data Science in a National\\nStatistical Office. Harvard Data Science Review, 4(4), oct 27 2022.\\n[11] Matthias Haucke, Rink Hoekstra, and Don van Ravenzwaaij. When numbers fail: do researchers agree on\\noperationalization of published research? Royal Society Open Science, 8(9):191354, September 2021.\\n[12] Nagireddy Neelakanteswar Reddy. Operationalization bias: A suboptimal research practice in psychology.\\nDecember 2022.\\n[13] Matthias Kuppler, Christoph Kern, Ruben L. Bach, and Frauke Kreuter. From fair predictions to just decisions?\\nconceptualizing algorithmic fairness and distributive justice in the context of data-driven decision-making.\\nFrontiers in Sociology, 2022.\\n[14] Nicolaus Henke, Jacques Bughin, Michael Chui, James Manyika, Tamim Saleh, Bill Wiseman, and Guru\\nSethupathy. The age of analytics: competing in a data-driven world. Technical report, McKinsey & Company,\\n2016.\\n[15] Hanqing Hu, Mehmed Kantardzic, and Tegjyot S Sethi. No free lunch theorem for concept drift detection in\\nstreaming data classification: A review. Wiley Interdisciplinary Reviews: Data Mining and Knowledge Discovery,\\n10(2):e1327, 2020.\\n[16] Firas Bayram, Bestoun S. Ahmed, and Andreas Kassler. From concept drift to model degradation: An overview\\non performance-aware drift detectors. Knowledge-Based Systems, 245:108632, June 2022.\\n[17] Alicja Gosiewska, Anna Kozak, and Przemysław Biecek. Simpler is better: Lifting interpretability-performance\\ntrade-off via automated feature engineering. Decision Support Systems, 150:113556, November 2021.\\n[18] Cynthia Rudin. Stop explaining black box machine learning models for high stakes decisions and use interpretable\\nmodels instead. Nature Machine Intelligence, 1(5):206–215, May 2019.\\n[19] Alex John London. Artificial intelligence and black-box medical decisions: Accuracy versus explainability.\\nHastings Center Report, 49(1):15–21, January 2019.\\n[20] Bertie Vidgen and Leon Derczynski. Directions in abusive language training data, a systematic review: Garbage\\nin, garbage out. PLOS ONE, 15(12):e0243300, December 2020.\\n[21] R. Stuart Geiger, Dominique Cope, Jamie Ip, Marsha Lotosh, Aayush Shah, Jenny Weng, and Rebekah Tang.\\n“garbage in, garbage out” revisited: What do machine learning application papers report about human-labeled\\ntraining data? Quantitative Science Studies, 2(3):795–827, 2021.\\n8\\n'),\n",
       " Document(metadata={'Published': '2021-01-07', 'Title': 'DOME: Recommendations for supervised machine learning validation in biology', 'Authors': 'Ian Walsh, Dmytro Fishman, Dario Garcia-Gasulla, Tiina Titma, Gianluca Pollastri, The ELIXIR Machine Learning focus group, Jen Harrow, Fotis E. Psomopoulos, Silvio C. E. Tosatto', 'Summary': 'Modern biology frequently relies on machine learning to provide predictions and improve decision processes. There have been recent calls for more scrutiny on machine learning performance and possible limitations. Here we present a set of community-wide recommendations aiming to help establish standards of supervised machine learning validation in biology. Adopting a structured methods description for machine learning based on data, optimization, model, evaluation (DOME) will aim to help both reviewers and readers to better understand and assess the performance and limitations of a method or outcome. The recommendations are formulated as questions to anyone wishing to pursue implementation of a machine learning algorithm. Answers to these questions can be easily included in the supplementary material of published papers.'}, page_content='DOME: Recommendations for supervised machine \\nlearning validation in biology \\nIan Walsh\\u200b1,*\\u200b, Dmytro Fishman\\u200b2,*\\u200b, Dario Garcia-Gasulla\\u200b3\\u200b, Tiina Titma\\u200b4\\u200b, Gianluca Pollastri\\u200b5\\u200b, The ELIXIR \\nMachine Learning focus group\\u200b#\\u200b, Jennifer Harrow\\u200b6,+\\u200b, Fotis E. Psomopoulos\\u200b7,+\\u200b & Silvio C.E. Tosatto\\u200b8,+ \\n \\n1\\u200bBioprocessing Technology Institute, Agency for Science, Technology and Research, Singapore, \\u200b2\\u200bInstitute of \\nComputer Science, University of Tartu, Estonia,  \\u200b3\\u200bBarcelona Supercomputing Center (BSC), Barcelona, Spain, \\n4\\u200bSchool of Information Technologies, Tallinn University of Technology, Estonia, \\u200b5\\u200bSchool of Computer Science, \\nUniversity College Dublin, Ireland, \\u200b6\\u200bELIXIR HUB, South building, Wellcome Genome Campus, Hinxton, Cambridge, \\nUK, \\u200b7\\u200bInstitute of Applied Biosciences, Centre for Research and Technology Hellas, Thessaloniki, Greece, \\u200b8\\u200bDept. of \\nBiomedical Sciences, University of Padua, Padua, Italy. \\n*\\u200bcontributed equally \\n#\\u200bsee list of co-authors at the end of the manuscript \\n+\\u200bcorresponding authors \\n \\n \\n \\nAbstract \\nModern biology frequently relies on machine learning to provide predictions and improve decision\\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\nprocesses. There have been recent calls for more scrutiny on machine learning performance and possible\\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\nlimitations. Here we present a set of community-wide recommendations aiming to help establish\\n \\n \\n \\n  \\n \\n \\n \\n \\n \\n \\n \\n \\nstandards of supervised machine learning validation in biology. Adopting a structured methods\\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\ndescription for machine learning based on data, optimization, model, evaluation (DOME) will aim to help\\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n  \\n \\nboth reviewers and readers to better understand and assess the performance and limitations of a method\\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n  \\n \\nor outcome. The recommendations are formulated as questions to anyone wishing to pursue\\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\nimplementation of a machine learning algorithm. Answers to these questions can be easily included in the\\n \\n  \\n \\n \\n \\n  \\n \\n \\n \\n \\n \\n  \\n \\nsupplementary material of published papers.  \\n \\nIntroduction  \\nWith the steep decline in the cost of high-throughput technologies, large amounts of biological data are\\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\nbeing generated and made accessible to researchers. Machine learning (ML) has been brought into the\\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\nspotlight as a very useful approach to understand cellular\\u200b1\\u200b, genomic\\u200b2\\u200b, proteomic\\u200b3\\u200b, post-translational\\u200b4\\u200b,\\n \\n  \\n \\n \\n \\n \\n \\n \\n \\n \\n \\nmetabolic\\u200b5 and drug discovery data\\u200b6 with the potential to result in ground-breaking medical\\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\napplications\\u200b7,8\\u200b. This is clearly reflected in the corresponding growth of ML publications (Figure 1),\\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\nreporting a wide range of modelling techniques in biology. While every novel ML method should be\\n  \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\nvalidated experimentally, this happens only in a fraction of the publications\\u200b9\\u200b. This sharp increase in\\n \\n \\n \\n \\n \\n  \\n \\n \\n \\n \\n \\n \\n \\n \\npublications inherently requires a corresponding increase in the number and depth of peer-reviews to\\n \\n \\n  \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\noffer critical assessment\\u200b10\\u200b and improve reproducibility\\u200b11,12\\u200b.  \\nGuidelines or recommendations on how to appropriately construct ML algorithms can help to ensure\\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\ncorrect results and predictions\\u200b13,14\\u200b. In the biomedical research field, communities have defined standard\\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\nguidelines and best practices for scientific data management\\u200b15 and reproducibility of computational\\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\ntools\\u200b16,17\\u200b. On the other hand, a demand exists in the ML community for a cohesive and combined set of\\n \\n \\n \\n \\n  \\n \\n \\n \\n \\n \\n \\n  \\n \\n \\n \\n \\n \\n1 \\nrecommendations with respect to data, the optimization techniques, the final model, and evaluation\\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\nprotocols as a whole.  \\n \\n \\nFigure 1. \\u200bExponential increase of ML publications in biology. The number of ML publications per year is based\\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n  \\n \\non Web of Science from 1996 onwards using the “topic” category for “machine learning” in combination with each\\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n  \\n \\n \\n \\nof the following terms: “biolog*”, “medicine”, “genom*”, “prote*”, “cell*”, “post translational”, “metabolic” and\\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n“clinical”. \\n \\nRecently, a comment highlighted the need for standards in ML\\u200b18\\u200b, arguing for the adoption of\\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\non-submission checklists\\u200b10 as a first step towards improving publication standards. Through a\\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\ncommunity-driven consensus, we propose a list of minimal requirements asked as questions to ML\\n \\n \\n \\n  \\n \\n \\n \\n \\n \\n \\n \\n \\n \\nimplementers (Box 1) that, if followed, will help to assess the quality and reliability of the proposed\\n \\n \\n \\n  \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\nmethods more faithfully. We have focused on Data, Optimization, Model and Evaluation (DOME) as\\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\neach component of an ML implementation usually falls within one of these four topics. Importantly, no\\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\nspecific solutions are discussed, only recommendations (Table 1) and a checklist are provided (Box 1).\\n \\n \\n \\n \\n \\n \\n \\n \\n  \\n \\n \\n \\n \\n \\nOur recommendations are made primarily for the case of supervised learning in biology in the absence of\\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n  \\n  \\n \\n \\n \\ndirect experimental validation, as this is the most common type of ML approach used. We do not discuss\\n \\n \\n \\n \\n  \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\nhow ML can be used in clinical applications\\u200b19,20\\u200b. It also remains to be investigated if the DOME\\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\nrecommendations can be extended to other fields of ML, like unsupervised, semi-supervised and\\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\nreinforcement learning. \\n \\nDevelopment of the recommendations \\n \\nThe recommendations outlined below were initially formulated through the ELIXIR ML focus group\\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\nafter the publication of a comment calling for the establishment of standards for ML in biology\\u200b18\\u200b.\\n \\n \\n \\n  \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\nELIXIR, initially established in 2014, is now a mature intergovernmental European infrastructure for\\n \\n \\n \\n \\n \\n \\n  \\n \\n \\n \\n \\n \\nbiological data and represents over 220 research organizations in 22 countries across many aspects of\\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\nbioinformatics\\u200b21\\u200b. Over 700 national experts participate in the development and operation of national\\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\nservices that contribute to data access, integration, training and analysis for the research community. Over\\n \\n \\n  \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n50 of these experts involved in the field of ML have established an ELIXIR ML focus group\\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n2 \\n(\\u200bhttps://elixir-europe.org/focus-groups/machine-learning\\u200b) which held a number of meetings to develop\\n \\n \\n  \\n \\n \\n \\n \\n \\nand refine the recommendations based on a broad consensus among them.  \\n3 \\nBroad topic \\nBe on the \\nlookout for \\nConsequences \\nRecommendation(s) \\nData  \\n- Data size & \\nquality \\n \\n- Appropriate \\npartitioning, \\ndependence \\nbetween train \\nand test data.  \\n \\n- Class imbalance \\n \\n- No access to \\ndata \\n●Data not \\nrepresentative of \\ndomain application. \\n \\n●Unreliable or biased \\nperformance \\nevaluation.  \\n \\n●Cannot check data \\ncredibility. \\nIndependence of optimization (training) and evaluation \\n(testing) sets. (\\u200brequirement)  \\nThis is especially important for meta algorithms, where \\nindependence of multiple training sets must be shown to be \\nindependent of the evaluation (testing) sets.  \\n \\nRelease data preferably using appropriate long-term \\nrepositories, including exact splits (\\u200brequirement\\u200b) \\n \\nSufficient evidence of data size & distribution being \\nrepresentative of the domain\\u200b. (\\u200brecommendation\\u200b) \\n \\nOptimization - Overfitting, \\nunderfitting and \\nillegal parameter \\ntuning \\n \\n- Imprecise \\nparameters and \\nprotocols given. \\n●Reported \\nperformance too \\noptimistic or too \\npessimistic. \\n \\n●Models noise or miss \\nrelevant \\nrelationships. \\n \\n●Results are not \\nreproducible. \\nClear statement that evaluation sets were not used for feature \\nselection, pre-processing steps or parameter tuning. \\n(\\u200brequirement\\u200b) \\n \\nReporting indicators on training and testing data that can aid \\nin assessing the possibility of under/overfitting e.g. train vs. \\ntest error. (\\u200brequirement) \\n \\nRelease definitions of all algorithmic hyper-parameters, \\nregularization protocols, parameters and optimization \\nprotocol. (\\u200brequirement\\u200b) \\n \\nFor neural networks, release definitions of train and learning \\ncurves. (\\u200brecommendation\\u200b) \\n \\nInclude explicit model validation techniques, such as N-fold \\nCross validation. (\\u200brecommendation\\u200b)  \\nModel \\n- Unclear if black \\nbox or \\ninterpretable \\nmodel \\n \\n- No access to: \\nresulting source \\ncode, trained \\nmodels & data \\n \\n- Execution time \\nis impractical \\n●An interpretable \\nmodel shows no \\nexplainable \\nbehaviour \\n \\n●Cannot cross \\ncompare methods, \\nreproducibility,  & \\ncheck data \\ncredibility.  \\n \\n●Model takes too \\nmuch time to \\nproduce results \\nDescribe the choice of black box / interpretable model. If \\ninterpretable show examples of it doing so. (\\u200brequirement\\u200b).  \\n \\nRelease of: documented source code + models + executable \\n+ UI/webserver + software containers. (\\u200brecommendation\\u200b) \\n \\nReport execution time averaged across many repeats. If \\ncomputationally tough compare to similar methods \\n(\\u200brecommendation\\u200b) \\nEvaluation \\n- Performance \\nmeasures \\ninadequate \\n \\n- No comparisons \\nto baselines or \\nother methods \\n \\n●Biased performance \\nmeasures reported.  \\n \\n●The method is falsely \\nclaimed as \\nstate-of-the-art.  \\n \\nCompare with public methods & simple models (baselines). \\n(\\u200brequirement\\u200b)  \\n \\nAdoption of community validated measures and benchmark \\ndatasets for evaluation. (\\u200brequirement\\u200b) \\n \\nComparison of related methods and alternatives on the same \\ndataset. (\\u200brecommendation\\u200b) \\n \\nTable 1. Supervised ML in Biology: concerns, the consequences they impart and recommendations/requirements\\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n(recommendations in \\u200bitalics\\u200b and requirements in \\u200bbold\\u200b). Key terms underlined.  \\n \\n \\nBox 1: Structuring a Materials and Methods Section for Supervised Machine Learning \\n \\nHere we suggest a list of questions that must be asked about each DOME section to ensure high quality \\nof ML analysis. \\n●\\nData\\u200b: \\u200b(this section is to be repeated separately for each dataset) \\n○\\nProvenance\\u200b: What is the source of the data (database, publication, direct experiment)? If\\n \\n  \\n \\n \\n \\n \\n \\n \\n \\n \\n  \\ndata is in classes, how many data points are available in each class e.g., total for the\\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\npositive (N\\u200bpos\\u200b) and negative (N\\u200bneg\\u200b) cases? If regression, how many real value points are\\n \\n \\n \\n \\n \\n  \\n \\n \\n \\n \\n \\n \\n \\nthere? Has the dataset been previously used by other papers and/or is it recognized by the\\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n   \\n \\n \\n \\ncommunity? \\n○\\nData splits\\u200b: How many data points are in the training and test sets? Was a separate\\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n  \\n \\nvalidation set used, and if yes, how large was it? Is the distribution of data types in the\\n \\n \\n \\n  \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\ntraining and test sets different? Is the distribution of data types in both training and test\\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\nsets plotted?  \\n○\\nRedundancy between data splits\\u200b: How were the sets split? Are the training and test sets\\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\nindependent? How was this enforced (e.g. redundancy reduction to less than X% pairwise\\n \\n \\n \\n \\n \\n \\n \\n  \\n \\n \\n \\n \\nidentity)? How does the distribution compare to previously published ML datasets?  \\n○\\nAvailability of data\\u200b: Is the data, including the data splits used, released in a public forum?\\n \\n \\n  \\n \\n \\n \\n \\n \\n \\n \\n   \\n \\n \\nIf yes, where (e.g. supporting material, URL) and how (license)?  \\n \\n●\\nOptimization\\u200b: \\u200b(this section is to be repeated separately for each trained model) \\n○\\nAlgorithm\\u200b: What is the ML algorithm class used? Is the ML algorithm new? If yes, why is\\n \\n  \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n  \\n \\n  \\nit not published in a ML journal, and why was it chosen over better known alternatives?  \\n○\\nMeta-predictions\\u200b: Does the model use data from other ML algorithms as input? If yes,\\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n  \\n \\nwhich ones? Is it completely clear that training data of initial predictors and meta-predictor\\n \\n   \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\nis independent of test data for the meta-predictor?  \\n○\\nData encoding\\u200b: How was the data encoded and pre-processed for the ML algorithm? \\n○\\nParameters\\u200b: How many parameters (\\u200bp)\\u200b are used in the model? How was \\u200bp\\u200b selected? \\n○\\nFeatures\\u200b: How many features (\\u200bf) are used as input? Was feature selection performed? If\\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n  \\nyes, was it performed using the training set only?  \\n○\\nFitting\\u200b: Is the number of parameters (\\u200bp) much larger than the number of training points\\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\nand/or is the number of features (f) \\u200blarge (e.g. in classification is p>>(N\\u200bpos\\u200b+N\\u200bneg\\u200b) and/or\\n  \\n \\n \\n \\n \\n \\n \\n \\n \\n  \\n \\n \\nf>100)? If yes, how was over-fitting ruled out? Conversely, if the number of training\\n \\n \\n \\n \\n \\n \\n \\n \\n  \\n \\n \\n \\n \\npoints seem very much larger than p and/or \\u200bf is small (e.g. N\\u200bpos\\u200b+N\\u200bneg\\u200b>>p and/or f<5) how\\n \\n \\n \\n \\n \\n  \\n   \\n \\n \\n \\n \\n \\n \\nwas under-fitting ruled out?  \\n○\\nRegularization\\u200b: were any over-fitting prevention techniques performed (e.g. early stopping\\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\nusing a validation set)? If yes, which ones? \\n4 \\n- Highly variable \\nperformance.  \\n●Unpredictable \\nperformance in \\nproduction. \\nEvaluate performance on a final independent hold-out set. \\n(\\u200brecommendation\\u200b) \\n \\nConfidence intervals/error intervals and statistical tests to \\ngauge prediction robustness. (\\u200brequirement\\u200b)  \\n○\\nAvailability of configuration\\u200b: Are the hyper-parameter configurations, optimization\\n \\n \\n \\n \\n \\n \\n \\n \\nschedule, model files and optimization parameters reported available? If yes, where (e.g.\\n \\n \\n \\n \\n \\n \\n \\n  \\n \\n \\n \\nURL) and how (license)?  \\n \\n●\\nModel\\u200b: \\u200b(this section is to be repeated separately for each trained model) \\n○\\nInterpretability\\u200b: Is the model black box or transparent? If the model is transparent, can you\\n  \\n \\n \\n \\n \\n \\n  \\n \\n  \\n \\n \\n \\ngive clear examples for this?  \\n○\\nOutput: \\u200bIs the model classification or regression? \\n○\\nExecution time\\u200b: How much real-time does a single representative prediction require on a\\n \\n \\n \\n \\n \\n  \\n \\n \\n \\n \\n  \\nstandard machine? (e.g. seconds on a desktop PC or high-performance computing cluster)  \\n○\\nAvailability of software\\u200b: Is the source code released? Is a method to run the algorithm such\\n \\n \\n  \\n \\n \\n \\n   \\n  \\n \\n \\n \\n \\nas executable, web server, virtual machine or container instance released? If yes, where\\n \\n \\n \\n \\n \\n \\n \\n \\n \\n  \\n \\n \\n(e.g. URL) and how (license)?  \\n \\n●\\nEvaluation\\u200b:  \\n○\\nEvaluation method\\u200b: How was the method evaluated? (E.g. cross-validation, independent\\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\ndataset, novel experiments) \\n○\\nPerformance measures\\u200b: Which performance metrics are reported? Is this set representative\\n \\n \\n \\n \\n \\n \\n  \\n \\n \\n \\n(e.g. compared to the literature)?  \\n○\\nComparison\\u200b: Was a comparison to publicly available methods performed on benchmark\\n \\n  \\n \\n \\n \\n \\n \\n \\n \\n \\ndatasets? Was a comparison to simpler baselines performed?  \\n○\\nConfidence\\u200b: Do the performance metrics have confidence intervals? Are the results\\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\nstatistically significant to claim that the method is superior to others and baselines?  \\n○\\nAvailability of evaluation\\u200b: Are the raw evaluation files (e.g. assignments for comparison\\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\nand baselines, statistical code, confusion matrices) available? If yes, where (e.g. URL) and\\n \\n \\n \\n \\n \\n \\n  \\n \\n \\n \\n \\n \\nhow (license)? \\n \\nThe above description is shown in table format in the Supplementary Material together with two fully \\nworked-out examples. \\n \\n \\nScope of the recommendations  \\nThe recommendations cover four major aspects of supervised ML according to the “DOME” acronym:\\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\ndata, optimization, model and evaluation. The key points and rationale for each aspect of DOME is\\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n  \\ndescribed below and summarized in Table 1. More importantly, Box 1 gives an actionable checklist to\\n \\n \\n \\n \\n \\n \\n \\n \\n \\n  \\n \\n \\n \\n \\n \\nimplement ML methods, with the actual recommendations codified as questions. \\n \\n1. Data \\nState-of-the-art ML models are often capable of memorizing all the variation in training data. Such\\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\nmodels when evaluated on data that they were exposed to during training would create an illusion of\\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\nmastering the task at hand. However, when tested on an independent set of data (termed test or validation\\n \\n \\n  \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\nset), the performance would seem less impressive, suggesting low generalization power of the model. In\\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\norder to tackle this problem, initial data should be divided randomly into non-overlapping parts. The\\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\nsimplest approach is to have independent train and test sets (possibly a third validation set). Alternatively,\\n \\n   \\n \\n \\n \\n \\n \\n \\n  \\n \\n \\n \\n \\n5 \\nthe cross-validation or bootstrapping techniques which choose a new train/test split multiple times from\\n \\n \\n \\n \\n \\n \\n  \\n \\n \\n \\n \\n \\n \\nthe available data, is often considered a preferred solution\\u200b22\\u200b.  \\nOverlapping of train/test data splits are particularly troublesome to overcome in biology. For example, in\\n \\n \\n \\n \\n \\n \\n \\n  \\n  \\n \\n \\n  \\npredictions on entire gene and protein sequences independence of train-test could be achieved by\\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\nreducing the amount of homologs in the data\\u200b10,23\\u200b. Modelling enhancer-promoter contacts require a\\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n  \\ndifferent criterion, e.g., not sharing one endpoint\\u200b24\\u200b. Modeling protein domains might require the\\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\nmulti-domain sequence to be split into its constituent domains before homology reduction\\u200b25\\u200b. In short,\\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\neach area of biology has its own recommendations for handling overlapping data issues and previous\\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\nliterature is vital to put forward a strategy. In Box 1, we propose a set of questions under the section ‘data\\n  \\n \\n \\n \\n  \\n \\n \\n  \\n \\n  \\n \\n \\n \\n \\n \\n \\n \\nsplits’ that should help to evaluate potential overlap between train and test data.  \\nReporting statistics on the data size and distribution of data types can help show if there is a good domain\\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n  \\n   \\n \\n \\nrepresentation in all sets. Simple plots and/or tables showing the number of classes (classification),\\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\nhistogram of real values binned (regression), the different types of biological molecules in the data are\\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\nvital pieces of information for each set. Further, in classification, including methods that address\\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\nimbalanced classes\\u200b26,27 are also needed if the class frequencies show so. Models trained on one dataset\\n \\n \\n \\n \\n  \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\nmay not be successful in dealing with data coming from adjacent but not identical datasets, a\\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n  \\nphenomenon known as covariance shift. The scale of this effect has been demonstrated in several recent\\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n  \\n \\n \\npublications, e.g. for prediction of disease risk from exome sequencing\\u200b28\\u200b. Although up to now the\\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\ncovariance shift remains an open problem, several potential solutions have been proposed in the area of\\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\ntransfer learning\\u200b29\\u200b. Moreover, the problem of training ML models that can generalize well on small\\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\ntraining data, usually requires special models and algorithms\\u200b30\\u200b.  \\nLastly, it is important to make as much data available for the public as possible\\u200b12\\u200b. Having open access to\\n   \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n  \\nthe data used for experiments including precise data splits would ensure better reproducibility of\\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\npublished research and as a result will improve the overall quality of published ML papers. If datasets are\\n \\n \\n \\n  \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n  \\n \\n \\nnot readily available for example in public repositories, authors should be encouraged to find the most\\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\nappropriate one, e.g. ELIXIR Deposition databases or Zenodo, to guarantee the long-term availability of\\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\nsuch data. \\n \\n2. Optimization \\nOptimization, also known as training, refers to the process of changing values that constitute the model\\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n(parameters and hyper-parameters), including pre-processing steps, in a way that maximizes the model’s\\n \\n \\n \\n \\n \\n \\n  \\n \\n \\n \\n \\n \\nability to solve a given problem. A poor choice of optimization strategy may lead to issues such as over-\\n \\n \\n  \\n \\n  \\n \\n \\n \\n \\n \\n \\n  \\n \\n \\n \\n \\nor under-fitting\\u200b31\\u200b. A model that has suffered severe over-fitting will show an excellent performance on\\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\ntraining data, while performing poorly on unseen data, rendering it useless for real-life applications. On\\n \\n \\n \\n \\n \\n \\n \\n \\n  \\n \\n \\n \\n \\n \\nthe other side of the spectrum, under-fitting occurs when very simple models capable of ca\\u200bpturing only\\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\nstraightforward dependencies between features are applied to data of a more complex nature. Algorithms\\n \\n \\n \\n \\n \\n \\n \\n \\n  \\n \\n \\n \\n \\nfor feature selection\\u200b32 can be employed to reduce the chances of over-fitting. However, feature selection\\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\nand other pre-processing actions come with their own recommendations. The ma\\u200bin one being to abstain\\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\nfrom using non-training data for feature selection and pre-processing - a particularly hard issue to spot for\\n \\n \\n \\n \\n \\n \\n \\n \\n   \\n \\n \\n  \\n \\n \\nmeta-predictors, which may lead to an overestimation of performance.  \\nFinally, the release of files showing the exact specification of the optimization protocol and the type of\\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\nparameters/hyper-parameters are a vital characteristic of the final algorithm. Lack of documentation,\\n \\n  \\n \\n \\n \\n \\n \\n \\n \\n \\n \\nincluding limited accessibility to relevant records for the involved parameters, hyper-parameters and\\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\noptimization protocol may further compound the understanding of the overall model performance.  \\n \\n6 \\n \\n \\n \\nFigure 2. (Top) Classification metrics. \\u200bFor binary classification, true positives (tp), false positives (fp), false\\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\nnegatives (fn) and true negatives (tn) together form the confusion matrix. As all classification measures can be\\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\ncalculated from combinations of these four basic values, the confusion matrix should be provided as a core metric.\\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n  \\n \\n \\nSeveral measures (shown as equations) and plots should be used to evaluate the ML methods. For descriptions on\\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\nhow to adapt these metrics to multi-class problems see \\u200b35\\u200b. \\u200b(Bottom) Regression metrics. \\u200bML regression attempts\\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\nto produce predicted values (p) matching experimental values (e). Metrics (shown as equations) attempt to capture\\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n  \\n \\nthe difference in various ways. Alternatively, a plot can provide a visual way to represent the differences. It is\\n \\n \\n \\n \\n \\n  \\n \\n \\n  \\n \\n \\n \\n \\n \\n \\n  \\nadvisable to report all in any ML work.  \\n \\n7 \\n \\n3. Model \\nEqually important aspects related to ML models are their interpretability and reproducibility.\\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\nInterpretable models can infer causal relationships from the data and can output logical reasoning for\\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\neach of its predictions. They are especially relevant in areas of discovery such as drug design\\u200b6 and\\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\ndiagnostics\\u200b33\\u200b. Conversely, black box models often give accurate predictions but do not provide insight\\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\ninto why they made the predictions in a way humans can understand. Both interpretable and black box\\n \\n \\n \\n \\n \\n \\n  \\n \\n \\n \\n \\n \\n \\n \\n \\n \\nmodels are discussed in more detail elsewhere\\u200b34\\u200b. However, developing recommendations on the choice of\\n \\n \\n  \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\nblack box or interpretability cannot be made as both have their merits given certain situations. The main\\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\nrecommendation would be that there is a statement on the model type, i.e. is it black box or interpretable\\n \\n \\n \\n \\n   \\n \\n \\n \\n \\n \\n   \\n \\n \\n \\n \\n(see Box 1), and if it is interpretable it should be a requirement to give clear examples of it doing so.  \\nReproducibility is a key component to ensuring research outcomes can be further utilized and validated\\n   \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\nby the wider community. Poor model reproducibility extends beyond the documentation and reporting of\\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\nthe involved parameters, hyper-parameters and optimization protocol. Lacking access to the various\\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\ncomponents of a model (source code, model files, parameter configurations, executables), as well as\\n \\n  \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\nhaving steep computational requirements to execute the trained models in the context of generating\\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\npredictions based on new data, can both make reproducibility of the model either limited or impossible.  \\n \\n4. Evaluation \\nThere are two types of evaluation scenarios in biological research. The first is the experimental validation\\n \\n \\n \\n \\n \\n \\n  \\n \\n \\n \\n  \\n \\n \\n \\nof the predictions made by the ML model in the laboratory. This is highly desirable but beyond the\\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n  \\n \\n \\n \\n \\n \\npossibilities of many ML studies. The second is a computational assessment of the model performance\\n \\n \\n \\n \\n \\n \\n   \\n \\n \\n \\n \\n \\n \\nusing established metrics. The following deals with the latter and there are a few possible related risks\\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n  \\n \\n \\n \\n \\nwith computational evaluation.  \\nStarting with the performance metrics, i.e. the quantifiable indicators of a model\\'s ability to solve the\\n \\n \\n \\n \\n \\n \\n \\n \\n \\n  \\n \\n \\n \\n \\n \\ngiven task, there are dozens of metrics available\\u200b35 for assessing different ML classification and regression\\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\nproblems. However, the plethora of options available, combined with the domain-specific expertise that\\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\nmight be required to select the appropriate metrics, can lead to the selection of inadequate performance\\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\nmeasures. Often, there are critical assessment communities advocating certain performance metrics for\\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\nbiological ML models, e.g. CAFA\\u200b3 and CAGI\\u200b28\\u200b, and it is recommended a new algorithm should use ones\\n \\n \\n \\n \\n \\n \\n \\n   \\n  \\n \\n \\n \\n \\n \\nfrom the literature and critical assessments. In the absence of literature, the ones shown in Figure 2 could\\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n  \\n  \\n \\nbe a starting point. \\nOnce performance metrics are decided, methods published in the same biological domain must be\\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\ncross-compared using appropriate statistical tests (e.g. Student\\'s t-test) and confidence intervals.\\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\nAdditionally, to prevent the release of ML methods that appear sophisticated but perform no better than\\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\nsimpler algorithms, baselines should be compared to the ‘sophisticated’ method and proven to be\\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\nstatistically inferior (e.g. small vs. deep neural networks).  \\n \\nOpen areas and limitations of the proposed recommendations   \\nThe field of Machine Learning in biology is vast. As such, it would be a herculean task to identify\\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n  \\n \\n  \\n \\n \\n \\n \\ncross-cutting recommendations that could be applicable to all branches of ML across all possible\\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\ndomains.\\nTherefore,\\nthis\\nwork focuses exclusively on supervised ML applications, excluding\\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\nunsupervised ML, clinical applications, and individual implementations through pipelines to name a few.\\n \\n \\n \\n \\n \\n \\n \\n \\n  \\n  \\n \\nMoreover, the primary goal of this work is to define requirements and best practices that can be of use in\\n \\n \\n \\n \\n \\n \\n   \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n  \\nthe reviewing process and creation of ML-related papers, while remaining agnostic as to the actual\\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n8 \\nunderlying solutions. The intent is to trigger a discussion in the wider ML community leading to future\\n \\n \\n \\n  \\n \\n  \\n \\n \\n \\n \\n \\n \\n \\n \\n \\nwork addressing possible solutions.  \\nSeveral key issues related to reproducibility (e.g. data is not published, data splits are not reported and\\n \\n \\n \\n \\n \\n \\n \\n  \\n \\n \\n \\n \\n \\n \\n \\n \\nmodel source code with its final parameters and hyperparameters are not released) can be aided by a\\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n  \\nmultitude of workflow systems that help to ensure and automate multi-step processes are completely\\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\nreproducible by tracking model parameters and exact versions of the source code and libraries. Examples\\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\nof commonly used workflows include Galaxy\\u200b36 and Nextflow\\u200b37 . Another \\u200bde facto standard practice in\\n \\n \\n \\n \\n \\n \\n \\n  \\n \\n \\n \\n \\n \\n \\nsoftware engineering is using version control systems such as Github to create an online copy of the\\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\nsource code, which can also include parameters and documentation. Similar version control systems exist\\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\nfor datasets. Public repositories can store experimental data on demand for significant amounts of time,\\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\nenabling a long-term reproducibility of the experiment. Existing software engineering tools can be easily\\n  \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\nused to address many of the DOME recommendations.  \\nAlthough having additional, more topic-specific recommendations in the future will undoubtedly be\\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\nuseful, in this work we aim to provide a first version that could be of general interest. Adapting the\\n \\n \\n \\n \\n \\n \\n \\n  \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\nDOME recommendations to address the unique aspects of specific topics and domains, would be a task of\\n \\n  \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n  \\n \\n \\nthose particular (sub-)communities. For example, having guidelines for data independence is tricky\\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\nbecause each biological domain has its own set of guidelines for this. Nonetheless, we believe it is\\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n  \\n \\nrelevant to at least have a recommendation that authors describe how they achieved data split\\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\nindependence. Discussions on the correct independence strategies are needed for all of biology. \\u200bGiven\\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\nconstructive consultation processes with ML communities, relying on our own experience, it is our belief\\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n   \\n \\n \\nthat our manuscript can be useful as a first iteration of the recommendations for supervised ML in\\n \\n \\n \\n \\n \\n \\n  \\n \\n \\n \\n \\n \\n \\n \\n \\n \\nbiology\\u200b. This will have the additional benefit of kickstarting the community discussion with a coherent\\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n  \\n \\nbut rough set of goals, thus facilitating the overall engagement and involvement of key stakeholders. For\\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\ninstance, topics to be addressed by (sub-)communities is how to adapt DOME to entire pipelines,\\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\nunsupervised, semi-supervised, reinforcement, and other types of ML. E.g. in unsupervised learning, the\\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\nevaluation metrics shown in Figure 2 would not apply and a completely new set of definitions are needed.\\n \\n \\n  \\n  \\n \\n \\n \\n  \\n \\n \\n \\n \\n \\n \\n \\nAnother considerable debate, as AI becomes more commonplace in society, is that ML algorithms differ\\n \\n \\n \\n \\n \\n \\n \\n \\n \\n  \\n \\n \\n \\n \\nin their ability to explain learned patterns back to humans. Humans naturally prefer actions or predictions\\n \\n \\n \\n \\n \\n \\n \\n  \\n \\n \\n \\n \\n \\n \\n \\nto be made with reasons given. This is the black box vs. interpretability debate and we point those\\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\ninterested to excellent reviews as a starting point for thoughtful discussions\\u200b38–41\\u200b. \\nFinally, we address the governance structure by suggesting a community-managed governance model\\n \\n \\n \\n \\n \\n \\n \\n  \\n \\n \\n \\nsimilar to the open-source initiatives\\u200b42\\u200b. Community managed governance has been used in initiatives such\\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n  \\n \\n \\nas MIAME\\u200b43 or the PSI-MI format\\u200b44\\u200b. This sort of structure ensures continuous community consultation\\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\nand\\nimprovement\\nof\\nthe\\nrecommendations\\nin\\ncollaboration\\nwith\\nacademic\\n(CLAIRE,\\nsee:\\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\nhttps://claire-ai.org/) and industrial networks (Pistoia Alliance, see: https://www.pistoiaalliance.org/).\\n \\n \\n \\n \\n \\n \\n \\n \\nMore importantly, this can be applied in particular to ML (sub-)communities working with specific\\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\nproblems requiring more detailed guidelines, e.g. imaging or clinical applications. We have set up a\\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n  \\nwebsite (URL: https://www.dome-ml.org/) to provide a platform for governance and community\\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\ninvolvement around the DOME recommendations, where news and upcoming events will be posted. As\\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\nthe recommendations and minimal requirements evolve over time, a version history will be available on\\n \\n \\n \\n \\n \\n \\n \\n  \\n \\n \\n \\n \\n \\n \\nthe website. The template methods section in human (e.g. DOC) and machine readable format (YAML),\\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\nas well as software for the automatic conversion of a YAML file into a human readable one are available\\n \\n \\n \\n \\n \\n \\n \\n \\n  \\n \\n \\n  \\n \\n \\n \\n \\n \\nfrom a dedicated GitHub repository (URL: https://github.com/MachineLearning-ELIXIR/dome-ml). \\n \\nConclusion \\n \\n9 \\nThe objective of our recommendations is to increase the transparency and reproducibility of ML methods\\n \\n \\n \\n \\n   \\n \\n \\n \\n \\n \\n \\n \\n \\nfor the reader, the reviewer, the experimentalist, and the wider community. We recognize that these\\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\nrecommendations are not exhaustive and should be viewed as a consensus-based first iteration of a\\n \\n \\n \\n \\n \\n \\n \\n \\n  \\n \\n \\n \\n  \\ncontinuously evolving system of community self-review. One of the most pressing issues is to agree to a\\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n   \\n   \\nstandardized data structure to describe the most relevant features of the ML methods being presented. As\\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\na first step to address this issue, we recommend including an “ML summary table”, derived from Box 1,\\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n  \\nin future ML studies (see Supplementary Material). We recommend including the following sentence in\\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\nthe methods section of all papers: “To increase the reproducibility of the machine learning method of this\\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\nstudy, the machine learning summary table (Table X) is included in the supporting information as per\\n \\n \\n \\n \\n \\n \\n \\n  \\n \\n \\n \\n \\n \\n \\n \\nconsensus guidelines (with reference to this manuscript).”  \\nThe development of a standardized approach for reporting ML methods has major advantages in\\n \\n \\n  \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\nincreasing the quality of publishing ML methods. First, the disparity in manuscripts of reporting key\\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\nelements of the ML method can make reviewing and assessing the ML method challenging. Second,\\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\ncertain key statistics and metrics that may affect the validity of the publication’s conclusions are\\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\nsometimes not mentioned at all. Third, there are unexplored opportunities associated with meta-analyses\\n \\n \\n  \\n \\n \\n \\n \\n \\n \\n \\n \\n \\nof ML datasets. Access to large sets of data can enhance both the comparison between methods and\\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\nfacilitate the development of better-performing methods while reducing unnecessary repetition of data\\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\ngeneration. We believe that our recommendations to include a “machine learning summary table” and the\\n \\n \\n \\n \\n \\n  \\n  \\n \\n \\n \\n \\n \\n \\navailability of data as described above will greatly benefit the ML community and improve its standing\\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\nwith the intended users of these methods. \\n \\n \\nThe ELIXIR Machine Learning focus group \\n \\nEmidio Capriotti (ORCID: 0000-0002-2323-0963) \\nDepartment of Pharmacy and Biotechnology, University of Bologna, Bologna (Italy) \\n \\nRita Casadio (ORCID: 0002-7462-7039) \\nBiocomputing Group, University of Bologna, Italy; IBIOM-CNR,Italy  \\n \\nSalvador Capella-Gutierrez (ORCID: 0000-0002-0309-604X) \\nINB Coordination Unit, Life Science Department. Barcelona Supercomputing Center (BSC), Barcelona, Spain \\n \\nDavide Cirillo  (ORCID: 0000-0003-4982-4716) \\nLife Science Department. Barcelona Supercomputing Center (BSC), Barcelona, Spain \\n \\nAlexandros C. Dimopoulos (ORCID: 0000-0002-4602-2040) \\nInstitute for Fundamental Biomedical Science, Biomedical Sciences Research Center \"Alexander Fleming\",\\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\nAthens, Greece \\n \\nVictoria Dominguez Del Angel (ORCID: 0000-0002-5514-6651) \\nCentre National de Recherche Scientifique, University Paris-Saclay, IFB, France \\n \\nJoaquin Dopazo (ORCID: 0000-0003-3318-120X) \\nClinical Bioinformatics Area, Fundación Progreso y Salud, Sevilla, Spain \\n \\nPiero Fariselli (ORCID: 0000-0003-1811-4762) \\n10 \\nDepartment of Medical Sciences, University of Turin, Turin, Italy \\n \\nJosé Mª Fernández (ORCID: 0000-0002-4806-5140) \\nINB Coordination Unit, Life Sciences Department, Barcelona Supercomputing Center (BSC), Barcelona, Spain \\n \\nDmytro Fishman (ORCID: 0000-0002-4644-8893) \\nInstitute of Computer Science, University of Tartu, Estonia \\n \\nDario Garcia-Gasulla (ORCID: 0000-0001-6732-5641) \\nBarcelona Supercomputing Center (BSC), Barcelona, Spain \\n \\nJen Harrow (ORCID:0000-0003-0338-3070) \\nELIXIR HUB, South building, Wellcome Genome Campus, Hinxton, Cambridge, UK.  \\n \\nFlorian Huber (ORCID: 0000-0002-3535-9406) \\nNetherlands eScience Center, Amsterdam, the Netherlands. \\n \\nAnna Kreshuk (ORCID:0000-0003-1334-6388)  \\nEMBL Heidelberg \\n \\nTom Lenaerts (ORCID: 0000-0003-3645-1455) \\nMachine Learning Group, Université Libre de Bruxelles, Artificial Intelligence Lab, Vrije Universiteit Brussel and\\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\nInteruniversity Institute of Bioinformatics in Brussels,Brussels, Belgium.  \\n \\nPier Luigi Martelli (ORCID: 0000-0002-0274-5669) \\nBiocomputing Group, University of Bologna, Italy \\n \\nArcadi Navarro (ORCID: 0000-0003-2162-8246) \\nInstitute of Evolutionary Biology (Department of Experimental and Health Sciences, CSIC-Universitat Pompeu\\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\nFabra), Barcelona, Spain \\nCatalan Institution of Research and Advanced Studies (ICREA), Barcelona, Spain \\nCRG, Centre for Genomic Regulation, Barcelona Institute of Science and Technology (BIST), Barcelona, Spain \\n \\nMarco Necci (ORCID: 0000-0001-9377-482X) \\nDept. of Biomedical Sciences, University of Padua, Padua, Italy. \\n \\nPilib Ó Broin (ORCID: 0000-0002-6702-8564) \\nSchool of Mathematics, Statistics & Applied Mathematics, National University of Ireland Galway, Ireland \\n \\nJanet Piñero (ORCID: 0000-0003-1244-7654) \\nResearch Programme on Biomedical Informatics (GRIB), Hospital del Mar Medical Research Institute (IMIM), \\nDepartment of Experimental and Health Sciences, Pompeu Fabra University (UPF), Barcelona, Spain  \\n \\nDamiano Piovesan (ORCID: 0000-0001-8210-2390) \\nDept. of Biomedical Sciences, University of Padua, Padua, Italy. \\n \\nGianluca Pollastri (ORCID: 0000-0002-5825-4949) \\nSchool of Computer Science, University College Dublin, Ireland \\n \\n11 \\nFotis E. Psomopoulos (ORCID: 0000-0002-0222-4273) \\nInstitute of Applied Biosciences, Centre for Research and Technology Hellas, Thessaloniki, Greece \\n \\nMartin Reczko (ORCID: 0000-0002-0005-8718) \\nInstitute for Fundamental Biomedical Science, Biomedical Sciences Research Center \"Alexander Fleming\",\\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\nAthens, Greece \\n \\nFrancesco Ronzano (ORCID: 0000-0001-5037-9061) \\nResearch Programme on Biomedical Informatics (GRIB), Hospital del Mar Medical Research Institute (IMIM),\\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\nDepartment of Experimental and Health Sciences, Pompeu Fabra University (UPF), Barcelona, Spain \\n \\nVenkata Satagopam (ORCID: 0000-0002-6532-5880) \\nLuxembourg Centre For Systems Biomedicine (LCSB), University of Luxembourg and ELIXIR-Luxembourg\\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\nNode \\n \\nCastrense Savojardo (ORCID: 0000-0002-7359-0633) \\nBiocomputing Group, University of Bologna, Italy \\n \\nVojtech Spiwok (ORCID: 0000-0001-8108-2033) \\nDepartment of Biochemistry and Microbiology, University of Chemistry and Technology, Prague, ELIXIR-Czech \\nRepublic \\n \\nMarco Antonio Tangaro (ORCID: 0000-0003-3923-2266) \\nInstitute of Biomembranes, Bioenergetics and Molecular Biotechnologies, National Research Council (CNR), Bari, \\nItaly \\n \\nGiacomo Tartari \\nInstitute of Biomembranes, Bioenergetics and Molecular Biotechnologies, National Research Council (CNR), Bari, \\nItaly \\n \\nDavid Salgado (ORCID: 0000-0002-5905-3591) \\nAix Marseille Univ, INSERM, MMG UMR1251, 13005 Marseille, France. \\n \\nTiina Titma (ORCID: 0000-0002-4935-8914) \\nSchool of Information Technologies, Tallinn University of Technology, Estonia \\n \\nSilvio C. E. Tosatto (ORCID: 0000-0003-4525-7793) \\nDept. of Biomedical Sciences, University of Padua, Padua, Italy. \\n \\nAlfonso Valencia (ORCID:0000-0002-8937-6789) \\nCatalan Institution of Research and Advanced Studies (ICREA), Barcelona, Spain \\nLife Science Department. Barcelona Supercomputing Center (BSC), Barcelona, Spain \\n \\nIan Walsh (ORCID: \\u200b0000-0003-3994-5522\\u200b) \\nBioprocessing Technology Institute, Agency for Science, Technology and Research, Singapore \\n \\nFederico Zambelli (ORCID: 0000-0003-3487-4331) \\nDept. of Biosciences, University of Milan, Milan, Italy \\n \\n12 \\n \\nAuthor contributions \\nIW, DF, JH, FP and SCET guided the development, writing and final edits. All members of the ELIXIR\\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\nmachine learning focus group contributed to the discussions leading to the recommendations and writing\\n \\n \\n \\n \\n \\n \\n \\n \\n  \\n \\n \\n \\n \\nof the manuscript. \\n \\nCompeting interests \\nThe authors declare no competing interests. \\n \\nAcknowledgements \\nThe work of the ML focus group was funded by ELIXIR, the Research infrastructure for life-science\\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\ndata. IW was funded by Core Budget of Singapore Agency for Science Technology and Research\\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n(A*STAR). \\n \\n \\nReferences  \\n1.\\nBaron, C. S. \\u200bet al.\\u200b Cell Type Purification by Single-Cell Transcriptome-Trained Sorting. \\u200bCell \\u200b179\\u200b, \\n527-542.e19 (2019). \\n2.\\nLibbrecht, M. W. & Noble, W. S. Machine learning applications in genetics and genomics. \\u200bNat. Rev. \\nGenet. \\u200b16\\u200b, 321–332 (2015). \\n3.\\nRadivojac, P. \\u200bet al.\\u200b A large-scale evaluation of computational protein function prediction. \\u200bNat. \\nMethods \\u200b10\\u200b, 221–227 (2013). \\n4.\\nFranciosa, G., Martinez-Val, A. & Olsen, J. V. Deciphering the human phosphoproteome. \\u200bNat. \\nBiotechnol.\\u200b 1–2 (2020) doi:10.1038/s41587-020-0441-3. \\n5.\\nYang, J. H. \\u200bet al.\\u200b A White-Box Machine Learning Approach for Revealing Antibiotic Mechanisms of \\nAction. \\u200bCell \\u200b177\\u200b, 1649-1661.e9 (2019). \\n6.\\nVamathevan, J. \\u200bet al.\\u200b Applications of machine learning in drug discovery and development. \\u200bNat. \\nRev. Drug Discov. \\u200b18\\u200b, 463–477 (2019). \\n7.\\nRajkomar, A., Dean, J. & Kohane, I. Machine Learning in Medicine. \\u200bN. Engl. J. Med. \\u200b380\\u200b, \\n1347–1358 (2019). \\n8.\\nAscent of machine learning in medicine. \\u200bNat. Mater. \\u200b18\\u200b, 407 (2019). \\n9.\\nLittmann, M. \\u200bet al.\\u200b Validity of machine learning in biology and medicine increased through \\ncollaborations across fields of expertise. \\u200bNat. Mach. Intell. \\u200b2\\u200b, 18–24 (2020). \\n10. Walsh, I., Pollastri, G. & Tosatto, S. C. E. Correct machine learning on protein sequences: a \\n13 \\npeer-reviewing perspective. \\u200bBrief. Bioinform. \\u200b17\\u200b, 831–840 (2016). \\n11. Bishop, D. Rein in the four horsemen of irreproducibility. \\u200bNature \\u200b568\\u200b, 435 (2019). \\n12. Hutson, M. Artificial intelligence faces reproducibility crisis. \\u200bScience \\u200b359\\u200b, 725–726 (2018). \\n13. Schwartz, D. Prediction of lysine post-translational modifications using bioinformatic tools. \\u200bEssays \\nBiochem. \\u200b52\\u200b, 165–177 (2012). \\n14. Piovesan, D. \\u200bet al.\\u200b Assessing predictors for new post translational modification sites: a case study \\non hydroxylation. \\u200bbioRxiv\\u200b (2020). \\n15. Wilkinson, M. D. \\u200bet al.\\u200b The FAIR Guiding Principles for scientific data management and stewardship. \\nSci. Data \\u200b3\\u200b, 160018 (2016). \\n16. Sandve, G. K., Nekrutenko, A., Taylor, J. & Hovig, E. Ten Simple Rules for Reproducible \\nComputational Research. \\u200bPLOS Comput. Biol. \\u200b9\\u200b, e1003285 (2013). \\n17. Grüning, B. \\u200bet al.\\u200b Practical Computational Reproducibility in the Life Sciences. \\u200bCell Syst. \\u200b6\\u200b, 631–635 \\n(2018). \\n18. Jones, D. T. Setting the standards for machine learning in biology. \\u200bNat. Rev. Mol. Cell Biol. \\u200b20\\u200b, \\n659–660 (2019). \\n19. Norgeot, B. \\u200bet al.\\u200b Minimum information about clinical artificial intelligence modeling: the MI-CLAIM \\nchecklist. \\u200bNat. Med. \\u200b26\\u200b, 1320–1324 (2020). \\n20. Luo, W. \\u200bet al.\\u200b Guidelines for Developing and Reporting Machine Learning Predictive Models in \\nBiomedical Research: A Multidisciplinary View. \\u200bJ. Med. Internet Res. \\u200b18\\u200b, e323 (2016). \\n21. ELIXIR: Providing a Sustainable Infrastructure for Federated Access to Life Science Data at \\nEuropean Scale. \\u200bsubmitted\\u200b. \\n22. Kohavi, R. A study of cross-validation and bootstrap for accuracy estimation and model selection. in \\nvol. 14 1137–1145 (Montreal, Canada, 1995). \\n23. Hobohm, U., Scharf, M., Schneider, R. & Sander, C. Selection of representative protein data sets. \\nProtein Sci. \\u200b1\\u200b, 409–417 (1992). \\n24. Xi, W. & Beer, M. A. Local epigenomic state cannot discriminate interacting and non-interacting \\nenhancer-promoter pairs with high accuracy. \\u200bPLoS Comput. Biol. \\u200b14\\u200b, e1006625 (2018). \\n25. Zhou, X., Hu, J., Zhang, C., Zhang, G. & Zhang, Y. Assembling multidomain protein structures \\n14 \\nthrough analogous global structural alignments. \\u200bProc. Natl. Acad. Sci. \\u200b116\\u200b, 15930–15938 (2019). \\n26. Chawla, N. V., Bowyer, K. W., Hall, L. O. & Kegelmeyer, W. P. SMOTE: synthetic minority \\nover-sampling technique. \\u200bJ. Artif. Intell. Res. \\u200b16\\u200b, 321–357 (2002). \\n27. He, H., Bai, Y., Garcia, E. A. & Li, S. ADASYN: Adaptive synthetic sampling approach for \\nimbalanced learning. in 1322–1328 (IEEE, 2008). \\n28. Daneshjou, R. \\u200bet al.\\u200b Working toward precision medicine: Predicting phenotypes from exomes in the \\nCritical Assessment of Genome Interpretation (CAGI) challenges. \\u200bHum. Mutat. \\u200b38\\u200b, 1182–1192 \\n(2017). \\n29. Pan, S. J. & Yang, Q. A Survey on Transfer Learning. \\u200bIEEE Trans. Knowl. Data Eng. \\u200b22\\u200b, 1345–1359 \\n(2010). \\n30. Vinyals, O., Blundell, C., Lillicrap, T. & Wierstra, D. Matching networks for one shot learning. in \\n3630–3638 (2016). \\n31. Mehta, P. \\u200bet al.\\u200b A high-bias, low-variance introduction to machine learning for physicists. \\u200bPhys. Rep. \\n(2019). \\n32. Guyon, I. & Elisseeff, A. An introduction to variable and feature selection. \\u200bJ. Mach. Learn. Res. \\u200b3\\u200b, \\n1157–1182 (2003). \\n33. He, J. \\u200bet al.\\u200b The practical implementation of artificial intelligence technologies in medicine. \\u200bNat. Med. \\n25\\u200b, 30–36 (2019). \\n34. Rudin, C. Stop explaining black box machine learning models for high stakes decisions and use \\ninterpretable models instead. \\u200bNat. Mach. Intell. \\u200b1\\u200b, 206–215 (2019). \\n35. Baldi, P., Brunak, S., Chauvin, Y., Andersen, C. A. & Nielsen, H. Assessing the accuracy of \\nprediction algorithms for classification: an overview. \\u200bBioinforma. Oxf. Engl. \\u200b16\\u200b, 412–424 (2000). \\n36. Goecks, J., Nekrutenko, A., Taylor, J., & Galaxy Team. Galaxy: a comprehensive approach for \\nsupporting accessible, reproducible, and transparent computational research in the life sciences. \\nGenome Biol. \\u200b11\\u200b, R86 (2010). \\n37. Di Tommaso, P. \\u200bet al.\\u200b Nextflow enables reproducible computational workflows. \\u200bNat. Biotechnol. \\u200b35\\u200b, \\n316–319 (2017). \\n38. Arrieta, A. B. \\u200bet al.\\u200b Explainable Artificial Intelligence (XAI): Concepts, taxonomies, opportunities and \\n15 \\nchallenges toward responsible AI. \\u200bInf. Fusion \\u200b58\\u200b, 82–115 (2020). \\n39. Guidotti, R. \\u200bet al.\\u200b A survey of methods for explaining black box models. \\u200bACM Comput. Surv. CSUR \\n51\\u200b, 1–42 (2018). \\n40. Adadi, A. & Berrada, M. Peeking inside the black-box: A survey on Explainable Artificial Intelligence \\n(XAI). \\u200bIEEE Access \\u200b6\\u200b, 52138–52160 (2018). \\n41. Holm, E. A. In defense of the black box. \\u200bScience \\u200b364\\u200b, 26–27 (2019). \\n42. O’Mahony, S. The governance of open source initiatives: what does it mean to be community \\nmanaged? \\u200bJ. Manag. Gov. \\u200b11\\u200b, 139–150 (2007). \\n43. Brazma, A. \\u200bet al.\\u200b Minimum information about a microarray experiment (MIAME)—toward standards \\nfor microarray data. \\u200bNat. Genet. \\u200b29\\u200b, 365–371 (2001). \\n44. Hermjakob, H. \\u200bet al.\\u200b The HUPO PSI’s Molecular Interaction format—a community standard for the \\nrepresentation of protein interaction data. \\u200bNat. Biotechnol. \\u200b22\\u200b, 177–183 (2004). \\n \\n \\n \\n16 \\nSupplementary Material \\n \\nMachine learning summary table: \\n \\n17 \\nDOME \\nVersion \\n1.0 \\nData \\nProvenance \\nSource of data, data points (positive, N\\u200bpos\\u200b  / negative, N\\u200bneg\\u200b). Used by \\nprevious papers and/or community. \\n \\nDataset splits \\nSize of N\\u200bpos\\u200b and N\\u200bneg\\u200b of training set, validation set (if present), test set. \\nDistribution of N\\u200bpos\\u200b and N\\u200bneg\\u200b across sets. \\n(section to be \\nrepeated for each \\ndataset) \\nRedundancy \\nbetween data \\nsplits \\nIndependence between sets. Strategy used to make examples \\nrepresentative (e.g. eliminating data points more similar than X%). \\nComparison relative to other datasets. \\n \\nAvailability of \\ndata \\nYes/no for datasets. If yes: Supporting information, website URL, \\nlicense. \\nOptimization \\nAlgorithm \\nML class (e.g. neural network, random forest, SVM). If novel \\napproach, reason is it not previously published. \\n(section to be \\nrepeated for each \\ntrained model) \\nMeta-predictions \\nYes/No. If yes: how other methods are used and whether the datasets \\nare clearly independent. \\n \\nData encoding \\nHow input data is transformed (e.g. global features, sliding window on \\nsequence). \\n \\nParameters \\nNumber of ML model parameters (p), e.g. tunable weights in neural \\nnetworks. Protocol used to select p. \\n \\nFeatures \\nNumber of ML input features (f), i.e. encoding of data points. In case \\nof feature selection: Protocol used, indicating whether it was \\nperformed on training data only. \\n \\nFitting \\nJustification for excluding over- (if p >> N\\u200bpos, train\\u200b+N\\u200bneg, train\\u200b or f > 100) and \\nunder-fitting (if p << N\\u200bpos, train\\u200b+N\\u200bneg, train\\u200b or f < 5). \\n \\nRegularization \\nYes/no for overfitting prevention. If yes: specify details and parameters \\nof technique used. \\n \\nAvailability of \\nconfiguration \\nYes/no for hyper-parameter configuration and model files. If yes: \\nSupporting information, website URL, license. \\nModel \\nInterpretability \\nBlack box or transparent. If transparent, provide examples. \\n(section to be \\nrepeated for each \\ntrained model) \\nOutput \\nSpecify whether the model produces a classification (e.g. binary \\npredictions) or regression (e.g. probability score). \\n \\nExecution time \\nCPU time of single representative execution on standard hardware \\n(e.g. seconds on desktop PC). \\n \\nAvailability of \\nsoftware \\nSource code repository (e.g. GitHub), software container, website \\nURL, license. \\n \\n \\n \\nExample tables for author reference: \\n \\nThe following is an example for a primary ML summary table built from (Walsh et al., Bioinformatics \\n2012). \\n \\n18 \\nEvaluation \\nEvaluation \\nmethod \\nCross-validation, independent dataset or novel experiments. \\n \\nPerformance \\nmeasures \\nAccuracy, sensitivity, specificity, etc. \\n \\nComparison \\nName of other methods and, if available, definition of baselines \\ncompared to. Justification of representativeness. \\n \\nConfidence  \\nConfidence intervals and statistical significance. Justification for \\nclaiming performance differences. \\n \\nAvailability of \\nevaluation \\nYes/no for raw evaluation files (e.g. assignments for comparison and \\nbaselines, confusion matrices). If yes: Supporting information, website \\nURL, license. \\nDOME \\nVersion \\n1.0 \\nData: \\u200bX-ray \\ndisorder \\nProvenance \\nProtein Data Bank (PDB) X-ray structures until May 2008 (training) and \\nfrom May 2008 until September 2010 (test). 3,813 proteins total. N\\u200bpos\\u200b  = \\n44,433 residues. N\\u200bneg\\u200b = 710,207 residues. Not previously used. \\n \\nDataset splits \\nN\\u200bpos,train\\u200b  = 37,495. N\\u200bneg,train\\u200b = 622,625. N\\u200bpos,test\\u200b  = 6,938. N\\u200bneg,test\\u200b = 87,582 \\nresidues. No separate validation set. 5.68% positives on training set. \\n7.34% positives on test set. \\n \\nRedundancy \\nbetween data splits \\nMaximum pairwise identity within and between training and test set is \\n25% enforced with UniqueProt tool. \\n \\nAvailability of data \\nYes, URL: http://protein.bio.unipd.it/espritz/.  \\nFree use license. \\nData: \\nDisProt \\ndisorder \\nProvenance \\nDisProt version 3.7 (January 2008) for training set, DisProt version 5.7 \\nfor test set. 536 proteins total. N\\u200bpos\\u200b  = 63,841 residues. N\\u200bneg\\u200b = 164,682 \\nresidues. Not previously used. \\n \\nDataset splits \\nN\\u200bpos,train\\u200b  = 56,414. N\\u200bneg,train\\u200b = 163,010. N\\u200bpos,test\\u200b  = 7,427. N\\u200bneg,test\\u200b = 1,672 \\nresidues. No separate validation set. 25.71% positives on training set. \\n41.04% positives on test set. \\n \\nRedundancy \\nbetween data splits \\nMaximum pairwise identity within and between training and test set is \\n40% enforced with UniqueProt tool. Less stringent threshold used to \\nmaintain adequate dataset size. \\n \\nAvailability of data \\nYes, URL: http://protein.bio.unipd.it/espritz/. \\nFree use license. \\n19 \\nData: \\u200bNMR \\ndisorder \\nProvenance \\nProtein Data Bank (PDB) NMR structures until May 2008 (training) and \\nfrom May 2008 until September 2010 (test) analyzed using the Mobi \\nsoftware. 2,858 proteins total. N\\u200bpos\\u200b  = 40,368 residues. N\\u200bneg\\u200b = 192,170 \\nresidues. Not previously used. \\n \\nDataset splits \\nN\\u200bpos,train\\u200b  = 29,263. N\\u200bneg,train\\u200b = 143,891. N\\u200bpos,test\\u200b  = 11,105. N\\u200bneg,test\\u200b = 48,279 \\nresidues. No separate validation set. 16.9% positives on training set. \\n18.7% positives on test set. \\n \\nRedundancy \\nbetween data splits \\nMaximum pairwise identity within and between training and test set is \\n25% enforced with UniqueProt tool. \\n \\nAvailability of data \\nYes, URL: http://protein.bio.unipd.it/espritz/. \\nFree use license. \\nOptimization Algorithm \\nBRNN (Bi-directional recurrent neural network) with ensemble \\naveraging. \\n \\nMeta-predictions \\nNo. \\n \\nData encoding \\nSliding window of length 23 residues on input sequence with “one hot” \\nencoding (i.e. 20 inputs per residue). \\n \\nParameters \\nESpritz p = 4,647 to 5,886 depending on model used. No optimization. \\n \\nFeatures \\nESpritz f = 460 for sliding window of 23 residues with 20 inputs per \\nresidue. No feature selection. \\n \\nFitting \\nThe number of training examples is between 30 and 100 times p, \\nsuggesting neither over- nor under-fitting. \\n \\nRegularization \\nNo. The training data is 30-100 times the number of model parameters. \\n \\nAvailability of \\nconfiguration \\nNo. \\nModel \\nInterpretability \\nBlack box, as correlation between input and output is masked. No \\nattempt was made to make the model transparent.  \\n \\nOutput \\nRegression, i.e. probability of residues being disordered. \\n \\nExecution time \\nESpritzS ca. 1 sec / protein, ESpritzP ca. 1,500 sec / protein on a \\nsingle Intel Xeon core. \\n \\nAvailability of \\nsoftware \\nWeb server, URL: http://protein.bio.unipd.it/espritz/ \\nLinux executable, URL: http://protein.bio.unipd.it/download/. \\nBespoke license free for academic use. \\nEvaluation \\nEvaluation method \\nIndependent datasets. \\n \\nPerformance \\nmeasures \\nAccuracy, sensitivity, specificity, selectivity, F-measure, MCC, AUC are \\nstandard. S\\u200bw\\u200b = Sens + Spec -1. \\n \\nComparison \\nDisopred, MULTICOM, DisEMBL, IUpred, PONDR-FIT, Spritz, CSpritz. \\nWide range of popular predictors used for comparison. \\n \\nConfidence  \\nBootstrapping was used to estimate statistical significance as in \\nCASP-8 (Noivirt-Brik et al, Proteins 2009). 80% of target proteins were \\nrandomly selected 1000 times, and the standard error of the scores \\nwas \\n \\n \\n \\nThe following is an example for meta-predictor ML summary built from (Necci et al., Bioinformatics \\n2017). \\n \\n20 \\ncalculated (i.e. 1.96 × standard_error gives 95% confidence around \\nmean for normal distributions). \\n \\nAvailability of \\nevaluation \\nNo. \\nDOME \\nVersion \\n1.0 \\nData \\nProvenance \\nProtein Data Bank (PDB). X-ray structures missing residues. N\\u200bpos\\u200b = \\n339,603 residues. N\\u200bneg\\u200b = 6,168,717 residues. Previously used in \\n(Walsh et al., Bioinformatics 2015) as an independent benchmark set. \\n \\nDataset splits \\ntraining set: N/A \\nN\\u200bpos,test\\u200b = 339,603 residues. N\\u200bneg,test\\u200b = 6,168,717 residues. No validation \\nset. 5.22% positives on the test set. \\n \\nRedundancy \\nbetween data splits \\nNot applicable. \\n \\nAvailability of data \\nYes, URL: http://protein.bio.unipd.it/mobidblite/. \\nFree use license. \\nOptimization Algorithm \\nMajority-based consensus classification based on 8 primary ML \\nmethods and post-processing. \\n \\nMeta-predictions \\nYes, predictor output is a binary prediction computed from the \\nconsensus of other methods; Independence of training sets of other \\nmethods with test set of meta-predictor was not tested since datasets \\nfrom other methods were not available. \\n \\nData encoding \\nLabel-wise average of 8 binary predictions. \\n \\nParameters \\np = 3 (Consensus score threshold, expansion-erosion window, length \\nthreshold). No optimization. \\n \\nFeatures \\nNot applicable. \\n \\nFitting \\nSingle input ML methods are used with default parameters. \\nOptimization is a simple majority. \\n \\nRegularization \\nNo.  \\n \\nAvailability of \\nconfiguration \\nNot applicable. \\nModel \\nInterpretability \\nTransparent, in so far as meta-prediction is concerned. Consensus and \\npost processing over other methods predictions (which are mostly \\nbalck boxes). No attempt was made to make the meta-prediction a \\nblack box.  \\n \\nOutput \\nClassification, i.e. residues thought to be disordered. \\n \\n \\n21 \\n \\nExecution time \\nca. 1 second per representative on a desktop PC. \\n \\nAvailability of \\nsoftware \\nYes, URL: http://protein.bio.unipd.it/mobidblite/. \\nBespoke license free for academic use. \\nEvaluation \\nEvaluation method \\nIndependent dataset \\n \\nPerformance \\nmeasures \\nBalanced Accuracy, Precision, Sensitivity, Specificity, F1, MCC. \\n \\nComparison \\nDisEmbl-465, DisEmbl-HL, ESpritz Disprot, ESpritz NMR, ESpritz \\nXray, Globplot, IUPred long, IUPred short, VSL2b. Chosen methods \\nare the methods from which the meta prediction is obtained. \\n \\nConfidence  \\nNot calculated. \\n \\nAvailability of \\nevaluation \\nNo. \\n'),\n",
       " Document(metadata={'Published': '2025-01-28', 'Title': 'Learning Curves for Decision Making in Supervised Machine Learning: A Survey', 'Authors': 'Felix Mohr, Jan N. van Rijn', 'Summary': 'Learning curves are a concept from social sciences that has been adopted in the context of machine learning to assess the performance of a learning algorithm with respect to a certain resource, e.g., the number of training examples or the number of training iterations. Learning curves have important applications in several machine learning contexts, most notably in data acquisition, early stopping of model training, and model selection. For instance, learning curves can be used to model the performance of the combination of an algorithm and its hyperparameter configuration, providing insights into their potential suitability at an early stage and often expediting the algorithm selection process. Various learning curve models have been proposed to use learning curves for decision making. Some of these models answer the binary decision question of whether a given algorithm at a certain budget will outperform a certain reference performance, whereas more complex models predict the entire learning curve of an algorithm. We contribute a framework that categorises learning curve approaches using three criteria: the decision-making situation they address, the intrinsic learning curve question they answer and the type of resources they use. We survey papers from the literature and classify them into this framework.'}, page_content='Springer Nature 2021 LATEX template\\nLearning Curves for Decision Making in\\nSupervised Machine Learning: A Survey\\nFelix Mohr1* and Jan N. van Rijn2\\n1Universidad de La Sabana, Ch´ıa, Cundinamarca, Colombia.\\n2Leiden Institute of Advanced Computer Science, Leiden\\nUniversity, Leiden, The Netherlands.\\n*Corresponding author(s). E-mail(s):\\nfelix.mohr@unisabana.edu.co;\\nContributing authors: j.n.van.rijn@liacs.leidenuniv.nl;\\nAbstract\\nLearning curves are a concept from social sciences that has been adopted\\nin the context of machine learning to assess the performance of a learn-\\ning algorithm with respect to a certain resource, e.g., the number of\\ntraining examples or the number of training iterations. Learning curves\\nhave important applications in several machine learning contexts, most\\nnotably in data acquisition, early stopping of model training, and model\\nselection. For instance, learning curves can be used to model the per-\\nformance of the combination of an algorithm and its hyperparameter\\nconfiguration, providing insights into their potential suitability at an\\nearly stage and often expediting the algorithm selection process. Var-\\nious learning curve models have been proposed to use learning curves\\nfor decision making. Some of these models answer the binary deci-\\nsion question of whether a given algorithm at a certain budget will\\noutperform a certain reference performance, whereas more complex mod-\\nels predict the entire learning curve of an algorithm. We contribute a\\nframework that categorises learning curve approaches using three cri-\\nteria: the decision-making situation they address, the intrinsic learning\\ncurve question they answer and the type of resources they use. We\\nsurvey papers from the literature and classify them into this framework.\\nKeywords: learning curves, supervised machine learning\\n1\\narXiv:2201.12150v2  [cs.LG]  28 Jan 2025\\nSpringer Nature 2021 LATEX template\\n2\\nLearning Curves for Decision Making\\n1 Introduction\\nLearning curves describe a system’s performance on a task as a function of\\nsome resource to solve that task. There can be a pre-defined budget of that\\nresource, limiting the amount of resources that can be spent. In other cases, the\\ngoal can be to obtain reasonable results while minimising the spent budget of\\nthat resource. Typical types of budgets are the number of examples the learner\\nhas observed before performing the task or the number of iterations or time\\nthe learner spends in an environment. The performance measure expresses the\\nquality of the obtained model, e.g., error rate or F1 measure. Learning curves\\nare an important source of information for making decisions on the following\\nmatters in machine learning:\\n• Data Acquisition determines how many data points should reasonably be\\nacquired to obtain a desired performance. The top right plot in Fig. 1 visu-\\nalises a scenario where we have already observed performance up to a certain\\namount of data (the blue learning curve). We can extrapolate this and make\\na prediction of what the performance would be if more data was available,\\ni.e., the value of the orange extrapolation at different vertical pink lines in\\nthe figure (see, e.g., Last, 2009; Weiss and Tian, 2008).\\n• Early Stopping of training a model. If we are committed to some specific\\nlearner (a learning algorithm and its hyperparameters), we might want to\\nminimise the training time (John and Langley, 1996; Provost et al, 1999)\\nor avoid over-fitting (Bishop, 1995; Goodfellow et al, 2016). The middle\\nright plot in Fig. 1 visualises a scenario where we have already observed\\nperformance up to a certain amount of budget, and based on the progression\\nof the learning curves on recent iterations, a decision can be made whether\\nto continue the learning or terminate it.\\n• Early Discarding in model selection. If we want to select from various mod-\\nels, we want to stop the evaluation of a candidate when we are reasonably\\ncertain that it is not competitive to the best-known solution (Domhan et al,\\n2015; Mohr and van Rijn, 2023; Swersky et al, 2014). The bottom right plot\\nof Fig. 1 visualises a scenario where we have already observed performance\\nup to a certain amount of budget (the blue learning curve) and already\\nan incumbent performance obtained by an earlier configuration (horizon-\\ntal dashed pink line). By using learning curve extrapolation techniques, we\\ncan determine whether the current configuration can surpass the incumbent\\nconfiguration; if not, discarding the current training process early (as in the\\ncase shown) would be justified.\\nMany techniques with varying complexity and required resources have been\\nproposed to address either of these problems. The complexity ranges from\\napproaches that simply recognise whether an already observed part of a learn-\\ning curve has converged (Bishop, 1995; Provost et al, 1999) to the creation\\nof parametric learning curve models, which capture a belief model for the\\nbehaviour of various learners at any possible budget (Klein et al, 2017b). While\\nsimple approaches may only rely on the observations made so far for a learner\\nSpringer Nature 2021 LATEX template\\nLearning Curves for Decision Making\\n3\\nFig. 1: The three types of decision-making situations in which learning curves\\nare typically used. The x-axis of each figure represents the budget in the appli-\\ncable unit, and the y-axis represents the performance.\\non the dataset of interest, more complex approaches may rely on additional\\nresources such as learning curves or features of other datasets (see, e.g., Leite\\nand Brazdil, 2010) or learners (see, e.g., Chandrashekaran and Lane, 2017) or\\nboth (see, e.g., Ruhkopf et al, 2023).\\nOur contribution is a unified framework of the usage of learning curves for\\ndecision making in machine learning and an extensive review of the literature\\nof approaches that fall within this framework. This framework categorises the\\nexisting literature along the following three axes:\\n1. The type of decision-making situation, i.e., whether it is used to make\\ndecisions about data acquisition, early stopping, or early discarding. See\\nSec. 4.1 for more details.\\n2. The type of technical question that can be answered with an approach,\\ne.g., some approaches can only answer the binary question whether a\\nmodel has converged, whereas other approaches are able to answer ques-\\ntions about the behaviour of any part of the learning curve. See Sec. 4.2\\nfor more details.\\n3. The data resources that are used to model the learning curve. For exam-\\nple, in some cases, data from different algorithms on the same dataset\\nis being used, whereas in other cases, data from the same algorithm on\\nother datasets. See Sec. 4.3 for more details.\\nWe perform an extensive literature survey in which we categorise published\\nlearning curve extrapolation models along the various axes of this framework.\\nThis literature survey is subject of Sec. 5, which is then summarised in Table 1.\\nWe focus specifically on supervised machine learning, in which learning curves\\ndescribe the predictive performance of a model produced by a learning algo-\\nrithm either as a function of the number of training instances or of the time or\\niterations spent for learning on a given dataset. We explicitly exclude learning\\nSpringer Nature 2021 LATEX template\\n4\\nLearning Curves for Decision Making\\ncurves that describe the performance of a learning agent in an environment\\nover time, i.e., the learning curve of an agent in a reinforcement learning\\nsetup (Waltz and Fu, 1965). Similarly, we briefly contrast learning curves to\\nother performance curves, such as active learning curves, feature curves, and\\ncapacity curves, and explain why we consider these out of scope for the litera-\\nture review. Still, we aim to survey exhaustively the literature that introduces\\napproaches that use learning curves in supervised learning.\\nContributions:\\nOur contributions are the following.\\n• We present a unified framework of the usage of learning curves for decision\\nmaking in machine learning and an extensive review of the literature on\\napproaches that fall within this framework. This framework contains three\\naxes, i.e., (i) the type of decision-making situation, see also Fig. 1 (ii) the\\ntype of question that can be answered, see also Fig. 10 and (iii) the data\\nresources that are used to model the learning curve, see also Fig. 11. While\\nthe first axis of this framework is also used in other literature (see, e.g.,\\nViering and Loog, 2023), to the best of our knowledge, the other two axes\\nhave not yet been explicitly identified.\\n• We conducted a literature survey in which we categorise methods presented\\nin the literature along the various axes of this framework. Sec. 5 lists all\\nthese methods (where each subsection represents a type of question being\\nanswered), and Table 1 overviews all methods along the three axes of our\\nframework.\\n• Based on the framework, we identify unexplored routes for further research.\\nMost notably, we note that there is a mismatch between the research ques-\\ntions being answered and the learning curve modelling method being used;\\nin many cases, a high-level modelling technique is used to answer a low-\\nlevel question. We speculate that matching the level of the question being\\nanswered with the appropriate level of the modelling technique can further\\nimprove the obtained results.\\nRelation to other literature reviews on learning curves:\\nAnother prominent literature review that centres around learning curves is\\nthe highly complementary work by Viering and Loog (2023), which has been\\ndeveloped in parallel. While both works have identified the three types of\\ndecision-making situations that are referred to in the literature, Viering and\\nLoog (2023) survey more theoretical work that analyses the shape of learning\\ncurves, whereas this work surveys work that is more oriented towards methods\\nthat extrapolate learning curves, thereby supporting the data scientists in\\nvarious decision-making situations.\\nStructure:\\nThis paper is structured into three main parts. Sec. 2 presents relevant back-\\nground knowledge on learning curves, including formal definitions and the\\nSpringer Nature 2021 LATEX template\\nLearning Curves for Decision Making\\n5\\nterminology relevant for the remainder. Sec. 3 presents relevant important con-\\ncepts that relate to how learning curves are generally modelled. Sec. 4 contains\\nour main contribution by introducing our framework for categorising methods\\nthat utilise learning curves for decision making in supervised learning. Follow-\\ning this framework, Sec. 5 exhaustively reviews approaches that explicitly or\\nimplicitly answer questions related to learning curves to make or recommend\\ndecisions in the context of supervised machine learning. Sec. 6 concludes our\\nfindings. Finally, Appendix A presents a table that overviews the most critical\\nnotation used throughout this paper.\\n2 Background on Learning Curves\\nThis section gives a conceptual background on learning curves. It first provides\\nan idealised formal definition in Sec. 2.1 followed by a definition of empirical\\nlearning curves in Sec. 2.2 that can be computed in practice. The concept of\\nutility curves is introduced in Sec. 2.3. Sec. 2.4 introduces important terminol-\\nogy such as anchor points, limit performance, and the saturation point. Finally,\\nSec. 2.5 contrasts the learning curves covered in this survey with other types\\nof performance curves used in machine learning.\\n2.1 Sample-Wise and Iteration-Wise Learning Curves\\nWe consider learning curves in the context of supervised machine learning.\\nFormally, in the supervised learning context, we assume some instance space\\nX and a label space Y. A dataset d ⊂{(x, y) | x ∈X, y ∈Y} is a finite\\nrelation between the instance space and the label space. We denote as D the\\nset of all possible datasets. A learning algorithm is a function a : D × Ω→H ,\\nwhere H = {h | h : X →Y} is the space of hypotheses and Ωis a source of\\nrandomness.\\nNote that learning curves can also be considered in other machine learning\\nsetups. In fact, learning curves appeared first in reinforcement learning (Waltz\\nand Fu, 1965) and have also been used for unsupervised learning (Meek et al,\\n2002). However, to give this survey focus, we consider learning curves for\\nsupervised learning.\\nThe performance of a hypothesis is typically expressed as risk, which is\\nalso often called out-of-sample error:\\nRout(h) =\\nZ\\nX,Y\\nloss(y, h(x)) dPX×Y.\\n(1)\\nHere, loss(y, h(x)) ∈R is the penalty for predicting h(x) for instance x ∈X\\nwhen the true label is y ∈Y, and PX×Y is a joint probability measure on\\nX × Y from which the available dataset d has been generated. As such, the\\nout-of-sample error represents the weighted summed error that hypothesis h\\nmakes on all possible instance-label pairs, weighted by their probabilities.\\nSpringer Nature 2021 LATEX template\\n6\\nLearning Curves for Decision Making\\nThe performance of a learning algorithm is simply the performance of the\\nhypothesis it produces. In contrast to the performance of a hypothesis, the\\nperformance of a learner depends on its input, i.e., on the data provided for\\nlearning. The average performance of learner a for a number n of training\\nexamples can then be expressed as\\nC(a, n) =\\nZ\\nω∈Ω,dtr∈D,|dtr|=n\\nRout(a(dtr, ω))dPX×YdPΩ,\\n(2)\\nwhere dtr ∈D is the dataset of size n used to induce a model using learner a.\\nIt is generally assumed that d is a collection of i.i.d. samples from PX×Y.\\nWhen we consider Eq. (2) as a function of the number of training samples\\nfor a fixed learner a, we obtain the sample-wise curve of learner a. That is,\\nthe sample-wise curve is the function C(a, ·) : N →R; so it is a sequence of\\nperformances, one for each training size. Fig. 2 (left) visualises this by means\\nof the green line and compares this to two other types of learning curves with\\nthe error rate as the loss (see Sec. 2.5).\\nAlternatively, many learning algorithms implement an iterative internal\\noptimisation process, which allows describing the learning progress over time\\nor a number of iterations. For example, neural network training produces a\\nnew hypothesis after every batch or epoch, ensemble learners like bagging\\nor boosting produce a new hypothesis after every added ensemble member,\\nand support vector machine optimizers yield updated attribute or instance\\ncoeffients in iteration. In the formal framework, a learner can be seen more\\ngenerally as a function a : D × Ω→H + that maps a dataset to a sequence of\\nhypotheses, one for each of its iterations. The above error function for learners\\ncan then be written as\\nC(a, n, t) =\\nZ\\nω∈Ω,dtr∈D,|dtr|=n\\nRout(a(dtr, ω)t)dPX×YdPΩ,\\n(3)\\nHere, t expresses some budget, for example, time or a number of iterations\\nover the dataset, often expressed in epochs.\\nBased on this notion, the iteration-wise curve of a learner a is defined for a\\nfixed dataset size n (often between 70% and 90% of the available data) and is\\nthen the function C(a, n, ·) : N →R. Fig. 2 (right) visualises an example of two\\niteration-wise curves. It can be seen that these iteration-wise curves are also\\ninfluenced by the number of samples. The sample-wise curve in this example\\n(visualised by the dashed line) is assumed to utilise the maximal number of\\niterations. Examples of such learning curves occur above all in the analysis of\\ndeep learning models (Domhan et al, 2015; Goodfellow et al, 2016).\\nThe two types of learning curves seem to be related and indeed look simi-\\nlar when visualised, but they have different semantics. The crucial difference is\\nthat iteration-wise curves are usually visualized for a fixed and finite number\\nof training samples (expressed by n), no matter how large t becomes. In fact,\\nSpringer Nature 2021 LATEX template\\nLearning Curves for Decision Making\\n7\\nsample size\\nerror rate\\nSample-Wise Learning Curve\\nsample-\\noptimized curve\\nstream curve\\niterations/time\\nIteration-Wise Learning Curve\\nFig. 2: Left: (Standard) sample-wise curve (green) for a single learner on a\\nparticular data source together with learning curves under sample optimisation\\n(pink) and learning curves on streams (blue). Right: iteration-wise curves of a\\nsingle learner on a particular data source for two different dataset sizes n1 < n2.\\nsince iterative learners typically automatically stop the learning process as\\nsoon as no progress is observed, it often holds that C(a, n) = limt→∞C(a, n, t).\\nBut this is not necessarily the case, specifically if an algorithm stops early\\n(in the iteration-wise curve), e.g., to avoid over-fitting (as the neural net-\\nwork in Fig. 3). Note that as t grows, each training instance is considered\\nan unlimited number of times; hence, iteration-wise curves show how much\\nthe learner can make out of a constant number of training instances. Instead,\\nsample-wise curves show the performance of the learner as the number of exam-\\nples grows. The latter typically means, for an infinite input space, that the\\ninformation basis available to the learner is growing strictly bigger, while the\\ninformation is constant in the case of iteration-wise curves.\\nNote that while the learning success can be expressed in a metric that\\nought to be maximised or minimised, in this paper we assume for simplic-\\nity that they are to be minimised. This is why the performance is expressed\\nthrough a loss such as the error rate. In this case, learning curves are (usually)\\ndecreasing. However, more generally, one can also be interested in increas-\\ning learning curves, e.g., when considering accuracy or the F1 measure. Since\\nlearning curves can be simply mirrored at the x-axis, every approach discussed\\nin this paper is applicable to both increasing or decreasing learning curves.\\nFor simplicity, this survey assumes that lower performance values are better\\n(error rate, log-loss/cross-entropy, Brier score, mean-square-error, etc.).\\n2.2 Empirical Learning Curves\\nThe above definitions of learning curves are purely theoretical. This is because\\nwe cannot evaluate equations (1-3) in practice. First, the out-of-sample error\\nRout, i.e., Eq. (1) cannot be computed in practice since the measure PX×Y is\\nunknown. Relying on this error, the learning curve values cannot be computed\\neither. The necessity to average over the oftentimes uncountable set of all\\npossible train sets can add additional problems.\\nSpringer Nature 2021 LATEX template\\n8\\nLearning Curves for Decision Making\\n0\\n1000\\n2000\\n3000\\nSize n of the training set dtr\\n0.125\\n0.150\\n0.175\\n0.200\\nerror rate\\nEmpirical Sample-Wise Learning Curves\\nRF\\nNN\\nSVM\\n0\\n50\\n100\\n150\\n200\\n250\\nIteration t (trees/epochs/optimization iterations)\\nerror rate\\nEmpirical Iteration-Wise Learning Curves\\nRF\\nNN\\nSVM\\nFig.\\n3:\\nEmpirical\\nlearning\\ncurves\\nfor\\nthe\\nwaveform\\ndataset.\\nLeft:\\nsample-wise curves at different training set sizes up to 80% of the data. The\\nremaining 20% are used to compute the error. Right: iteration-wise curves\\nwith one entry for each forest size (RF), epoch (NN), or optimization iteration\\n(SVM) when a fixed set of 80% of the available data is used in each iteration\\nfor training and the rest to compute the error.\\nTo compute learning curves in practice, we rely on empirical estimates of\\nthe above quantities. We estimate the out-of-sample error by the internal error:\\nRin(h) = 1\\n|d|\\nX\\n(x,y)∈d\\nloss(y, h(x)),\\n(4)\\nwhere d is the dataset used for assessing the performance of hypothesis h.\\nNote that the dataset d may or may not contain instances used to create the\\nhypothesis h. In most practical applications, dataset d consists of instances\\nthat have not been seen during the creation of hypothesis h (i.e., the test set).\\nHowever, in theory, the internal error can also be estimated based on the train\\nset dtr or a combination of instances from the train set and test set.\\nWe consider an empirical learning curve any set of estimates of a true\\nlearning curve for different sizes or iterations. We can use various estima-\\ntion procedures to estimate the performance using a given training size, such\\nas using a regular holdout set or cross-validation. The latter leads to vari-\\nous estimates, and averaging over these estimates yields an estimate of the\\nsample-wise curve in Eq. (2) at size n. To obtain an empirical estimate of the\\niterative learning curve in Eq. (3), we do the same except that we stop the\\nlearning algorithm after t iterations.\\nFig. 3 shows empirical learning curves for a Random Forest (RF), a Neural\\nNetwork (NN) with 100 neurons in a hidden layer, an a support vector machine\\n(SVM) with RBF kernel on a concrete and widely used benchmark dataset (the\\nwaveform dataset). The empirical curves are the scatter points; the lines here\\nare only a visual aid. The error rate is here obtained from a single validation\\nfold, i.e., without averaging, which explains the rather unsmooth behavior.\\nThe iteration-wise curves were created using 80% of the data for training.\\nSince empirical learning curves are the only way to gain insights about true\\nlearning curves, quite some studies have been published with the sole goal of\\nSpringer Nature 2021 LATEX template\\nLearning Curves for Decision Making\\n9\\nsharing empirical learning curves with the community and thereby improv-\\ning the understanding of how they behave. Perlich et al (2003) contrast the\\nlearning curves on decision trees and logistic regression on different datasets.\\nNotably, the authors also compare learning curves, e.g., they report whether\\none curve is below the other (dominates it) on all considered training set sizes\\nor whether the two learning curves cross. Several other studies report learning\\ncurves for specific learners. Ng and Jordan (2001) compare logistic regression\\nand naive Bayes. Mørch et al (1997) conduct a study similar to the one by\\nPerlich et al (2003) to compare linear vs. non-linear classifiers on a smaller\\nscale. Recently, a number of learning curve databases have been published,\\ni.e., learning curves of different network architectures on typical image classi-\\nfication datasets (Bornschein et al, 2020; Dong and Yang, 2020; Siems et al,\\n2020), learning curves of different machine learning algorithms on tabular data\\n(Mohr et al, 2022), and for mixtures of these tasks (Eggensperger et al, 2021;\\nPfisterer et al, 2022).\\nEmpirical studies of this type do not answer generalising questions about\\nlearning curves but rather report experimental results. This is different from\\ncontributions in which certain model assumptions are made and data is com-\\npared to those models, e.g., with the aim to compute the goodness of fit of\\nthat model. In Sec. 3, we briefly discuss some of such models.\\n2.3 Utility Curves\\nThe concept of learning curves can be further generalised to a utility\\ncurve (Last, 2007, 2009; Weiss and Tian, 2008). The utility usually involves\\na trade-off between the performance and the computational cost of training a\\nmodel. The specific details can be different per task. The utility is connected\\nto the learning curve in so far as the utility is also a function of the budget\\nand is directly influenced by the predictive performance. Therefore, one could\\nargue that the utility curve U is obtained by passing the learning curve to\\nthe utility function alongside other parameters that influence the utility, most\\nnotably the cost of acquiring new instances and the cost to train a model on\\nthe respective dataset size. The learning curves associated with utility costs\\nare visualised in Fig. 4 (orange) and compared to a normal learning curve.\\nAssuming there is a linear cost associated with further training the classifier,\\nwe can see that the utility curve (which makes a trade-off between performance\\nand cost) peaks at a given point, and deteriorates afterwards.\\n2.4 Terminology of Learning Curves\\nWhile this survey is not primarily about the shapes of learning curves, the\\nshapes of learning curves play an important role when using them to make deci-\\nsions. Hence, we consider it necessary to convey some of the most important\\ninsights about the basics of the shapes of learning curves. However, we refer to\\na recent exhaustive survey on the shapes of learning curves (Viering and Loog,\\n2023) for details on this topic. Fig. 5 visualises some important concepts.\\nSpringer Nature 2021 LATEX template\\n10\\nLearning Curves for Decision Making\\nerror rate\\nbudget\\nUtility Curve\\nFig. 4: A utility curve with the corresponding learning curve.\\nAnchor Points\\nIn this survey, we adopt the term anchor to refer to a point for which the\\nempirical learning curve carries a performance estimate. There is no established\\nname for such points in literature. They are called sample sizes in (John and\\nLangley, 1996; Leite and Brazdil, 2007; Provost et al, 1999), and those authors\\nrefer to a collection of such samples sizes as a schedule (Figueroa et al, 2012;\\nJohn and Langley, 1996; Meek et al, 2002; Provost et al, 1999). However, for\\niteration-wise curves, the term ‘sample’ is misleading because the curve plots\\nperformance against the number of times all instances (out of a fixed set) are\\npresented to the learner, which is not the same as sample size. Besides, the term\\nsample size is quite overloaded in the context of machine learning, because this\\nfield deals with various types of samples in different contexts, e.g., train and\\nvalidation samples, etc. Another terminology observed sometimes is the one\\nof sample landmarks (F¨urnkranz and Petrak, 2001; Leite and Brazdil, 2005).\\nHowever, this term is also slightly confusing, since landmarks are generally\\nunderstood as the performances of cheap-to-evaluate learners, which was also\\nthe motivation for this terminology by F¨urnkranz and Petrak (2001). A less\\nused terminology is the term anchor (Kielh¨ofer et al, 2024; Kolachina et al,\\n2012; Mohr and van Rijn, 2021, 2023), which is not ambiguous in the machine\\nlearning context and captures the idea that analysis is based on some selected\\npoints. It serves well to immediately create an association with a particular\\nsize of a sampled training data set or a number of visited instances that is\\nused in the context of building an empirical learning curve.\\nThroughout this paper, we formally use the symbol b to refer to an\\nanchor. Thereby, we abstract away from sample sizes n or iteration t. In other\\nwords, the symbol b is used to indicate points on both sample-wise curves or\\niteration-wise curves, and it should be clear from the context which one is\\nmeant (if the difference is relevant).\\nFor example, in Fig. 3 above, we have the following anchors. For the\\nsample-wise curve, a geometric schedule with n ∈{⌈2i/2⌉| i ∈N} is cho-\\nsen. In this concrete case, the anchors are {1, 2, 3, 4, 6, 8, 12, 16, 23, 32,\\n46, 64, 91, 128, 182, 256, 363, 512, 725, 1024, 1449, 2048, 2897, 3750}. In the\\niteration-wise curve, every iteration is used as an anchor, so t ∈{1, .., 200}.\\nSpringer Nature 2021 LATEX template\\nLearning Curves for Decision Making\\n11\\ntrain set size |dtr| or number of iterations t\\n0.18\\n0.20\\n0.22\\n0.24\\n0.26\\n0.28\\nError Rate\\nLearning Curve (C)\\nObservations of an Empirical LC at anchors\\nLimit Performance\\nSaturation Point/Performance (bsat/psat)\\nPre-Exponential Point/Performance\\nFig. 5: Concepts related to a learning curve. Red: Limit performance. Green:\\nSaturation point bsat (vertical line) and saturation performance psat (hori-\\nzontal line). Orange: Pre-exponential point (vertical line) and pre-exponential\\nperformance (horizontal line). The curve plateaus at a performance of 0.2; the\\nplateau is not visualized here to emphasise the difference between the pre-\\nexponential point and saturation point.\\nLimit Performance\\nIt is generally assumed that learning curves converge to some value. In the\\ncase of iterative learning curves, there are sometimes oscillations in the curve,\\nbut even in such cases, the curve usually converges to some (in those cases,\\ntypically a bad) value eventually. We are not aware of a particular term that\\nis used to describe the score to which a learner converges. Cortes et al (1994)\\ndescribes the limiting performance or the asymptotic performance of the data;\\ni.e., it is not the property of a particular learner but the best achievable perfor-\\nmance among all learners under consideration (even though only tested with\\ntwo model types in the paper). In this paper, we adopt the term limit perfor-\\nmance of the learner (on a fixed number of training instances in the case of\\niteration-wise curves), and we denote this performance as plim.\\nSaturation Point\\nIntuitively, the saturation point is the anchor after which the performance\\nconvergences. That is, the anchor after which all values are in a distance of less\\nthan some pre-defined and typically very small ε. In early works, this point has\\nbeen called stopping point (Figueroa et al, 2012; Leite and Brazdil, 2003, 2004;\\nMeek et al, 2002; Provost et al, 1999). Provost et al (1999) characterise this\\npoint as follow: “Models built with smaller training sets [than smin] have lower\\naccuracy than models built with from training sets of size smin, and models\\nbuilt with larger training sets have no higher accuracy.” In the context of\\nthose works, namely progressive sampling, the term stopping point makes sense\\nbecause they progressively sample until they reach the convergence region, and\\nthen stop the sampling procedure. In the absence of such a mechanism, the\\nterm appears a bit odd. The notion of saturation in the context of learning\\nSpringer Nature 2021 LATEX template\\n12\\nLearning Curves for Decision Making\\ncurves was proposed by Tomanek (2010) and seems appropriate since the limit\\nperformance has not been reached, but it has almost been reached. The curve\\nis saturated up to a mistake of ε.\\nWe will denote the saturation point itself as bsat, emphasising that the\\nsaturation point is an anchor. The saturation performance is simply the value\\nof the learning curve at that point. Both are defined in the context of a fixed\\nlearner a. Hence, we can write the saturation performance as psat := C(a, bsat)\\nin the context of a sample-wise curve or psat := C(a, n, bsat) in the context of\\nan iteration-wise curve.\\nPre-Exponential Point\\nA related concept is the pre-exponential point and, correspondingly, the pre-\\nexponential performance. The saturation point may be expensive to reach\\nin the sense that a lot of training data is necessary to obtain the satura-\\ntion performance. We call the smallest anchor point for which an increase by\\na factor of q leads to a performance improvement of less than some δ, i.e.,\\nC(a, n) −C(a, q · n) < δ. Reasonable candidates for q can be 2 or 10, while\\ncandidates for δ can be 0.01 or 0.001 if the metric is the error rate. Its seman-\\ntic is that from the pre-exponential point on, one needs more than qk (i.e., an\\nexponentially increasing number of) training samples or iterations to improve\\nby a low margin of kδ. Correspondingly, the pre-exponential performance is\\nthe performance that can be obtained by a comparably small anchor point.\\nUtility-Based Stopping Point\\nThe utility-based stopping point is the point at which the acquisition of fur-\\nther data points has a negative impact on the utility of the data analysing\\nentity. Therefore, this concept is associated with utility curves. The utility-\\nbased stopping point is not related to the saturation point but, if at all, rather\\nto the pre-exponential point; we denote it as bu\\nsat. In Fig. 4, this is the peak of\\nthe yellow curve.\\nPlateau\\nThe right-sided open interval bounded by the saturation point from the left\\nis consistently called the plateau of the curve. However, a curve can also have\\nintermediate plateaus, i.e., intervals of (almost) constant performance without\\nbeing the final plateau.\\nWell-behaved Learning Curves\\nTo our knowledge, the notion of a well-behaved learning curve is first used by\\nProvost et al (1999). A learning curve is said to be well behaved if its slope\\nis monotonically non-decreasing (for error-based learning curves). An even\\nstricter criterion demanding convexity of the curve, which implies monotonic-\\nity, has been introduced recently by Mohr and van Rijn (2021, 2023). The\\nproperty of being well behaved is one of the true learning curve. The (linear\\ninterpolation of an) empirical learning curve can often violate this condition,\\nSpringer Nature 2021 LATEX template\\nLearning Curves for Decision Making\\n13\\nspecifically when the number of validations conducted at the anchors is small\\nor when the learning curve has reached a plateau.\\nWhile it is known that not all learning curves are well behaved (Loog and\\nDuin, 2012; Loog et al, 2019), empirical studies suggest that such curves are\\nrather an exception and that most sample-wise curves are well behaved. Learn-\\ning curves that are not well behaved are found above all in the context of\\ndeep learning, where a double descent or peaking phenomenon can be observed\\n(at times) for both sample-wise curves and iteration-wise curves (Nakkiran\\net al, 2020); the effect was also observed decades ago for other learners (Val-\\nlet et al, 1989). However, extensive empirical studies have shown that most\\nsample-wise curves are even convex (Mohr and van Rijn, 2023) and hence well\\nbehaved. Some recent works suggest that potential ill behaviour can be mit-\\nigated by appropriate configuration or wrapping of learners (Mhammedi and\\nHusain, 2021; Nakkiran et al, 2021; Viering et al, 2020).\\n2.5 Relation to Other Types of Performance Curves\\nWe briefly discuss the relation of learning curves to other types of curves\\nand problem settings. These curves are fundamentally different from learning\\ncurves, and therefore, a more detailed coverage is beyond our scope.\\n2.5.1 Learning Curves in Active Learning\\nActive learning is a setting where the data scientist can acquire the label of\\narbitrary instances (Settles, 2009) and hence can actively increase the training\\nset. On a concrete data source with a concrete initial dataset, a specific active\\nlearning strategy creates a deterministically extended dataset for any arbitrary\\nanchor. While this allows drawing a curve that plots performance against the\\nnumber of training instances, this curve is not the one described in Eq. (2),\\nbecause the datasets are not sampled i.i.d. from PX×Y but dictated by the\\nactive learning strategy; they are sample-optimised. Fig. 2 displays how this\\ntheoretically relates to the normal sample-wise curve. We do not consider this\\ntype of learning curve in this survey.\\n2.5.2 Learning Curves under Optimal Class Distribution\\nSimilarly, we obtain such a biased learning curve if we do not preserve the class\\ndistribution. Weiss and Provost (2003) have shown that it can be advantageous\\nto over-sample instances of a minority class if they occur substantially more\\nseldom than instances of a majority class. One can then ask for the best class\\ndistribution for a certain anchor. Similar to the active learning case, this creates\\na new distribution of datasets that does not coincide with PX×Y anymore. If\\nwe optimise over the class distribution at each anchor, we obtain a curve with\\nthe same axis labels as a sample-wise curve but a different semantics (and most\\nlikely different values). For the case of two classes, this type of learning curve\\nis obtained by taking the budget-wise maximum of a performance surface as\\nproposed by Forman and Cohen (2004).\\nSpringer Nature 2021 LATEX template\\n14\\nLearning Curves for Decision Making\\nnum features\\nerror rate\\nFeature Curve\\nFig. 6: Feature curve example for a single learner, once for a fixed and finite\\ndataset size and once for an infinite dataset size.\\n2.5.3 Learning Curves on Data Streams\\nAnother type of learning curve that violates the implicit assumptions made\\nin Eq. (2) is obtained when learning from data streams, a scenario in which\\ntraining instances are coming in sequentially and need to be processed under\\nstrict time and memory constraints (Bifet et al, 2018). Incremental learners like\\nHoeffding trees (Domingos and Hulten, 2000) and models induced by stochastic\\ngradient descent are natural solutions to this problem domain. When applied\\nto a data stream, these algorithms explicitly forget an instance once it has been\\nprocessed. This is not only to free memory but also to address the problem of\\nconcept drift, i.e., the fact that PX×Y changes over time. In such a case, we do\\nnot have that C(a, n) = limt→∞C(a, n, t) but rather that C(a, n) = C(a, n, n),\\nwhere the learner is updated at every incoming training instance, and each of\\nthe n instances was considered exactly once; for example, similar to training\\na neural network for one epoch with batch size 1. While this produces a kind\\nof sample-wise curve, the result is clearly different from the learning curve\\nreceived in the normal batch setting. Fig. 2 displays how these type of curves\\ntheoretically relate to the normal sample-wise curves.\\nEven though syntactically equivalent to a sample-wise curve, data stream\\nlearning curves are substantially different and need different treatment. Due\\nto the (potential) concept drift over the time dimension, the i.i.d. assumption\\nis not guaranteed (da Costa et al, 2016). From a theoretical viewpoint, extrap-\\nolating the learning curve over time to meaningfully predict future behaviour\\nbecomes impossible without the i.i.d. assumption. Due to its special nature,\\nthe data stream setting is beyond our scope.\\n2.5.4 Feature Curves\\nLearning curves always consider a fixed number of features. Instead, one can\\nfix the number of training instances and consider the performance as a function\\nof the number of features. This yields so-called feature curves (Hughes, 1968;\\nViering and Loog, 2023).\\nDefining meaningful feature curves is conceptually more difficult than\\nlearning curves because of the importance that single features can have. For\\nSpringer Nature 2021 LATEX template\\nLearning Curves for Decision Making\\n15\\nsimplicity, consider only sample-wise curves for this comparison. In such learn-\\ning curves, every point n is associated with the expected performance when\\nusing n training instances. These n training instances are assumed to be drawn\\nindependently and identically distributed from the underlying distribution.\\nAccording to the aforementioned definitions of a learning curve (per Eq. 2 and\\nEq. 3), there is no notion of a more informative instance (even though this\\nnotion clearly exists in the field of active learning). In particular, the order in\\nwhich instances are drawn is irrelevant. However, in the context of features,\\nsome features are often more informative than others (and again, other fea-\\ntures that have not been measured may be even more informative). Therefore,\\nin order to get an adequate overview, feature curves require some kind of aver-\\naging over all possible sets of features of a fixed size that can be formed from\\na base set of (available) features, as done by Hughes (1968). The fact that\\nsome features might be more important than other features makes it hard to\\nmodel and extrapolate feature curves, as there is no reasonable set of assump-\\ntions to build these models on. Fig. 6 displays two theoretical examples of\\nfeature curves. When having a finite number of samples, the performance of\\nthese curves will, in the limit (when more features are added), deteriorate due\\nto the curse of dimensionality. Of course, when having infinite samples, the\\nperformance of such feature curves will go to perfect performance.\\nNote that feature curves and learning curves can be integrated. For exam-\\nple, Strang et al (2018) look at the combination of the number of instances\\nand the number of features. Since the effect of the number of features and the\\nnumber of instances on the overall performance is clearly not independent,\\nconsidering both together is sensible. At the same time, due to the ambigu-\\nous semantics of feature curves already discussed above, using such combined\\ncurves is not necessarily straightforward for decision making, and we are not\\naware that the combined curve has been used for decision making so far.\\n2.5.5 Capacity Curves\\nCortes et al (1994) introduce a curve that plots the performance of a config-\\nurable learner as a function of the complexity of its instantiation. For example,\\nthe learner could be a neural network, and the complexity would then be the\\nnumber of hidden layers. In doing this, a fixed dataset size is assumed. In that\\npaper, this type of curve has no specific name, but we dub it the capacity curve\\nbecause they plot the performance as a function of capacity. Fig. 7 displays\\nexamples of capacity curves.\\nCapacity curves are interesting from a theoretical viewpoint as they allow\\nus to analyse the intrinsic noise level of the given data. More precisely, one can\\nask for the performance of a learner of some complexity level on, perhaps, an\\ninfinite number of data points. If this value can be computed for every com-\\nplexity level, then we obtain a performance curve over the model complexity.\\nIf the number of data points is large enough, this curve can be assumed to be\\nmonotonically decreasing. If we have a maximally flexible learner (such as a\\nneural network) that can, in principle, assimilate any function, then the curve\\nSpringer Nature 2021 LATEX template\\n16\\nLearning Curves for Decision Making\\nmodel capacity\\nCapacity Curve\\nFig. 7: Capacity curve example for a learner class whose complexity can be\\nincreased (e.g., a neural network), once for a fixed and finite dataset size and\\nonce for the theoretical case of an infinite dataset size.\\nwill converge towards the intrinsic noise of the data. That is, no learner can\\nimprove over that performance.\\n2.5.6 Curriculum Learning\\nCurriculum learning is a paradigm inspired by human learning strategies, par-\\nticularly how humans learn complex tasks by gradually increasing the difficulty\\nof the examples they are exposed to (Wang et al, 2022).\\nIn curriculum learning, instead of randomly presenting training instances to\\nthe model, instances are presented in a meaningful order, typically from simpler\\nto more complex sets of training instances. This can help the model learn more\\neffectively and converge faster by initially focusing on easier instances that are\\nsimpler to learn and gradually introducing more difficult instances.\\nLearning curves related to curriculum learning come with all sorts of novel\\nchallenges, such as interpreting the learning curve in case more complex test\\ninstances are provided. Therefore, it is hard to make assumptions about what\\na well-behaved curriculum learning curve would look like. For this reason, we\\nconsider curriculum learning to be out of scope.\\n3 Modelling a Learning Curve\\nA learning curve model is a characterisation of the true learning curve derived\\nfrom an empirical learning curve. The empirical learning curve is the result\\nof sampling from a stochastic process that underlies noise stemming from\\nrandomness in data splits and the learning algorithm itself. It is typically\\nassumed (Domhan et al, 2015; Figueroa et al, 2012; Klein et al, 2017a,b; Mohr\\nand van Rijn, 2023; Swersky et al, 2014) that, for any learner a and any budget\\nb ∈N, this stochastic process follows the distribution\\nf (a, b) ∼N(µa,b, σ2\\na,b) = µa,b + N(0, σ2\\na,b),\\n(5)\\nwhere µa,b is either C(a, b) as per Eq. (2) if modelling a sample-wise curve or\\nC(a, n, b) for some (implicit and not further specified) training set size n as\\nper Eq. (3) when modelling an iteration-wise curve. It assumes a noise that\\nfollows a Gaussian distribution with zero mean and dispersion σ2\\na,b that may\\nSpringer Nature 2021 LATEX template\\nLearning Curves for Decision Making\\n17\\nvary over different anchor sizes. The assumption of a Gaussian noise is rea-\\nsonable, because most loss functions form an average over sample-wise scores,\\nwhich implies a Gaussian distribution through the Central Limit Theorem.\\nFor simplicity, we will not make a difference between the two types of learning\\ncurves, so that anchors are always denoted as budgets b (regardless of whether\\nthis refers to training set size or iterations, epochs, trees, etc.).\\nThe task of forming a learning curve model for one or multiple learners\\nis inherently one of supervised machine learning and requires the ability to\\ngeneralise across anchors (and possibly even learners). In practice, observations\\nare only available for a finite number of learner-anchor combinations O =\\n{(a1, b1), (a2, b2), (a3, b3), . . .}, and the task is to learn, for each learner a, a\\nmodel ˆfa(b) that expresses the belief about f (a, b) for any budget b, not only\\nthose in O. Therefore, it is not enough to simply create an explicit estimate\\nof µa,bi for the anchors (a, bi) ∈O, but some general pattern must be learned.\\nObserve that a here is not a parameter of ˆfa(b) since one often does not\\ngeneralise across learners. However, some approaches advocate a single model\\nˆf (a, b) that estimates µa,b for any learner-budget combination, where a ∈A\\nand A is the set of all possible learners (Klein et al, 2017a,b; Swersky et al,\\n2014).\\nDuring this process of building a learning curve model, one generally needs\\nto cope with two types of uncertainty. First, the aleatoric uncertainty is σ2\\na,b,\\nwhich is intrinsic and averaged out in the true learning curve. Again, this is the\\nuncertainty arising from randomness in the learner itself (if applicable) and\\nrandom effects in the splits (or more general: data collection) when computing\\nthe empirical learning curve. Second, the epistemic uncertainty is the one the\\nlearning curve model itself has about the estimate of the mean value µa,b. This\\nuncertainty can be removed by gathering more observations (i.e., extending\\nobservation set O).\\nIt is important to understand that epistemic uncertainty generally does\\nnot indicate model quality. Epistemic uncertainty is not related to correctness:\\nA model can have no epistemic uncertainty (be absolutely sure) about an\\nactually wrong prediction, and similarly, it can be uncertain about a prediction\\nthat is actually correct. Also, epistemic uncertainty gives no indication about\\nwhether the class from which the predictive model is inferred is suitable for the\\ntask, i.e., whether the true curve can be captured by the model that is fitted\\n(e.g., a power law). H¨ullermeier and Waegeman (2021) discuss this for the\\nmore general case of selecting an appropriate machine learning model. Recent\\nresults suggest that many learning curves are not adequately captured even by\\na very flexible parametric model, i.e., the 4-parameter MMF model (Kielh¨ofer\\net al, 2024), which motivates other, possibly non-parametric approaches for\\nmodelling, such as the one used in freeze-thaw Bayesian optimisation (Swersky\\net al, 2014). Therefore, the uncertainty at the meta-level about whether a\\nmodel class is suitable for a task cannot be captured in epistemic uncertainty\\nand must be studied independently.\\nSpringer Nature 2021 LATEX template\\n18\\nLearning Curves for Decision Making\\nModelling uncertainty in learning curve models typically implies modelling\\nthe epistemic uncertainty about the curve mean µa,b. This uncertainty refers to\\narbitrary anchors b, both those from the observed data O as well as the anchors\\nthat were not part of this. The three common patterns to define beliefs about\\nµa,b are (i) point estimates (no uncertainty is expressed), (ii) range estimates,\\ne.g., confidence intervals for µa,b, or (iii) distribution estimates, which quantify\\na full belief model over the true value of µa,b. The aleatoric uncertainty can\\nalso be quantified and modelled, e.g., by taking many different samples per\\nanchor.\\nFig. 8 illustrates all of these concepts together. The blue line µa,· repre-\\nsents the true learning curve as per Eq. 2 or Eq. 3 for all possible values of b.\\nAs this integrates out all possible data splits as well as random factors from\\nthe algorithm, each point is an average from a distribution of many possi-\\nble performance values. The variance of this distribution is expressed as σ2\\na,·.\\nThis variance stems from the aleatoric uncertainty, in the sense that it is high\\nwhen the aleatoric uncertainty is high, and vice versa. As the budget b (rep-\\nresented on the x-axis) increases, the aleatoric uncertainty typically decreases,\\nand thereby the variance naturally decreases as well. The orange elements are\\nrelated to observations of the learners’ performance and the learning curve\\nmodel. The orange points (which all together form O) show observations made\\non the empirical learning curve (as a sample from the blue distributions, this\\ncan be one or more per point). Note that these do not necessarily need to align\\nwith µa,· or fall within the variance bandwidth. The orange solid line shows a\\npoint estimate defined by a parametric model obtained from the observations,\\nand in this example, it substantially deviates from the true curve (cf. Sec. 3.1).\\nThe orange shaded area is a range estimate modelling epistemic uncertainty,\\nwhich grows as one moves away from the available data (cf. Sec. 3.2). Finally,\\nthe dashed orange lines are distribution estimates for different budgets b (cf.\\nSec. 3.3). Each of these dashed lines can be seen as a probability density func-\\ntion (rotated 90 degrees) for a certain budget b. Each curve shown sketches the\\ndistribution of the belief about where the true mean may be situated, in this\\nfigure modelled through Gaussian distributions. As we move away from the\\nobservations, the shape of these bells grows bigger, indicating higher epistemic\\nuncertainty.\\nIn the following sections, we will explain the three model types in more\\ndepth. We explain the concept for models of the type ˆfa(b), i.e., models for a\\nspecific learner a, because this is the most common case. Generalising a curve\\nmodel across learners requires additional logic, which we discuss along with\\nthe models that utilise such generalised model in Sec. 5.2.3.\\n3.1 Point Estimates of the Learning Curve\\nThe simplest type of learning curve model for a learner a just estimates the\\nmean curve values µa,b for any possible budget b and ignores uncertainty\\naspects (solid orange line in Fig. 8). Given an empirical learning curve in\\nthe form of some finite samples from this process at different anchors B =\\nSpringer Nature 2021 LATEX template\\nLearning Curves for Decision Making\\n19\\nµa,·\\nσ2\\na,·\\nObservations\\nˆfa (Point Estimate)\\nˆfa (Range Estimate)\\nˆfa (Distribution Estimate)\\nFig. 8: Visualisation of various forms of uncertainty in learning curve mod-\\nelling. The blue line µa,· represents the true learning curve; the dot · means\\nthat it is for all budgets b. The variance over all possible curves that could\\nbe sampled is expressed as σ2\\na,·. The variance stems from the aleatoric uncer-\\ntainty. Orange points are anchors at which a learner’s performance has been\\nobserved. From this, an empirical learning curve can be modelled. The orange\\nsolid line, area, and dashed lines are point estimate, range estimate, and dis-\\ntribution estimate models, respectively. The latter two models also express\\nepistemic uncertainty.\\n{b1, .., bn}, a regression model ˆfa(·|θ) : N →R is trained with respect to some\\nmodel class with parameters θ. A considerable number of different parametric\\nmodels have been proposed over time for this task. To our knowledge, the first\\nproposal of such classes was made, apparently independently, by Cortes et al\\n(1993) and John and Langley (1996) with the three-parametric inverse power\\nlaw (IPL)\\nµa,b = α + βb−γ,\\n(6)\\nwhere the parameters α, β, γ > 0 need to be optimised to fit the learning curve\\nfor learner a. Frey and Fisher (1999) took a simplified variant of that model\\n(αb−β) and compared it to a logarithmic (α log b + β), and an exponential\\nmodel (α · 10−βb). While it has been argued, at least for the power-law family,\\nthat there is a theoretical foundation for it (Seung et al, 1992), the considered\\nmodel classes are typically not theoretically motivated but rather pop up in\\nan ad-hoc manner. For example, Gu et al (2001) extended the above three\\nclasses, without a specific motivation, by a vapor pressure model, the Morgan-\\nMercer-Flodin (MMF) model, and a Weibull model.\\nDepending on the purpose of the model, it is essential to distinguish\\nbetween best-fitting and best-predictive models. As was pointed out by Gu\\net al (2001), the model class that can best accommodate a given set of anchor\\npoints is not always the one that will make the best predictions on a high\\nanchor when having been fit only on some initial anchors. To understand the\\nlearning curve of a learner on a given dataset, one is interested in a best-fitting\\nmodel class. For extrapolation, one is interested in a best-predictive one.\\nSpringer Nature 2021 LATEX template\\n20\\nLearning Curves for Decision Making\\nOur work does not seek to give a broad overview of different model classes\\nbut rather about the usage of such models. We expose the inverse power law\\nmodel because it is arguably the most prominent model class and has been\\nadvocated by many authors as a good fit for nearest neighbours, SVMs, deci-\\nsion trees, and neural networks (Frey and Fisher, 1999; Gu et al, 2001; Hess\\nand Wei, 2010; Richter and Khoshgoftaar, 2019). However, other models have\\nbeen proposed, e.g., based on differential equations (Boonyanunta and Zeep-\\nhongsekul, 2004) or other physical laws (Gu et al, 2001), and authors have\\nargued that other models, such as logarithmic shape can be a better fit (Gu\\net al, 2001; Singh, 2005). For an updated and exhaustive overview of used\\nmodel classes, we refer to the work by Viering and Loog (2023).\\n3.2 Range Estimates of the Learning Curves\\nIt has been recognised that incorporating some notion of uncertainty into the\\nmodel itself is important (Mukherjee et al, 2003). Formally, this amounts to\\nlearn a range estimate function ˆfa : N →R2 such that ˆfa(b) ≡[u, v] with\\nsome pre-defined semantic relationship between µa,b and the interval [u, v]. In\\nFig. 8, this type of estimate is visualised through the orange area.\\nThe semantics of the interval [u, v] depend on what exactly is being mod-\\nelled, which also implies how the model is created. In the earliest known\\nattempt on this matter, Mukherjee et al (2003) model the (believed) interquar-\\ntile range of the actual distribution f (a, b) with this interval, i.e., values that\\nare expected to be observed with a certain probability if sampling at a specific\\nanchor (aleatoric uncertainty). In this approach, two parametric (i.e., inverse\\npower law) models are built, one from the 25 and one from the 75 quantile\\nfor each observed anchor b. Even though the mean does not necessarily lie\\nbetween these quartiles in general, this is the case in a Gaussian distribution,\\nwhich is a sensible assumption as explained above. In contrast, Figueroa et al\\n(2012) use it to model a confidence interval of µa,b (epistemic uncertainty).\\nIn this specific case, only one parametric model is learned, and the confidence\\ninterval around the curve is obtained through analytical rather than stochastic\\ntechniques. The confidence interval-based approach can also be thought of as\\nputting a probabilistic bound on the gap between the predicted performance\\nˆfa(b) and the true value µa,b.\\nAgain, the mere presence of interval-based predictions that express epis-\\ntemic uncertainty should not lead to the conclusion or belief that there is a\\nnecessary relationship to correctness. In particular, an epistemic uncertainty\\nof 0 does not imply a correct prediction. Suppose the model ˆfa is chosen from a\\nclass of which the mean curve µa,· is not a member. In that case, it is guaranteed\\nthat there will be wrong predictions, regardless of the epistemic uncertainty\\nexpressed by the model. But even if µa,· is among the models from which ˆfa\\ncan be built, it can still (and usually will) happen that, based on insufficient\\nobservations, a wrong ˆfa will be picked. In such a case, it is still conceivable\\nthat, depending on the probabilistic model on which ˆfa rests, the epistemic\\nuncertainty would be 0 for anchors b whereas f (a, b) ̸= ˆfa(b); here ˆfa(b) is just\\nSpringer Nature 2021 LATEX template\\nLearning Curves for Decision Making\\n21\\na value since the epistemic uncertainty of 0 means that the interval ˆfa(b) has\\nonly one value.\\n3.3 Distribution Estimates of Learning Curves\\nIn a more ambitious case, we can try to learn a full belief model of the learn-\\ning curve C. Formally, this amounts to learn a distribution estimate function\\nˆfa : N →{p | p is a distribution in the domain of the performance measure}.\\nFig. 8, this model corresponds to the sequence of dashed orange distributions\\n(in this figure, displayed for only 14 values of budget b).\\nAs with the previous approaches, one typically uses parametric functions\\n(e.g., the inverse power law) as a basis but specifies distributions over their\\nparameters instead of a single (based on the maximum likelihood) assignment.\\nThe distribution over parameters then induces a distribution of the space of\\nlearning curves. Such a belief model is, for example, well-defined in a Bayesian\\nframework that defines the posterior distribution of models given the observed\\ndata and assumes a certain model class. This posterior distribution cannot be\\nefficiently computed exactly but approximate it through sampling (Domhan\\net al, 2015; Klein et al, 2017b).\\nClearly, distribution estimates are the most flexible way of modelling uncer-\\ntainty and allow many interesting operations. In particular, one can quantify\\nthe probability that the limit performance of a learning curve will be above\\nor below some threshold τ, which is very useful for confidence-based early\\ndiscarding (Domhan et al, 2015).\\n4 A Framework to Categorise Learning Curves\\nMethods for Decision Making\\nBased on the common ground of learning curves and their models introduced\\nin Sec. 2 and Sec. 3, this section presents a framework for categorising decision-\\nmaking methods that use learning curves. We identify three orthogonal criteria\\nalong which those approaches can be categorised. The first criterion relates to\\nthe decision-making situation in which learning curves are used. We discuss\\nthese situations exhaustively in Sec. 4.1. The second dimension covers the\\ntechnical question that is answered about a learning curve to support the\\ndecision. For example, are we interested in the saturation point or a complete\\nmodel? These technical questions are sketched in Sec. 4.2, and we structure\\nthe literature review of Sec. 5 according to this axis. Finally, different data\\nresources can be used to conduct an analysis with learning curves, e.g., other\\nlearning curves or features describing the datasets or the learning algorithms.\\nThese resources are covered in Sec. 4.3.\\n4.1 Types of Decision-making Situations\\nLearning curves are an important resource in at least three types of decision-\\nmaking situations:\\nSpringer Nature 2021 LATEX template\\n22\\nLearning Curves for Decision Making\\n1. Quantitative Data Acquisition Consider the situation where a data sci-\\nentist has a model trained on a set of observations, the performance is\\nknown, and there is the option to spend additional resources (e.g., money\\nor labelling effort) to obtain additional training observations. The decision\\nthat needs to be made is: The acquisition of how many more labels is eco-\\nnomically reasonable? This question has an obvious connection to the field\\nof active learning, which addresses the question of which instances should\\nbe labelled next (qualitative acquisition). Another question is whether we\\nshould acquire other features instead.\\n2. Early Stopping (of training an independently considered model). In the\\nsituation where one is committed to some specific learner (a learning\\nalgorithm and its hyperparameters), minimising the training effort is a\\nreasonable goal. Specifically, if large amounts of data are available and\\ntraining is costly, the aim is to train until the saturation point is reached.\\nBeing able to detect or predict whether a learner’s performance saturates\\nafter a given number of observations or iterations can support making\\ndecisions on this.\\n3. Early Discarding (in model selection). Similarly, if we want to select from\\nvarious models, we want to stop the evaluation of a candidate when we\\nare sure that it is not competitive to the current best solution. We com-\\npare the learner performance to that of another learner instead of its\\nown performance on more training investment. For example, consider the\\nsituation where the learning curve of an algorithm seems to approach\\nthe saturation point, and we have already seen a superior model before,\\nof which it is unlikely that the current algorithm will improve over. In\\nthis case, we can discard the performance of this learner based on the\\nperformance in relation to other models.\\nThere is a large methodological overlap in creating a decision basis among\\nall these decision-making situations. For example, whether more data points\\nwould be helpful to improve performance is related to the question of the\\ntraining size that should be chosen to minimise training effort. Both questions,\\nat their core, ask for the saturation point of the learning curve.\\nIn the following sections, we will discuss each of the three decision-making\\nsituations in more depth.\\n4.1.1 Quantitative Data Acquisition\\nQuantitative data acquisition focuses on the question of how many training\\nexamples should be considered, given that they are all sampled i.i.d. from the\\nsame source. Quantitative data acquisition does not consider or pay attention\\nto the possibility of acquiring specific instances, which would be considered in\\nqualitative data acquisition. Qualitative data acquisition is mainly studied in\\nthe field of active learning and does not ask whether or how many instances\\nshould be acquired but for which instances a label should be acquired. Since\\nactive learning undermines the i.i.d. assumption, it generates a different type\\nof learning curve and is not covered in this survey.\\nSpringer Nature 2021 LATEX template\\nLearning Curves for Decision Making\\n23\\nThe relevance of learning curves for quantitative data acquisition rises from\\ntheir ability to give insights into intrinsic properties of the data source as\\nwell as into the relationship between the number of training examples and the\\nutility of having that number of samples. On the one hand, intrinsic properties\\nrefer to the intrinsic noise of the data (Cortes et al, 1994), which tells us about\\nthe best possible performance of any learner no matter how much data would\\nbe available from the source. If we know that, with the given data, we already\\nachieve a performance close to the intrinsic noise, then data acquisition should\\nfocus on acquiring additional features instead of new instances. On the other\\nhand, the utility is mainly determined by the cost of acquiring (additional)\\nexamples, the performance obtained with a certain number of samples, and the\\ncost to train a model with a given number of instances (Last, 2007, 2009; Weiss\\nand Tian, 2008). In this economic context, there are mainly five questions that\\ncan be considered:\\n1. Possibility. Can the classification performance be improved by more data?\\n2. Potential. What is the best possible predictive performance given unlim-\\nited training observations?\\n3. Maximization Principle. By how much can the predictive performance be\\nimproved if there is a budget for a fixed number of additional data points?\\n4. Minimization Principle. How many instances are necessary to obtain a\\ncertain degree of predictive performance?\\n5. Utility maximization. Which sample size maximises a given utility func-\\ntion?\\nIn the context of data acquisition, we typically deal with sample-wise curves.\\nFurthermore, one is often not committed to a particular learner; therefore, the\\nperformance measure in the above questions is implicitly the best one of a\\nportfolio of learners. That is, one assumes a set of learners that is considered\\nadmissible for the prediction task due to external restrictions. In general, due to\\nthe ability to parameterise learners, this set is usually infinite. When referring\\nto the portfolio’s performance at a specific anchor point, we are interested in its\\nbest-performing algorithm at that specific point at the learning curve (Mohr\\nand van Rijn, 2023). Of course, if one is committed to one particular learner,\\nthen the situation simplifies to a portfolio of size 1.\\nSince data acquisition is not for free, it is sensible to relate potential pre-\\ndictive performance improvements with the costs to collect the additional\\nlabels. Therefore, instead of looking only at predictive performance, one looks\\nat utility of an anchor point. While performance typically only improves with\\nan increased number of observations, the utility also considers acquisition\\ncosts, which negatively affect the utility. Therefore, the goal is to decide how\\nmany instances should be labelled to maximise utility, i.e., how many addi-\\ntional instances are justified before the added value no longer outweighs the\\nadditional costs (Last, 2007; Weiss and Tian, 2008).\\nSpringer Nature 2021 LATEX template\\n24\\nLearning Curves for Decision Making\\n4.1.2 Early Stopping\\nEarly stopping means interrupting the training process of a learner if the learn-\\ning curve has converged. The term ‘early’ refers to the fact that the learning\\nprocess would normally be continued, e.g., because more data is available or\\nother stopping criteria are not yet satisfied. That is, one uses the learning\\ncurve to judge that, despite more available resources or other criteria that\\nwould encourage further training, investing more time will not improve the\\nperformance of the considered model class any further. The training process\\ncan then be stopped early, i.e., earlier than if that criterion would not be used.\\nFig. 9 shows this logic in the left (red) part in which the blue learning curve\\nof learner a is used to detect that not all the data is necessary and only 500\\ntraining instances are used (to save training time).\\nEarly\\nstopping\\ncan\\nbe\\napplied\\nto\\nboth\\nsample-wise curves\\nand\\niteration-wise curves. Early stopping in sample-wise curves means retraining\\na model on different training set sizes to create an empirical learning curve\\nwith training set size as the budget. This can make sense if we do not already\\nknow that the saturation point is larger than the available dataset size; oth-\\nerwise we should immediately train on the complete dataset. We can then try\\nto analyse the sample-wise curve of the learner for increasing training sizes\\nand stop as soon as we find that performances do not change significantly\\nbetween two anchors (John and Langley, 1996; Provost et al, 1999). For iter-\\native learners (such as neural networks), an iteration-wise curve is usually a\\nby-product that can cheaply be created in parallel to learning. Hence it might\\nseem more appropriate to do early stopping based on an iteration-wise curve\\nrather than the sample-wise curve. An additional advantage of early stopping\\nin iteration-wise curves is that it can help avoid over-fitting, e.g., in neural\\nnetworks (Bishop, 1995; Goodfellow et al, 2016) or gradient boosting. While\\nit is conceivable that building sample-wise curves, even for iterative learners,\\ncould be useful in some cases, we are not aware of any such work being done\\nfor early stopping (or any other purpose).\\nThe early stopping problem can be addressed retrospectively and pro-\\njectively. Retrospective early stopping means to stop after observing the\\nsaturation point. Projective early stopping means to predict the saturation\\npoint before it is reached and stop precisely at the (believed) saturation\\npoint. The projective approach is particularly important in the case of\\nsample-wise curves.\\nEarly stopping in sample-wise curves and data acquisition might seem sim-\\nilar since both define a sample size at which a process should be stopped.\\nHowever, the concepts are fundamentally different in three ways:\\n1. Stopped Process: Early stopping means to stop a training process (at\\nthe saturation point) run in a machine. In contrast, the decision-making\\nsituation in data acquisition is to stop the data acquisition process (at\\nthe economic saturation point). The latter is sometimes carried out by\\nhumans.\\nSpringer Nature 2021 LATEX template\\nLearning Curves for Decision Making\\n25\\n500\\n1000\\n1500\\n2000\\n2500\\n3000\\n3500\\n4000\\n0.20\\n0.25\\n0.30\\nEarly Stopping\\nstops here\\navaiable data\\nData Acquisition\\nstops here\\nEarly Stopping vs. Stopping in Data Acquisition\\nµa,·\\nµ(arg mina∈A µa,∞,·)\\nMatter of Early Stopping\\nMatter of Data Acqusition\\nFig. 9: Early stopping with sample-wise curves stops the training process of\\na single learner a at its saturation point using its (actually empirical) learning\\ncurve. It shows the learning curve of two learners, the blue curve and the green\\ndashed curve. The latter has the best saturation performance. The available\\ndata marks a restriction to this process. In contrast, data acquisition considers\\nthe available data as the decision variable, and it stops collecting data when\\nall learners have reached their saturation performance.\\n2. Role of Available Data: In early stopping, the available amount of training\\ndata is a given constraint under which early stopping operates, and data\\nacquisition precisely seeks to control this quantity in an economically\\noptimal fashion.\\n3. Used Performance Curve: Early Stopping uses a single learning curve of\\nlearner a to decide upon early stopping of the training of a. Data acqui-\\nsition uses the learning curves of all learners under consideration (i.e., a\\nfinite set A) and considers the budget-wise best performance achievable\\n(by any learner).\\nTo clarify this difference, consider also the right (green) part of Fig. 9. The\\nlearner a may be the best solution available given the 1500 data points (in this\\ncase, a barely needs 500 of them to attain saturation performance). However, if\\nmore data were available, then at least one other learner could take advantage\\nof that additional data and outperform a. The figure shows the curve of an\\noptimal learner a∗∈arg mina∈A µa,∞that has the best performance if no\\nlimit is posed on the available training data (as in Cortes et al (1994)). Only\\nafter 3000 instances, no learner will improve the overall possible performance\\nanymore; therefore, at this point, the data acquisition process stops.\\n4.1.3 Early Discarding\\nIn many setups, the learner itself is a matter of optimisation. Consider the\\nsituation where we have a (possibly infinite) set of learners, e.g., a finite set of\\nalgorithms, each of which can be instantiated with a possibly infinite number\\nof hyper-parametrisations. The task is to find the (hyper-parametrised) learner\\nwhich performs best for the given data of size n in the sense that it creates,\\nSpringer Nature 2021 LATEX template\\n26\\nLearning Curves for Decision Making\\non average, the best model. Formally, if A is the (infinite) set of parametrised\\nlearners, the goal is to find\\narg min\\na∈A C(a, n).\\n(7)\\nThis task is commonly known as model selection.\\nWhile it is uncommon in literature to be so explicit and describe the model\\nselection problem through the value of the learning curve at some sample\\nsize, this formulation is rather precise and insightful. It emphasises that which\\nlearner is best might depend on the number of available training points. Note\\nthat one needs to separate some portion of the data for validation in practice\\nto estimate model performances. In other words, most approaches in practice\\ndo not even address the above problem but instead\\narg min\\na∈A C(a, ⌈αn⌉),\\n(8)\\nwhere α ∈]0, 1[ (open interval) is the training portion, typically between 70%\\nand 90%, where the remaining portion of 1 −α is used to estimate C(a, ⌈αn⌉),\\ntypically in some (possibly repeated) hold-out validation.\\nWe could conceive that this procedure of estimating C(a, ⌈αn⌉) might\\ninvolve the construction of an empirical learning curve as a sub-routine or\\non-the-fly. First, if a is an iterative learner, then the model performance\\nC(a, ⌈αn⌉) = C(a, ⌈αn⌉, t∗) is the performance of the iteration-wise curve at\\nsome point t∗where the learning process is stopped. Therefore, the whole\\niteration-wise curve C(a, ⌈αn⌉, t∗) is available for analysis. Second, even if\\na is not incremental, one could create an schedule {α1, .., αk} of increasing\\nαi ≤α and thereby create a sample-wise curve (Mohr and van Rijn, 2023).\\nSuch a sample-wise curve would also offer the perspective, via extrapolation,\\nto address Eq. (7) rather than just Eq. (8).\\nIn the light of the availability of such a (partial) empirical learning curve,\\nearly discarding is the practice of aborting the performance estimation proce-\\ndure of a candidate as soon as it becomes apparent from that curve that the\\ncandidate cannot be the solution to the above optimisation problem.\\nFormally, this is to drop a candidate a as soon as the criterion\\nC(a, bref ) > min\\na∗∈A C(a∗, bref )\\n(9)\\ncan be verified, where bref is usually n or ⌈αn⌉. In other words, as soon as\\nit can be shown that a is not the best learner of all possible learners A, no\\nfurther resources should be committed to training learner a. Note that, even\\nthough the terms are frequently mixed up in literature, this is very different\\nfrom early stopping, in which the convergence of the curve of a single learner\\nis considered in isolation (cf. Fig. 1).\\nEarly discarding has been applied to both observation (Adriaensen et al,\\n2023; Mohr and van Rijn, 2021, 2023; Ruhkopf et al, 2023) and iteration (Adri-\\naensen et al, 2023; Domhan et al, 2015; Klein et al, 2017b; Ruhkopf et al, 2023;\\nSpringer Nature 2021 LATEX template\\nLearning Curves for Decision Making\\n27\\nSwersky et al, 2014) learning curves. In the first case, one is sampling from\\nC(a, ·) at different anchors n and hopes to be able to drop sub-optimal can-\\ndidates at n ≪bref anchors (much) smaller than the target size bref . In the\\nsecond case, one always uses the complete dataset (or at least all data desig-\\nnated for training, say n), observes samples of C(a, n, ·) for different anchors\\n(maybe epochs) t, and seeks to avoid convergence if it can be foreseen that\\nthe convergence performance will be sub-optimal.\\nEarly discarding is more aggressive than early stopping because it does not\\nneed to wait until the learning curve converges. On the contrary, one tries to\\navoid reaching convergence since this is considered a waste of resources in the\\ncase that the learner performs sub-optimal. In an extreme case and depending\\non available knowledge about learners (cf. Sec. 4.3), one could only use a single\\npoint of an empirical learning curve to discard a candidate.\\nSituations in which early discarding plays a role can be further classified\\ninto horizontal and vertical scenarios (and a mixture of the two):\\n1. Horizontal Model Selection. Horizontal model selection implies an apriori\\nfixed finite set of learning algorithms, from which one has to be selected.\\nEmpirical learning curves are grown iteratively for the whole set or shrink-\\ning subsets of it. Successive halving and related works are a prominent\\nexample of horizontal decision making (Van den Bosch, 2004; Jamieson\\nand Talwalkar, 2016). However, these approaches only consider the last\\nanchor point (rather than the complete learning curve).\\n2. Vertical Model Selection. Vertical means that the set of learners is gener-\\nally not limited to a finite set, and the set of evaluated learner candidates\\nevolves over time (i.e., not fixed apriori). Learners are evaluated one after\\nanother. Each learner is evaluated in an iterative fashion to grow a learn-\\ning curve and allow for early discarding. Examples are the early discarding\\nroutine for deep networks by Domhan et al (2015) or, more generally, for\\nlearning curve cross-validation (Mohr and van Rijn, 2021, 2023).\\n3. Diagonal Model Selection. This case is similar to the vertical decision-\\nmaking situation with the difference that one does allow to continue the\\nevaluation of a candidate at a later point. Hence, candidates are not\\nevaluated one after another, but the evaluation of different candidates\\ncan be interleaved. Examples are Bayesian optimisation-based approaches\\nto pause and continue evaluations of (not necessarily iterative) learn-\\ners (Klein et al, 2017a; Swersky et al, 2014). Non-iterative learners must\\nbe trained from scratch with the increased budget.\\nAnother approach that addresses this type of decision-making situa-\\ntion is Hyperband (Li et al, 2017) and Bayesian optimisation based on\\nprogressive sampling (Zeng and Luo, 2017). However, neither of these\\napproaches considers learning curves even though they implicitly con-\\nstruct them. Decisions are taken based on the observations of the largest\\nanchor point considered so far.\\nSpringer Nature 2021 LATEX template\\n28\\nLearning Curves for Decision Making\\n4.2 Technical Questions Asked About Learning Curves\\nA plethora of questions can be asked about learning curves. Fig. 10 gives an\\noverview of these questions. The figure is organised in three layers (depth\\ndimension) corresponding to the three types of estimates discussed in Sec. 3.\\nEach layer consists of a set of questions that can be posed about learning\\ncurves. From bottom to top, the questions are ordered by complexity, and\\nan arrow from one question to another indicates that the question with the\\nincoming arrow is more general. Answering the more general question also\\nimplies answering the less general question.\\nIn the simplest case, we can answer a binary question. There are four\\nrelevant questions, i.e., (i) whether some specific anchor point, e.g., the dataset\\nsize, is beyond the saturation point (bsat ≤bref ), (ii) whether the performance\\nof a learner at the saturation point is better than some baseline τ (psat ≤\\nτ), (iii) whether the performance pref := C(a, bref ) at some reference point\\nbref is better than some threshold τ, or (iv) whether a specific anchor point\\nis beyond the utility-based stopping point (bu\\nsat < bref ). To our knowledge,\\nthe only approaches in this category are those implicitly answering question\\n(iii) by discarding candidates that are not believed to be competitive (see,\\ne.g., Jamieson and Talwalkar, 2016; Petrak, 2000; Zeng and Luo, 2017); here\\nτ = mina∈A C(a, bref ) is the (unknown) best performance of any learner on\\nthe target size.\\nA family of slightly more general questions tries to order a set A of learning\\nalgorithms w.r.t. their performance at some (future) anchor bref . We denote\\nthis ordering as πa∈A ∼C(a, bref ). Here, bref is typically the maximum avail-\\nable training data, even if an iteration-wise curve is considered because then\\nthis is the termination performance of that curve. In the simplest case, we could\\nask for a concrete pair of two learning algorithms a1, a2 whether C(a1, bref ) ≥\\nC(a2, bref ), i.e., which will perform better at some reference point. For example,\\nthe work by Leite and Brazdil (2005) answers this question. If this question is\\nsimultaneously asked for a set of or even all possible pairs of algorithms, one\\nasks for a partial or even the full ranking πa∈A ∼C(a, bref ) of algorithms. A\\nparticular case is to ask only for the best algorithm a∗= arg mina C(a, bref ),\\nwhich implicitly answers that C(a∗, bref ) ≤C(a, bref ) for any learner a but\\nwithout explicitly asking for any other comparisons. Still, the comparison is\\nmerely qualitative, and answering this question does not necessarily require to\\nquantify any aspect of any learning curve.\\nThe above questions are merely qualitative and not quantitative, which\\ngives rise to a third level of complexity, where the concrete values of\\nbsat (Provost et al, 1999), psat (Cortes et al, 1993), pref (Baker et al, 2018;\\nChandrashekaran and Lane, 2017; Leite and Brazdil, 2003, 2004), or bu\\nsat (Weiss\\nand Tian, 2008) are being modelled. Here, the reference performance pref is the\\nperformance at a fixed reference point, often the number of training samples\\navailable for cross-validation.\\nAll questions up to this point produce closed answers in the sense that the\\nanswer is either a boolean value, a number, or a finite ranking of candidates.\\nSpringer Nature 2021 LATEX template\\nLearning Curves for Decision Making\\n29\\nPortfolio of curves\\nWhat is C\\nWhat is U\\nCurves of learner\\nWhat is C(a, ·) or C(a, ·, ·)\\nWhat is U(a, ·) or U(a, ·, ·)\\nConservative bound on curve\\nDetermine C(a, ·) or C(a, ·, ·)\\nDetermine U(a, ·) or U(a, ·, ·)\\nValue\\nWhat is bsat\\nWhat is psat\\nWhat is pref\\nWhat is bu\\nsat\\nRanking\\nWhat is\\nπa∈A ∼C(a, bref )\\nBinary\\nbsat ≤bref\\npsat ≤τ\\npref ≤τ\\nbu\\nsat ≤bref\\nDistribution estimates\\nRange estimates\\nPoint estimates\\nFig. 10: Technical questions that can be asked on learning curves.\\nAt the fourth level, the task is to make assertions about arbitrary points of\\na learning curve. However, the answers do not yet refer to the value of the\\nlearning curve itself but only a bound on those values.\\nAt a fifth level, we could eventually ask for a model of the whole learning\\ncurve of a learner a, i.e., C(a, ·) for sample-wise curves (Figueroa et al, 2012;\\nFrey and Fisher, 1999; Gu et al, 2001; John and Langley, 1996; Mukherjee\\net al, 2003) or C(a, n, ·) for iteration-wise curves given a fixed (sample) anchor\\nn (Cortes et al, 1993; Domhan et al, 2015). We can ask a similar question for\\nthe utility curve. While the question is on the same level, it is more general,\\nas it is based on the learning curve itself (Last, 2007, 2009; Weiss and Tian,\\n2006) and combines it with other information such as acquisition costs.\\nFinally, at the sixth and most general level, we could ask for a model of the\\nwhole performance function C, i.e., the model of the learning curves across all\\nlearners (Klein et al, 2017a,b; Swersky et al, 2014). Analogously, this question\\ncould be asked for the whole utility function U, which is arguably the most\\ncomplex and general question that can be asked, although we are not aware\\nof any works that have done so.\\nSpringer Nature 2021 LATEX template\\n30\\nLearning Curves for Decision Making\\nSince all of the above questions are answered based on observational statis-\\ntics, we can also consider noise and uncertainty aspects for the quantitative\\nquestions. In the simplest case (blue layer, cf. Sec. 3.1), we only get point esti-\\nmates, i.e., the estimate of C at one or a set of points. Since these estimates\\nare always afflicted with uncertainty, it is reasonable to ask for quantifica-\\ntions of this uncertainty. One form is to express strict bounds as in the fourth\\nlayer, which may be derived from assumptions about the learning curve shape,\\ne.g., convexity (Mohr and van Rijn, 2021, 2023). Another form is to express\\nprobabilistic bounds like confidence intervals around C (Figueroa et al, 2012;\\nKoshute et al, 2021) (cf. Sec. 3.2). In the most general form, we can ask for\\na full belief model of the learning curves, specifying a probability distribution\\nover the values of C at one point or a set thereof (Domhan et al, 2015; Klein\\net al, 2017a,b; Swersky et al, 2014) (cf. Sec. 3.3).\\nIt is common practice to solve relatively simple questions by implicitly\\nanswering more complex ones. For example, a typical question in the context\\nof model selection is whether the performance of a candidate learner at some\\ngiven data or in the limit will beat a known baseline (Domhan et al, 2015;\\nLeite and Brazdil, 2005; van Rijn et al, 2015). This is the binary question of\\npref ≤p∗, where p∗is the best-known performance. Often, this question is\\nanswered by estimating pref (maybe plim) explicitly and then comparing it to\\np∗(Leite and Brazdil, 2005; van Rijn et al, 2015), which is an answer to a\\nslightly more complicated question. Domhan et al (2015); Swersky et al (2014)\\nbuilt an explicit curve model, for estimating plim and pref , respectively. The\\napproach answers this binary question by building an entire learning curve\\nmodel and then derives the binary answer from it. The rationale behind this is\\nthe notion that one often needs rather complex models to find a high-quality\\nanswer to a simple question, and it is just a side effect that one can then even\\nanswer other questions with those models.\\n4.3 Used Data Resources for Inference\\nAbove we have discussed a series of questions that can be asked about learning\\ncurve properties, which in turn are important for decision making in a spe-\\ncific context. Of course, answering these questions requires specific informative\\nresources. In a concrete decision-making situation, we are typically confronted\\nwith a dataset and a learner or a portfolio of learners. We call this the target\\ndataset and the current learner. That is, we want to say something about the\\nlearning curve of the current learner in a domain in which we have a finite (the\\ntarget) dataset d available.\\nFig. 11 shows the types of data resources that can be used to answer ques-\\ntions about learning curves. We can utilise empirical learning curves gathered\\non the target dataset, empirical learning curves gathered on other datasets,\\ndataset meta-features, and features describing the learners. Learning curves\\ngathered on the target dataset come at a particular computational cost, as they\\nneed to be generated during the process. Typically, when modelling the current\\nlearner on the target dataset, a partial empirical learning curve is constructed,\\nSpringer Nature 2021 LATEX template\\nLearning Curves for Decision Making\\n31\\nData\\nResources\\nDataset\\nMeta-\\nFeatures\\nLearner\\nFeatures\\nTarget\\nDataset\\nOther\\nDatasets\\nCurves on\\nCurrent\\nLearner\\nCurves on\\nOther\\nLearners\\nCurves on\\nCurrent\\nLearner\\nCurves on\\nOther\\nLearners\\nFig. 11: Taxonomy of data resources for learning curve analysis\\nwhich can then be step-wise extended or discarded (Domhan et al, 2015; Leite\\nand Brazdil, 2003; Provost et al, 1999). When using learning curves of other\\ndatasets, those are usually available up to a large portion of the dataset size\\n(with a specific schedule of anchors) in the case of sample-wise curves or until\\nconvergence in the case of iteration-wise curves. This is because these curves\\ncould be prepared offline before the target dataset became available (Leite and\\nBrazdil, 2003, 2010; van Rijn et al, 2015). Both learning curves of the current\\nlearner and other learners can be utilised for this. In the context of model\\nselection, various learners are usually evaluated. Therefore we can acquire var-\\nious learning curves of other learners on the target dataset (Baker et al, 2018;\\nChandrashekaran and Lane, 2017; Klein et al, 2017a,b; Swersky et al, 2014).\\nOther types of data resources that can be used are meta-features on\\nthe datasets and learner features. These are measurable qualities of the\\ndataset and learner, respectively, and these can indicate how similar specific\\ndatasets (or learners) are. These give the decision-making algorithm a sense\\nof which learning curves are more informative for the current learner and tar-\\nget dataset. To the best of our knowledge, the only line of research utilising\\nmeta-features for learning curve modelling is the work of Leite and Brazdil\\n(2008, 2010); Ruhkopf et al (2023). The description of learners through fea-\\ntures for the sake of model prediction is specifically prevalent in the analysis\\nof iteration-wise curves (Baker et al, 2018; Klein et al, 2017b; Swersky et al,\\n2014). The development and analysis of meta-features is a research field in its\\nown right; for more information, we refer the reader to Brazdil et al (2022).\\nSpringer Nature 2021 LATEX template\\n32\\nLearning Curves for Decision Making\\n5 Literature Review on learning curve\\nextrapolation methods\\nThis section presents the literature review that covers methods to model and\\nutilise learning curves. We organise approaches based on the key problem they\\nresolve on a rather abstract level and independent of the purpose or type of\\ndecision-making situation in which they were presented. We organise it along\\nthe framework presented in Fig. 10, particularly along the axis that reflects\\nthe technical question asked about a learning curve. The motivation to not\\nuse the type of decision-making situation, which is also a prominent property\\nof these methods, is that the same approach can be used in different decision-\\nmaking situations. For example, Leite and Brazdil (2004) present and motivate\\nan approach to identify the portion of some given data that should be used for\\ntraining (i.e., determine the saturation point), but the same approach could be\\nused to determine how much more data would be needed to obtain saturation\\nperformance. Similarly, Domhan et al (2015) present an approach that aims to\\ndecide during training whether a neural network will become competitive (ask\\nfor saturation performance); however, they did this by modelling the complete\\nlearning curves.\\nFig. 12 gives an overview of all the methods categorised in this framework.\\nWe make the following two observations.\\n• Most learning curve methods address the early discarding / model selection\\ndecision-making situation. This implies that there is an opportunity for more\\nresearch on, for example, data acquisition or early stopping. We note that\\nresearch towards active learning provides many approaches that handle data\\nacquisition (which we do not cover), which might serve as a basis for a\\nliterature search.\\n• Second, most methods are centred around the middle levels of problem com-\\nplexity they address, i.e., predicting the actual value of a learner at a certain\\npoint and predicting the complete curves of a learner. It seems logical that\\nthere are benefits for exploiting the situation where either the complete port-\\nfolio is modelled (e.g., the benefit of parameter sharing, the opportunity of\\nmodel acquisition) or the binary problem is solved (because of the simplicity\\nof the problem definition). Methods addressing the binary situation, such as\\nSuccessive Halving and Hyperband, have attracted quite some attention.\\nTo further structure the overview, we divide the whole literature on methods\\nfor learning curves into two roughly even groups according to the usage of\\na learning curve model as described in Sec. 3. Approaches in the first group\\ndo not employ a learning curve model. These approaches address the ques-\\ntions defined in the four lowest levels of the framework (see Sec. 5.1). In\\ncontrast, approaches of the second group, i.e., which employ a learning curve\\nmodel, address questions in the top two levels of the framework (see Sec. 5.2).\\nApproaches with a curve model are more general, but that does not neces-\\nsarily mean they give better answers to simpler questions. In fact, Kielh¨ofer\\nSpringer Nature 2021 LATEX template\\nLearning Curves for Decision Making\\n33\\nData Acquisition\\nEarly Stopping\\nEarly Discarding\\n/ Model Selection\\nUnclear / Mixed\\nPortfolio\\nSwersky et al\\n(2014); Wistuba\\nand Pedapati (2019)\\nKlein et al (2017a,b)\\nCurves of Learner\\nRichter and Khosh-\\ngoftaar (2019)\\nCortes et al (1993);\\nJohn and Langley\\n(1996); Gu et al\\n(2001); Mukherjee\\net al (2003); Figueroa\\net al (2012); Domhan\\net al (2015); Cardona-\\nEscobar et al (2017);\\nAdriaensen et al (2023);\\nKielh¨ofer et al (2024);\\nEgele et al (2024)\\nFrey and Fisher\\n(1999); Boonyanunta\\nand Zeephongsekul\\n(2004); Singh (2005);\\nHess and Wei (2010);\\nKolachina et al (2012);\\nKoshute et al (2021)\\nConservative\\nbound on curve\\nSabharwal et al\\n(2016); Mohr and\\nvan Rijn (2023)\\nValue /\\nQuantitative\\nprediction\\nWeiss and Tian\\n(2006, 2008)\\nMeek et al (2002);\\nBishop (1995); John\\nand Langley (1996);\\nProvost et al (1999);\\nLeite and Brazdil\\n(2003, 2004); Ng\\nand Dash (2006)\\nChandrashekaran\\nand Lane (2017);\\nBaker et al (2018)\\nLast (2007, 2009);\\nCortes et al (1994)\\nRanking /\\nQualitative\\nprediction\\nLeite and Brazdil (2005,\\n2007, 2008, 2010);\\nvan Rijn et al (2015);\\nRuhkopf et al (2023)\\nBinary\\nPetrak (2000); Van den\\nBosch (2004); Jamieson\\nand Talwalkar (2016);\\nLi et al (2017); Zeng\\nand Luo (2017)\\nFig. 12: Overview of the methods (indicated by the bibliographic reference)\\ncovered and categorised in this framework. Each method is categorised along\\nthe problem type they solve (vertical axis) and the type of decision-making\\nsituation they explicitly address (horizontal axis). Some methods are employed\\nto address multiple decision-making situations. Some papers do not explicitly\\nstate which decision-making situation is being addressed, or address multiple.\\nThese are categorised in the last column. Several models can be used for more\\ndecision-making situations than the original paper evaluated them on. For\\nexample, the portfolio approaches could in theory be used for any decision-\\nmaking situation.\\net al (2024) show that the model-free MDS approach discussed in Sec. 5.1.2 in\\nspecific situations outperforms a parametric model such as the ones presented\\nby Gu et al (2001) discussed in Sec. 5.2.1. In total, the approaches cover 10\\nof the questions discussed in Sec. 4.2, which are organized in the following\\nsub-sections:\\n5.1. Approaches without learning curve model:\\n1. Is the target performance of a learner worse than the one of the best\\nlearner, i.e., pref > mina∈A C(a, bref )?\\n2. What is the ordering πa∈A ∼C(a, bref ) of the learning algorithms w.r.t.\\ntheir performance at some target anchor?\\n3. What is the saturation performance (psat)?\\n4. What is the saturation point (bsat)?\\n5. What is the utility-based stopping point (bu\\nsat)?\\nSpringer Nature 2021 LATEX template\\n34\\nLearning Curves for Decision Making\\n6. What is the value of the learning curve at a specific (fixed and known)\\npoint? (C(a, n) or C(a, n, t))\\n7. What is a lower/upper bound of the learning curve at any point?\\n(C(a, ·) or C(a, n, ·))\\n5.2. Approaches with a model for the learning curve:\\n1. What is the value of the learning curve at any (queryable) point?\\n(C(a, ·) or C(a, n, ·))\\n2. What is the utility at an arbitrary anchor point? (U(a, ·) or U(a, n, ·))\\n3. What is the value of the learning curve at any queryable point of any\\nqueryable learner? (C(·, ·))\\nWe organise the rest of this section exactly according to this scheme.\\nFor each approach, we always consider the most general problem it solves,\\nindependently of how this solution is used in the context of a paper. For\\nexample, Domhan et al (2015) decide whether the saturation performance of\\na learner beats some threshold (question at the binary level) but develop a\\nlearning curve model and are hence discussed alongside the approaches for\\nquestion 1 in Sec. 5.2.1.\\n5.1 Approaches Without Learning Curve Models\\nMany interesting questions related to learning curves can be addressed without\\neven building an explicit learning curve model. None of the questions in the\\nlower layers in Fig. 10 necessarily requires a learning curve model. Fig. 13\\nshows a summary of all the approaches we are aware of, which make significant\\nassertions or decisions related to learning curves without building a learning\\ncurve model.\\n5.1.1 Prediction of Candidate Competitiveness (Binary)\\nIn this section, we discuss approaches that answer the early stopping criterion\\nposed in Eq. 9 without using a learning curve model. The early discarding\\ncriterion can be seen as an instantiation of the binary question pref ≤τ,\\nwhere τ = mina∗∈A C(a∗, bref ) is the best value that any learner on the\\navailable resources. This (conceptually simple) question is often answered by\\napplying sophisticated learning curve models. The usage of learning curve\\nmodels is understandable since, at the time of the decision, neither pref nor\\nmina∗∈A C(a∗, bref ) is known; therefore, extrapolating learning curves offers a\\npossibility to make assessments about these quantities. However, it is also pos-\\nsible to say something about the early discarding condition without a learning\\ncurve model.\\nAs far as we know, the only type of approach in this category is hori-\\nzontal early discarding with an implicit affirmation of the early discarding\\ncriterion that only uses the last anchor of the empirical learning curve. That\\nis, during the model selection process, all remaining members of a candi-\\ndate set A are trained to some budget. This budget can be either a sample\\nsize (sample-wise curves), learning iterations, or time (iteration-wise curves).\\nSpringer Nature 2021 LATEX template\\nLearning Curves for Decision Making\\n35\\nCortes et al (1994)\\nBishop (1995)\\nJohn and Langley (1996)\\nProvost et al (1999)\\nPetrak (2000)\\nMeek et al (2002)\\nLeite and Brazdil (2003)\\nLeite and Brazdil (2004)\\nVan den Bosch (2004)\\nLeite and Brazdil (2005)\\nNg and Dash (2006)\\nWeiss and Tian (2006)\\nLeite and Brazdil (2007)\\nLeite and Brazdil (2008)\\nWeiss and Tian (2008)\\nLeite and Brazdil (2010)\\nvan Rijn et al (2015)\\nJamieson and Talwalkar (2016)\\nSabharwal et al (2016)\\nLi et al (2017)\\nZeng and Luo (2017)\\nChandrashekaran and Lane (2017)\\nBaker et al (2018)\\nRuhkopf et al (2023)\\nMohr and van Rijn (2023)\\n1994\\n1995\\n1996\\n1999\\n2000\\n2002\\n2003\\n2004\\n2005\\n2006\\n2007\\n2008\\n2010\\n2015\\n2016\\n2017\\n2018\\n2022\\n2023\\nApproaches Without Explicit Learning Curve Model\\nssat\\nsu\\nsat\\npref > τ\\nπa∈A ∼C(a, sref)\\nC(a, ·)\\nC(a, sref)\\nplim\\nFig. 13: Citations across approaches without learning curve models. The\\ncolours indicate the question addressed about learning curves. Normal arrows\\nindicate that the paper cited the other paper, and coloured arrows indicate an\\nexperimental comparison with previous approaches.\\nThen, some portion of candidates A−is removed from A based on the last\\nvalue of their respective curve. The implicit assumption is that C(a, bref ) >\\nmina∗∈A C(a∗, bref ) for every a ∈A−.\\nTo our knowledge, the first algorithm in this line was introduced in the\\nWrapped Progressive Sampling procedure (WPS) by Van den Bosch (2004).\\nIn this approach, a dynamically computed subset of the candidate set A is\\ndiscarded (instead of a constant fraction as 50%). WPS creates a histogram\\nwith ten bins b1, .., b10 of the candidates in A based on their validation accuracy\\nto decide which candidates are discarded. Then WPS identifies the largest\\nindex i−= max{i ∈N, < 10 : |bi| < |bi+1|} of a bin that has fewer elements\\nthan its successor. Then, all candidates in bins with an index lower or equal i−\\nare dropped. In an extreme case, all except one candidate might be dropped\\nright in the first iteration, e.g., if |b10| = 1 and |b9| = 0.\\nMore recently, successive halving (Jamieson and Talwalkar, 2016) and\\nhyperband (Li et al, 2017) have been introduced, which are both adequate\\nmethods based on a simple concept. Successive halving considers a set of can-\\ndidate models A, which all receive an initial budget. It iteratively drops 50%\\nof the candidate set A while doubling the remaining candidates’ budget until\\na single candidate model remains. Hyperband is a series of successive halving\\nbrackets, where each bracket is initialised with an increased initial budget and\\na new initial set of candidates. Zeng and Luo (2017) proposed an extended\\nversion in which all the candidates that perform worse than the best candi-\\ndate by some constant and pre-defined margin are discarded. Interestingly, the\\nabove approaches operate without a learning curve model for extrapolation\\nand refrain from using the existing observations other than comparing the last\\nseen values. Therefore, neither predictions nor observations of recent trends\\nare being utilised. Nonetheless, these methods perform well empirically and\\ncome with theoretical guarantees.\\nSpringer Nature 2021 LATEX template\\n36\\nLearning Curves for Decision Making\\n5.1.2 Candidate Ranking\\nThe methods in this section aim to rank the learning algorithms with respect to\\ntheir (expected) performance at the full dataset size. In our scheme in Sec. 4.2\\nand Fig. 10, we denote this question as πa∈A ∼C(a, bref ). Depending on the\\navailable resources (cf. Sec. 4.3), such a ranking can be based on the other\\nlearning curves available from other contexts and the explicit characterisation\\nof such contexts, e.g., features that describe datasets or algorithms.\\nRanking Without Context Description\\nThe first approach we are aware of to address this problem was metalearning\\non data samples (MDS) presented by Leite and Brazdil (2005) through the\\nnotion of comparing two learners, i.e., a ranking of two. Given a dataset,\\nMDS decides which of two algorithms is the better choice on a given dataset.\\nTherefore, it can be used for early discarding. The authors specifically use an\\nSVM and a C5 decision tree but rightfully claim that any algorithm could be\\nused. The formal basis of the work is the same as the one introduced in their\\nprevious work (Leite and Brazdil, 2003, 2004). Similar to previous work, it\\nassumes that empirical learning curves (with standardised anchor sizes) for the\\nlearner under examination are already known for other datasets. Additionally,\\nit builds upon the idea of quantifying the distance between the target and\\nthe other datasets based on the sum of squared distances over the already-\\nknown performances at anchors of the target dataset. Once the most similar k\\nlearning curves have been identified, MDS assigns a score to each learner that\\nis the mean accuracy of its k nearest neighbours (at the final anchor). It then\\nselects the algorithm with the higher score.\\nLeite and Brazdil (2005) acknowledge that this method may result in poor\\nrankings because even the closest learning curve on other datasets can still\\nbe substantially different and propose learning curve adaption as a remedy.\\nInstead of forming the mean directly over the target anchors of the nearest\\nneighbour empirical learning curves, the authors first scale those curves to\\nmake them more similar to the shape already observed on the target dataset.\\nTo this end, they compute a scaling constant under which the overall anchor-\\nwise distance is minimised and then multiply all the scores with this constant.\\nThis version of the MDS algorithm is called AMDS (probably for Adaptive\\nMDS).\\nInterestingly, one can argue that this adaption technique could be applied\\neither before or after determining the k nearest neighbours. Doing it before\\ncould lead to other (and better) nearest neighbours because then the neigh-\\nbours are determined more with respect to the shape of the learning curve, and\\nthe offset plays much less of a role. However, in the above paper, the adaption\\nis done after retrieving the neighbours.\\nThe authors extended the approach by creating an online sampling scheme\\nwith the SetGen algorithm (Leite and Brazdil, 2007). SetGen is an online\\nadaption of AMDS in that, after each acquired anchor, it is decided whether\\nand which anchor should be evaluated with each algorithm; this considers both\\nSpringer Nature 2021 LATEX template\\nLearning Curves for Decision Making\\n37\\nthe (believed) accuracy and the runtime. This procedure can be seen as a way\\nof racing between the algorithms. The potential of each additional amount of\\nbudget is judged based on the metalearning database.\\nAn implicit assumption of all approaches in this line of research is that the\\ndatasets in the database from which performances are extracted need to be\\nat least as big as the target dataset. This issue was first explicitly treated in\\nthe Pairwise Curve Comparison approach (PCC) (van Rijn et al, 2015). This\\nalgorithm builds upon the works of Leite and Brazdil (2010) and implements\\na voting scheme to identify the best learning algorithm of a portfolio; votes\\nare distributed based on wins, which are determined based on the predicted\\nperformance at the complete dataset d. van Rijn et al (2015) explicitly discuss\\nthe issue if the sizes of datasets in the database and the target dataset are not\\nidentical. In general, the point for which predictions must be made is typically\\nnot one of the anchors; it is typically not a power of 2 but rather 90% of\\nthe given dataset size (due to the holdout scheme). A remedy is to resort to\\nthe closest available anchor in the schedule. However, if the highest anchor\\navailable for another dataset is much smaller than the required training size\\nof the target dataset, then it is unclear how that curve should be used.\\nRanking With Context Description\\nThe first work we are aware of that realises an explicit context description was\\nproposed by Leite and Brazdil (2008, 2010). Similar to the methods discussed\\nearlier, these methods select the k nearest datasets to measure the relevance\\nof known complete learning curves of other datasets for performance predic-\\ntion on the target dataset. The main difference is that, in addition to the\\ncontribution of the partial learning curve itself to the distance, they also use\\nthe distance between the datasets in terms of their meta-features. More pre-\\ncisely, they compute the Manhattan distance between seven range-normalised\\ndataset meta-features, e.g., dataset size, number of symbolic and numerical\\nattributes, etc. The overall distance between the datasets is then the sum of\\nthe distance between the partial learning curves and the distance in terms of\\nmeta-features. This work was marginally refined in the Selection of Algorithms\\nusing Metalearning approach (SAM), which applies the same logic but assigns\\na weight to each of the two distance sources (Leite and Brazdil, 2010); the\\nweight is however implicitly assumed to be set to 0.5.\\nA recent and entirely different approach to candidate ranking is the MASIF\\ntransformer framework (Ruhkopf et al, 2023). This approach takes partial\\nlearning curves of different learners on the current task, which may have poten-\\ntially different lengths and combines them with dataset meta-features in order\\nto predict latent utility values of each learner as expected for the complete\\ndataset. The transformer is trained based on previous experiences on datasets\\nfor which true rankings among the learners have been computed for the com-\\nplete dataset. It is unclear to which degree the utility values predicted by\\nthe transformer resemble the actual performance of the learners at the target\\nSpringer Nature 2021 LATEX template\\n38\\nLearning Curves for Decision Making\\nsize, but the paper suggests that the ordering πa∈A ∼C(a, bref ) of the learn-\\ners according to these scores is relatively faithful to the true ordering induced\\nby the actual performances.\\n5.1.3 Identification of Saturation Performance\\nEstimating the limit or saturation performance psat is helpful in two decision-\\nmaking situations:\\n1. for sample-wise curves, it can be used for data acquisition by checking\\nwhether the availability of more data promises to improve predictive\\nperformance significantly.\\n2. for iteration-wise curves, it can be used for early discarding. Given a\\nthreshold p, we can determine whether training a model to convergence\\nwill achieve a generalisation performance of at least p.\\nTo the best of our knowledge, the question on estimates of psat is the only\\nof the above questions that received a substantial amount of theoretical con-\\ntributions. This is not surprising since psat is an asymptotic quantity that\\nis arguably suited for theoretical analysis. The root of this line of research\\nis the statistical mechanics framework (Seung et al, 1992). This and related\\nresearch (Amari and Murata, 1993; Fine and Mukherjee, 1999; Murata et al,\\n1992) consider a type of capacity curve in which asymptotic properties of the\\nlearning curve are expressed in terms of the number of parameters, usually\\nthose of a neural network. However, a side observation of these works is that\\nthere is a kind of symmetrical behaviour between the train error and the\\nvalidation error (sometimes called generalisation error).\\nCortes et al (1994) take these observations to use the mean of the two\\nempirical curves to estimate psat as soon as the training error starts to rise,\\ni.e., as soon as the model cannot accommodate the training data perfectly\\nanymore. Although this work considers capacity curves to identify the intrinsic\\nnoise level of the data, i.e., the minimum error necessarily made by any learner,\\nthey also report the asymptotic performance of a neural network on a single\\ndata set.\\nTo the best of our knowledge, this is the only approach that estimates\\npsat without building an explicit learning curve model. While several other\\nmethods are capable of estimating psat of iteration-wise curves of neural net-\\nworks (Domhan et al, 2015; Swersky et al, 2014), these rely on full learning\\ncurve models, which are therefore discussed in Sec. 5.2.1.\\n5.1.4 Identification of Saturation Point\\nIdentifying the saturation point bsat of sample-wise curves is useful in the\\nfollowing cases:\\n1. Early-Stopping with sample-wise curves: Which portion of the available\\ndata is necessary to obtain saturation performance?\\nThis is relevant if |d| > bsat or the relationship between |d| and bsat is\\nunknown and training on full d is potentially undesirable.\\nSpringer Nature 2021 LATEX template\\nLearning Curves for Decision Making\\n39\\n2. Early-Stopping with iteration-wise curves: How many iterations are nec-\\nessary until performance converges? This applies to incremental learners,\\nsuch as neural networks.\\n3. Data acquisition: How many additional labelled observations are neces-\\nsary to obtain (near-optimal) performance? This applies if |d| < bsat. The\\nquestion can be posed for a specific learner or a portfolio.\\nRetrospective Approaches\\nThe simplest way of determining the saturation point bsat is to incrementally\\nbuild a learning curve and stop as soon as it is believed that the saturation\\npoint has been exceeded. If we do this, we can estimate that the saturation\\npoint lies between the last two anchors. For iteration-wise curves, determining\\nbsat comes for free as a side-product of the training procedure. It is com-\\nmonly used for training neural networks (Bishop, 1995; Goodfellow et al,\\n2016). On the other hand, it requires restarting and is potentially costly for\\nsample-wise curves.\\nJohn and Langley (1996) define a dynamic sampling approach to determine\\nthe bsat for sample-wise curves. A straightforward approach mentioned in that\\npaper is to observe whether the performance has become worse on the last\\nsampled anchor. If so, one might consider gathering empirical evidence that the\\nsaturation point has been exceeded, i.e., it should be somewhere between the\\nlast two anchors. However, the authors argue that preliminary results indicate\\nthat this approach often stops too early. This is mainly caused by high aleatoric\\nuncertainty, which implies noisy empirical learning curves. On the other hand,\\none could argue that this approach stops far too late because it can require\\nquite some iterations until, by chance, the observed performance is worse than\\nthe one of the last iteration. Therefore, John and Langley (1996) propose also\\na model-based approach to avoid this problem, which we discuss in Sec. 5.2.1.\\nOf course, when having access to such a model, we can query the expected\\nperformance and compare it to the performance at the last anchor.\\nProvost et al (1999) address the stability issues and also (some of) the effi-\\nciency issues of the above trivial approach in a scheme they call progressive\\nsampling. Similar to dynamic sampling (John and Langley, 1996), progressive\\nsampling induces models for each anchor in the schedule until the convergence\\nof the learning curve is detected. There are two main differences between the\\ntwo approaches. First, the authors propose to use geometrical instead of arith-\\nmetic schedules, i.e., a schedule of the form bk instead of bk, where b is a\\nconstant and k is the position of an anchor in the schedule. They prove that\\nevery geometric schedule is asymptotically optimal in terms of runtime; that is,\\nevery such schedule has the same asymptotic runtime as the schedule that eval-\\nuates only on bsat. This optimality proof only holds in the asymptotic calculus;\\nin practice, there are better and less good geometric schedules. For example,\\nProvost et al (1999) propose a dynamic programming approach (called DP),\\nwhich efficiently computes the cost-optimal schedule based on a prior distribu-\\ntion on bsat and a given training runtime model. The second difference is that\\nSpringer Nature 2021 LATEX template\\n40\\nLearning Curves for Decision Making\\nProvost et al (1999) check whether the saturation point has been reached using\\na method called linear regression with local sampling (LRLS), which samples\\nnot only at but also closely around anchors to estimate the slope at an anchor\\nand stop if the slope is close to zero.\\nThe practical benefit of the LRLS scheme is not entirely clear for\\nsample-wise curves. First, since the slope is also based on empirical values,\\nfrom a theoretical viewpoint, it is not clear that the criterion is necessarily bet-\\nter than the naive approach suggested by John and Langley (1996). Second,\\nthe approach is more expensive than the naive approach because more obser-\\nvations need to be sampled; this can be a substantial factor, especially for large\\nanchors. LRLS becomes relevant when applied to learners that have a compu-\\ntational complexity for training that scales worse than linear in the number of\\ntraining points (e.g., Gaussian processes or decision trees). For learners with\\ntraining complexity linear in the number of observations, it will often be more\\nexpensive than evaluating a learner’s performance on the complete dataset.\\nThis can be seen with a simple calculation, in which we assume roughly linear\\ntraining time complexity: Suppose a costly anchor at 40% of the overall data\\nsize. If we draw only two additional samples around this anchor, then the run-\\ntime is around 120% of the runtime we would have had if we had trained on\\nthe complete dataset once. While Provost et al (1999) argue that LRLS only\\nadds a constant factor to the runtime, Sarkar et al (2015) reasonably argue\\nthat this factor is often prohibitive in practice.\\nThe approaches in this section explicitly assume that more data than bsat\\nis available. This implies that they can be used primarily for early stopping\\nscenarios rather than data acquisition scenarios. Still, if a learner does not\\nattain saturation performance on the complete dataset, the approaches can\\ndetect this at the cost of the additional evaluations at the non-final anchors.\\nConcerning the stability of estimates, Beleites et al (2013) point out the\\nnecessity to have an estimate for the confidence interval not only for the per-\\nformance at the anchors in the training schedule but also on the test data.\\nThey argue that confidence intervals are essential when deciding whether rea-\\nsonable generalisation statements can be made for a classifier. This changes\\nthe notion of the stopping point to, perhaps, a confident stopping point. The\\noptimal stopping point may be reached early, but the validation fold sizes may\\nstill be too small to assure stable assertions. Typically, the confidence inter-\\nvals are large on small anchors and then contract for increasing anchor sizes.\\nBased on credible intervals, the authors propose choosing the anchor point\\nthat achieves a sufficiently narrow interval on the test data.\\nNg and Dash (2006) address the impact of class imbalance on the per-\\nformance of a learner. That approach hypothesises that, without further\\nknowledge, the class distribution in which all classes have the same number of\\nobservations is optimal. The authors modified the aforementioned progressive\\nsampling scheme by creating train sets at each anchor such that all classes have\\nthe same distribution. For anchors of sizes that would require more instances\\nof a class than are available in the existing data, random instances of that class\\nSpringer Nature 2021 LATEX template\\nLearning Curves for Decision Making\\n41\\nare replicated until the class balance is established again. Note that while the\\nwork is based on the findings by Weiss and Provost (2003) (cf. Sec. 2.5.2), they\\napply a different strategy. Instead of optimising over the class distribution for\\na given anchor size, they try to find the stopping point under the premise that\\nthe training set will always be balanced.\\nRegarding the stopping point of iteration-wise curves, a common technique\\nis to separate some data that is not used for training but to compute an\\niteration-wise curve online to detect convergence (Bishop, 1995).\\nProjective Approaches\\nA different idea to obtain the saturation point bsat was proposed by Leite and\\nBrazdil (2003, 2004) through the notion of metalearning (Brazdil et al, 2022).\\nSimilar to Provost et al (1999), a geometric schedule is used. The assumption is\\nthat we already know the performances of the current learner at all anchors in\\nthe schedule on other datasets. The idea is to compute, on the target dataset,\\nthe performances only for the very first anchors and then to predict the satura-\\ntion point by aggregating the (known) saturation point on the k most similar\\nlearning curves of the other datasets. The distance measure here is the sum\\nof differences between the curves at the initial anchors; the concrete anchors\\nused in their paper are (91, 128, 181, 256, . . . ), corresponding to the powers\\nof\\n√\\n2. The authors consider different aggregation measures such as mean and\\nminimum (Leite and Brazdil, 2003) and the median (Leite and Brazdil, 2004).\\nThe authors discuss the potential issue that, among the k nearest neigh-\\nbour curves, some or even all of the curves can be substantially different from\\nthe partial learning curve on the target dataset. Using the k nearest learning\\ncurves to predict the stopping point would not work in such cases. Follow-up\\nwork (Leite and Brazdil, 2007) proposes a remedy to this problem, in which\\nthe curves are not used directly but are adjusted via a concept called curve\\nadaptation (discussed in Sec. 5.1.6).\\n5.1.5 Finding the Utility-Based Stopping Point\\nThe problem of identifying the utility-based stopping point was, to our\\nknowledge, first addressed by Meek et al (2002) and was also independently\\ninvestigated by Weiss and Tian (2006). In these papers, a retrospective\\napproach is applied. The idea is similar to the aforementioned concept of pro-\\ngressive sampling (Provost et al, 1999), except that the analysis is done for\\nutility rather than learning curves. In contrast to the learning curve, the utility\\ncurve does not plateau but starts to deteriorate after its peak (see Fig. 4).\\nThe main difficulty with the concept of utility in the context of learning\\ncurves is to find a unifying scale for (i) the costs of data acquisition and training\\ntime and (ii) the model performance. Meek et al (2002) avoid this problem by\\nadopting the notion of implicit utility through the comparison with a baseline.\\nThey stop the algorithm when the ratio between the benefit improvement and\\nthe augmented runtime drops below a pre-defined threshold.\\nSpringer Nature 2021 LATEX template\\n42\\nLearning Curves for Decision Making\\nIn contrast, Weiss and Tian (2006) compute an explicit utility, and the algo-\\nrithm stops as soon as the observed utility decreases for the first time, which\\nis taken as the indicator that the utility-based stopping point has been passed.\\nThe authors adopt the concept of the net utility of a potential classifier, which\\nis the difference (in utility) between predictive performance (under a hypo-\\nthetical number of training instances) and the cost to acquire the (additional)\\ninstances. Notably, the assumption is that the user has no control over the\\ninstances for which labels will be acquired next, which contrasts the approach\\nfrom active learning. To merge different types of inconveniences (i.e., acquisi-\\ntion costs and prediction errors) into a single utility measure, the user has to\\ndefine costs per unit, e.g., costs for acquiring a single usable training instance\\nand costs for making a wrong prediction. Furthermore, the authors consider\\nthe problem of deciding online whether or not to acquire more data and, in\\nthe affirmative case, how many instances should be considered in the acquisi-\\ntion batch before reconsidering. The latter effectively corresponds to deciding\\nupon a progressive sampling scheme (Provost et al, 1999). However, the paper\\ndoes not analyse the effects of fixed costs per batch (such as the computational\\ncosts of training a model), which implies that one could set batch sizes to 1\\nwithout consequence.\\nA consecutive version of that paper also adds the CPU cost for model\\ninduction to the costs of a point on the learning curve (Weiss and Tian, 2008).\\nThe original paper only considered acquisition costs and the prediction error.\\nThis model also seems suitable for iteration-wise curves, where there would be\\nno data acquisition costs.\\nIn all of the above approaches, the usage of the empirically gathered learn-\\ning curves for decision making is minimal. Moreover, the approaches ignore all\\nexcept the last two points on the learning curve. In this sense, and in terms\\nof the stopping point approaches, the above works are retrospective in nature.\\nNo model of the learning curve is built, and no projections of errors on bigger\\ntraining sizes are made, which, for example, could make sense to predict that\\nthis utility peak event will occur in the future or even only in the next iteration.\\nLast (2007) proposes such an approach, which we discuss in Sec. 5.2.2.\\n5.1.6 Performance Prediction at Fixed Point\\nThe problem of predicting the learning curve value is naturally a regression\\nproblem, where the goal is to predict µa,b for a fixed budget b. Essentially,\\ngiven a fixed learner a and the budget expressed in either observations n or\\niterations t, it is about predicting C(a, n) for sample-wise curves, and pre-\\ndicting C(a, n, t) for sample-wise curves. The attributes are the performance\\nvalues at different (cheap) anchors and potentially additional contextualising\\nattributes. Using these attributes, one explicitly or implicitly generalises over\\ndatasets or learning algorithms (or both). Models that generalise over datasets\\nare typically called meta-models and rather aim at model ranking (Brazdil\\net al, 2022; Ruhkopf et al, 2023), which we discuss in Sec. 5.1.2. This is because\\nSpringer Nature 2021 LATEX template\\nLearning Curves for Decision Making\\n43\\nit is difficult to generalise exact performance across datasets. When general-\\nising over learners, one typically trains a single model for the target dataset,\\ntrained from learning curves on the same dataset belonging to other learning\\nalgorithms. Models that generalise over learners are typically called surrogate\\nmodels (Eggensperger et al, 2018).\\nWe organise this section by how explicit the generalisation is made over\\none of the two concepts. If no additional attributes are available, one implic-\\nitly assumes that all entries in the database are to a degree suitable to predict\\nvalues in a new situation. There is no explicit context in this case, and we are\\ngeneralising implicitly over datasets or learners. Since no explicit contextual-\\nisation exists, those approaches can be used for both purposes, regardless of\\ntheir original purpose. In contrast, additional contextualising attributes can\\ndescribe the dataset, i.e., we have meta-features of the datasets available, or\\nthey can describe the learning algorithms to which the learning curve values\\nbelong. In principle, one could utilise both types of additional attributes, but\\nwe are not aware of any approaches that adopt both.\\nGeneralization Without Explicit Context\\nGeneralisation from learning curves without explicit context means to pre-\\ndict the performance of a given learner on some dataset based on previously\\nacquired learning curves that are not equipped with additional information,\\ni.e., features describing the dataset or the algorithm used to produce them.\\nTo justify the prediction model, the existing empirical learning curves either\\nstem from the same algorithm on other datasets, or other algorithms on the\\nsame dataset. Either of these implicitly qualifies them to be relevant for the\\nnew task. In other words, one simply uses a set of unannotated existing learn-\\ning curves as aids to predict the behaviour of a new, only partially known,\\nlearning curve.\\nChandrashekaran and Lane (2017) developed an approach that explicitly\\nused regression to predict the target performance without context. Probably\\nwithout noticing, the approach mainly re-invents the approaches previously\\ndeveloped by Leite and Brazdil (2005, 2007, 2010); van Rijn et al (2015),\\nsince it computes the most similar other learning curves in the portfolio and\\nobtains a prediction based on the average over those curves. The three dif-\\nferences are that Chandrashekaran and Lane (2017) (i) consider uncertainty\\nin the prediction based on the variance in the neighbourhood, (ii) adopt an\\naffine transformation (instead of a linear transformation) of the existing learn-\\ning curves and apply this before selecting the most similar ones, and (iii) that\\nthey do not use a fixed schedule but, due to the focus on iteration-wise curves,\\nsimply a continuous schedule that is stopped as soon as there is enough evi-\\ndence that the target performance will not be better than a current threshold.\\nThe Euclidean norm between the vectors describing the performances at the\\nanchors is used as the distance function between two curves.\\nThe reason why the approach is discussed here and not in Sec. 5.1.2 together\\nwith the others is subtle and worth being discussed. In both lines of research,\\nSpringer Nature 2021 LATEX template\\n44\\nLearning Curves for Decision Making\\ntarget performance values from related curves are averaged to estimate the per-\\nformance of the current learner. However, Chandrashekaran and Lane (2017)\\nexplicitly treat this as a performance prediction, which is then used for early\\ndiscarding by comparing against a threshold. In contrast, the works in the line\\nof Leite and Brazdil (2005) do not treat this value as an actual prediction but\\nsimply as some score used to order a pair or a set of learners.\\nWhile the approach generalises across algorithm configurations, it ignores\\nthe configurations and generalises from learning curves without explicit con-\\ntext. In other words, there is no reason why the approach could not be used\\nalso for generalisation across datasets.\\nCardona-Escobar et al (2017) presented an approach that predicts values\\nat all anchors. The authors adopt a series of support vector regression models,\\none for each anchor not evaluated so far. It is not entirely clear with which\\ndata the models are trained, but we presume that it follows the same logic as\\nChandrashekaran and Lane (2017) and uses the fully known learning curves\\nof previously evaluated neural network configurations to do so. Interestingly,\\nsimilar to chaining (Gkioxari et al, 2016) in classification, they use as inputs\\nfor the j-th future anchor not only the known partial learning curve values but\\nalso use the predictions for the anchor points predicted before j.\\nGeneralization With an Explicit Algorithm Context\\nGeneralisation across algorithms only considers the target dataset and assumes\\nthat a number of (complete) empirical learning curves on that dataset are\\nalready available for different algorithms. The explicit generalisation requires\\nthat the previous learning curves are explicitly associated with features\\ndescribing the algorithm to which they belong.\\nBaker et al (2018) propose a method that uses features describing both the\\nlearning curve (including up to second-order differences) and the algorithm.\\nThe approach predicts the performance of neural networks based on features\\nthat describe the architecture (number of layers and weights) as well as the\\nhyperparameters of the learning algorithm (such as learning rate, learning\\nrate decay, etc.). They adopt linear and kernel-based support vector regression\\nmachines, random forests, and simple linear regression based on ordinary least\\nsquares. Even though the authors suggest using kernel-based support vector\\nregression machines, they find that simple linear regression also often compares\\nhighly competitive for this prediction task.\\nFollowing this idea, Long et al (2020) additionally add textual descriptions\\nof the architecture to predict the learning curves of neural networks. Indeed,\\nthe architecture description by Baker et al (2018) is rather simplistic and only\\nimmediately well suited if all layers are of the same type, e.g., dense layers,\\nand have the same number of neurons. Long et al (2020) report substantial\\nperformance improvements for convolutional network architectures compared\\nto the approach taken by Baker et al (2018).\\nSpringer Nature 2021 LATEX template\\nLearning Curves for Decision Making\\n45\\n5.1.7 Performance Bounding\\nPerformance bounding tries to give explicit lower or upper bounds on the per-\\nformance value at some specific or arbitrary budget. Answering this question\\nis essential to make high-confidence decisions on early discarding as discussed\\nin Sec. 4.1.3; formally, we denote it as C(a, ·). Typically, one is interested in\\nC(a, bref ), bref being the size of the dataset intended for training or the limit\\nperformance of an iteration-wise curve, but the models we discuss here are\\nmore general.\\nPerformance bounding is intuitively a simpler problem than performance\\nprediction (Sec. 5.1.6), but there are usually also higher expectations with\\nrespect to the accuracy of the assertion. When a performance bound is\\nexpressed, one would expect that the true value is at least as high as specified\\nwith high probability.\\nOne approach that addresses this problem is Data Allocation using Upper\\nBounds (DAUB)\\n(Sabharwal et al, 2016). Given a set of configurations, it\\nfirst runs all configurations on two anchors of the dataset, effectively building\\nthe initial segment of the learning curve. Based on this initial segment per\\nconfiguration, it determines an optimistic performance bound for each learning\\ncurve that likely will not exceeded (i.e., an upper bound for measures that\\nneed to be maximised, such as accuracy, and a lower bound for measures\\nthat need to be minimised, such as error rate). This performance bound is\\ndetermined by calculating the linear regression slope of the last two segments.\\nThe performance on the last anchor is extrapolated to the full size of the\\ndataset according to this slope. Therefore, there is an optimistic upper bound\\non the performance that a configuration can obtain. After that, it goes into\\nthe following loop: It runs the most promising configuration on a larger sample\\nsize and updates the performance bound. It reevaluates which configuration\\nhas the most potential at that budget and assigns more budget to the most\\npromising configuration until one configuration has been run on the entire\\ndataset. Therefore, this is an example of horizontal model selection.\\nAlternatively, learning curve cross-validation uses a similar approach but\\naddresses this in a more flexible, vertical setting (Mohr and van Rijn, 2021,\\n2023). The method explicitly assumes that sample-wise curves have a convex\\nbehaviour. The convexity of the curve allows for deriving a best-case extrap-\\nolation from a partial empirical learning curve. The convexity assumption is\\nused to linearly extrapolate the empirical learning curve and prune learners\\nwhen they can no longer improve on the best-known solution.\\n5.2 Approaches With Learning Curve Models\\nThis section covers learning curve approaches that utilise an explicit model\\nto model the entire learning curve, as described in Sec. 3. Fig. 14 shows an\\noverview of all the approaches we discuss and how they cite each other. In the\\nbase form, there is a model for a specific learner a that is able to predict the\\nperformance C(a, ·) or C(a, n, ·) at any sample size or iteration respectively\\nSpringer Nature 2021 LATEX template\\n46\\nLearning Curves for Decision Making\\nCortes et al (1993)\\nJohn and Langley (1996)\\nFrey and Fisher (1999)\\nGu et al (2001)\\nMukherjee et al (2003)\\nBoonyanunta and Zeephongsekul (2004)\\nSingh (2005)\\nLast (2007)\\nLast (2009)\\nHess and Wei (2010)\\nKolachina et al (2012)\\nFigueroa et al (2012)\\nSwersky et al (2014)\\nDomhan et al (2015)\\nKlein et al (2017a)\\nKlein et al (2017b)\\nCardona-Escobar et al (2017)\\nWistuba and Pedapati (2019)\\nRichter and Khoshgoftaar (2019)\\nKoshute et al (2021)\\nAdriaensen et al (2023)\\nEgele et al (2024)\\nKielh¨ofer et al (2024)\\n1993\\n1996\\n1999\\n2001\\n2003\\n2004\\n2005\\n2007\\n2009\\n2010\\n2012\\n2014\\n2015\\n2017\\n2019\\n2021\\n2023\\n2024\\nApproaches With Explicit Learning Curve Model\\nC(a, ·)\\nC(·, ·)\\nu(C(a, ·))\\nFig. 14: Citations across approaches with a learning curve model. The colours\\nindicate the question addressed about learning curves. Normal arrows indicate\\nthat the paper cited the other paper, and coloured arrows indicate an experi-\\nmental comparison with previous approaches.\\n(yellow), discussed in Sec. 5.2.1. On top of such curves, a utility curve U\\ncan be defined (green), discussed in Sec. 5.2.2. Finally, the curve models can\\neven generalise across the learners, leading to generalised curve models C(·, ·)\\n(beige). One additional benefit of these types of models is that they can also\\nperform model acquisition, i.e., assess the performance of a learner for which we\\nhave no performance evaluations yet. These models are discussed in Sec. 5.2.3.\\nIn this section, we describe the learning curve model according to the\\nnotion of budget, using the variable b to avoid having to distinguish between\\nsample sizes n or iterations t. In the spirit of Sec. 3, we use the notation\\nf (a, b) to refer to the original random variable that generates observations\\nof the performance of learner a at budget b (independently of whether f\\nhere is a sample-wise curve with b being the sample size or whether f is an\\niteration-wise curve with some implicit sample size n and b being the itera-\\ntion). Accordingly, µa,b = E[f (a, b)] is the mean value of the curve of learner\\na at budget b.\\n5.2.1 Performance Prediction at Any Point\\nWe will discuss approaches that build a full learning curve model ˆfa(b) for a\\nspecific learner a. That is, they model the curve mean µa,b for a fixed learner\\na at any budget b (effectively modelling µa,·). We divide the approaches\\ninto three groups, corresponding to the three model types for ˆf discussed in\\nSec. 3, which are also reflected in the three layers of Fig. 10. Accordingly,\\nwe first discuss approaches that provide point estimates of the curve, i.e.,\\nˆfa : N →R, then approaches that explicitly treat those estimates with uncer-\\ntainty and introduce a notion of bounds on them, i.e., ˆfa : N →R2, and\\nfinally approaches that create entire probabilistic belief models over curves,\\ni.e., ˆfa : N →{p | p is a distribution of a real-valued random variable}.\\nSpringer Nature 2021 LATEX template\\nLearning Curves for Decision Making\\n47\\nPoint Estimates\\nTo our knowledge, the first approach that used observed data to fit a learn-\\ning curve model was presented by Cortes et al (1993). In that paper, the\\nthree-parametric inverse power law shown in Eq. (6) was used to build an\\niteration-wise curve. The usage of the power law model is justified with the\\nfindings in the statistical mechanic framework (Seung et al, 1992) and used\\nto predict the predictive performance on the complete dataset (for two neural\\nnetwork architectures on the NIST dataset). The authors find that the predic-\\ntive performance on 60k instances can be almost perfectly predicted using the\\ninverse power law. Unfortunately, it is not entirely clear on which anchors the\\nestimates are built. Notably, this is the only paper we are aware of that fits\\nboth a train- and test-error curve. The paper reports some information about\\nnoise through boxplots on the curve but does not explicitly incorporate them\\ninto the model.\\nJohn and Langley (1996) proposed a similar approach. Similar to the work\\nof Cortes et al (1993), the model is used to estimate the performance of a\\nlearner on the complete dataset with the goal of stopping the training proce-\\ndure early. One difference is that the model is adopted for a sample-wise curve\\ninstead of an iteration-wise curve. Concerning convergence detection, John and\\nLangley (1996) employ the Probably Close Enough criterion, which detects\\nconvergence if the probability that the accuracy of a model trained on the\\ncomplete dataset will be at most some ε worse than the current model’s per-\\nformance is less than some δ. However, the paper itself then does not adopt\\na notion of probabilities but stops if the performance on the complete dataset\\npredicted by an inverse power law model is not by at least ε better than\\nthe currently observed performance; they call the approach Extrapolation of\\nLearning Curves. In other words, the uncertainty is not quantified.\\nThe inverse power law has been used in many applications. For example,\\nseveral approaches have used the inverse power law to model the performance\\nof neural networks in different domains (Alwosheel et al, 2018; Cho et al, 2015).\\nIt is noteworthy that recent works show evidence against the usage of any\\ncommonly used models for neural networks, at least in the initial parts of the\\ncurve, due to the (sample-wise) double descent (Nakkiran et al, 2020). At least\\nfor certain combinations of architectures, datasets, and training procedures,\\nthere is empirical evidence that the learning curve exhibits non-monotonic\\nbehaviour, which contradicts all existing learning curve models like the inverse\\npower law model.\\nRange Estimates\\nMukherjee et al (2003) built inverse power law models in the domain of DNA\\ndata. The main contribution of that paper is to analyse the appropriateness of\\nthe inverse power model on eight medical datasets. To this end, they construct\\nuncertainty bounds around the mean learning curve consisting of the q25 and\\nq75 curves fitted from those statistics, respectively. This way, a learning curve\\nmodel including information about dispersion is obtained. Experiments are\\nSpringer Nature 2021 LATEX template\\n48\\nLearning Curves for Decision Making\\nconducted for a support vector machine on eight medical datasets in which the\\nleave-one-out validation result (estimate of the learning curve on |d| −1 data\\npoints for training) is compared to the boundaries suggested by the model.\\nHaving an explicit model for the q25 and q75 curves, one can obtain for an\\narbitrary anchor b not only a point estimate of µa,b but an estimate of the\\ninter-quartile range of f (a, b) itself, which is arguably more informative given\\nthat, assuming Gaussian noise, the interval should also contain µa,b.\\nFigueroa et al (2012) modify the aforementioned approach in two ways.\\nFirst, different anchors are associated with different weights, usually to assign\\nhigher weights to larger anchors since they are more informative. Second,\\nimplicitly assuming a standard Gaussian distribution of the observations as in\\nEq. (5), they compute a 95% confidence interval to describe the uncertainty\\nrather than the interquartile range. Based on this information, for a query\\npoint b, they predict a confidence interval instead of a point estimate. Figueroa\\net al (2012) also applied this approach to medical data, just as Mao et al (2016)\\nfor EEG data. More recently, it was also successfully used for sensor commu-\\nnication (Oyedare and Park, 2019). This work aims to predict a reasonable\\nsample size, which is perhaps more reasonably addressed by the utility-based\\napproaches discussed in Sec. 5.1.5.\\nRecently, Koshute et al (2021) have used the inverse power law to predict\\nthe minimum anchor point on which a learner must be trained to reach near-\\npsat performance with a given desired confidence. This approach can be seen as\\na combination of the above two approaches. Similar to Figueroa et al (2012),\\nthey compute the confidence interval at all anchors. However, instead of using\\nthese to estimate confidence intervals at arbitrary points, they fit a single curve\\non the lower bounds of the confidence intervals at the known anchors. The\\nresulting model is not used to make predictions on arbitrary anchors but to\\ncompute the cheapest anchor that will obtain with a pre-defined probability\\n(size of confidence-interval) a performance that is ε-close to psat, where ε is a\\nhyperparameter controlled by the user.\\nThe idea of computing confidence intervals is also adopted by learning curve\\ncross-validation (Mohr and van Rijn, 2021, 2023). A Morgan-Mercer-Flodin\\nmodel is created to decide whether or not to skip intermediate anchors and\\nevaluate the learner on the full dataset size. However, the confidence intervals\\nare used differently than in the above cases and are not used for the inverse\\npower law model itself. In contrast, the confidence bounds are used to compute\\nthe range of possible slopes of the learning curve between two anchors.\\nDistribution Estimates\\nThe first approach to predict distribution estimates for any anchor point was\\npresented by Domhan et al (2015). The approach assumes learning curves to\\nbe instances of a parametric model that is a linear combination of known\\nmodel classes, such as the inverse power law, and others (Gu et al, 2001).\\nThe main difference to the above approaches is that, instead of estimating the\\nparameters through a maximum likelihood approach, they estimate, for each\\nSpringer Nature 2021 LATEX template\\nLearning Curves for Decision Making\\n49\\nparameter, the whole posterior distribution adopting Monte-Carlo Markov\\nChains (MCMC). The approach is successfully used to early discard neural\\nnetwork architectures by predicting the saturation performance psat of an\\niteration-wise curve, thereby discarding learners as soon as the probability that\\nit is competitive drops below a pre-defined threshold. The Bayesian model pro-\\nposed by Domhan et al (2015) also explicitly estimates the noise σ2\\na,b, which\\nis assumed to be homoscedastic, i.e., identical for all budgets b. In this sense,\\nthe approach quantifies both epistemic and aleatoric uncertainty (cf. Sec. 3).\\nA recent alternative for estimating distributions has been proposed by\\nAdriaensen et al (2023) through the notion of prior fitted neural networks\\n(PFN) (Hollmann et al, 2023). Based on the approach by Domhan et al (2015),\\nAdriaensen et al (2023) consider a set of basis functions, over which they\\ndefine a prior. They then sample a large number of curves from this prior\\nand train a variation of a transformer neural network with it, which is able\\nto predict distributions for a target anchor based on a partial learning curve.\\nSince no sampling from the posterior is required anymore, compared to the\\nMCMC approach by Domhan et al (2015), predictions can be obtained much\\nfaster, and the authors claim that the prediction performance is comparable or\\neven better, which could however not be confirmed in subsequent experiments\\ndiscussed below.\\nThe latest approach we are aware of is the robust estimation RoBER\\npresented by Egele et al (2024). The approach follows the idea of MCMC\\nintroduced by Domhan et al (2015) but applies a different sampling algorithm\\nto obtain more stable estimates. The results suggest that classical learn-\\ning extrapolation significantly outperforms PFN-based extrapolations at the\\ncurrent state of research.\\n5.2.2 Utility Prediction at Any Point\\nUtility prediction combines learning curve models as discussed in Sec. 5.2.1\\nwith utility models as discussed in Sec. 5.1.5. The learning curve model ˆf is\\nused as a basis to estimate the performance at any point, and the utility at\\nbudget b is then computed as a function of the modelled performance ˆfa(b) at\\nand the associated costs for budget b.\\nThe first approach in this direction was presented by Last (2007). This\\nwork is very similar to the works of Weiss and Tian (2006) but makes util-\\nity forecasts rather than looking back. Therefore, it is projective instead of\\nretrospective. The error rate that serves as input to the model is obtained\\nfrom a parametric model (i.e., a power-law) trained on the empirical values\\nobtained at earlier budgets. This framework enables one to analytically com-\\npute the optimal dataset size. The main advantage of the approach over the\\none of Weiss and Tian (2006) is that one does not need to go through sev-\\neral acquisition iterations, which is a benefit if those are associated with fixed\\ncosts. Therefore, the learning curve has become a resource for decision mak-\\ning. While this work assumes the empirical learning curve to be available, Last\\nSpringer Nature 2021 LATEX template\\n50\\nLearning Curves for Decision Making\\n(2009) embeds his idea into an algorithm that follows a progressive sampling\\nscheme in a follow-up work.\\nOne use case in which the above techniques have been adopted has been\\nreported in the context of automated software configuration (Sarkar et al,\\n2015). The context of that paper is that every instance is a parametrisation\\nof a software library, and obtaining its label requires the costly execution of\\na benchmark on such a configuration. The goal is to understand how many\\nobservations must be acquired to learn a reliable prediction model. To this\\nend, the authors adopt the projective sampling approach of Last (2007, 2009).\\nThe latter work raises an important issue by stating that knowing the\\nsaturation point is (often) not enough. Instead, we often also need to know the\\nperformance at the saturation point. Sarkar et al (2015) argue that if the user\\nis unaware of the expected performance at that point, substantial resources\\nmight be required to obtain the observations to reach the saturation point.\\nHowever, if the actual performance at that point is known to be mediocre, the\\nuser could anticipate this and not invest the required resources. To this end,\\nthey also incorporate the utility model proposed by Weiss and Tian (2008).\\n5.2.3 Performance at Any Point for Any Learner\\nThe approaches discussed in this section are the most general ones developed\\nto date regarding learning curves in that they create a model for the complete\\nfunction C, i.e., generalising both over both budgets and learners. Such a model\\nis so versatile that it can be used in all types of decision-making situations,\\ne.g., data acquisition, early stopping, and early discarding. Additionally, these\\ncan be used for model acquisition (selecting a yet unseen promising model).\\nFreeze-thaw Bayesian optimisation models the behaviour of learning curves\\nthrough Gaussian processes (Swersky et al, 2014). An important contribution\\nof that work is a non-stationary kernel for Gaussian processes that supports\\nexponentially decaying learning curve models; it can easily be checked that\\nstandard kernels like a linear or Gaussian kernel do not lead to meaning-\\nful learning curve models. Assuming that the kernel reflects the model class\\nappropriately, one additional benefit of using a Gaussian process is that one\\nautomatically obtains estimates for the noise σ2\\na,b at an arbitrary anchor b.\\nUsing their kernel and the current set of observations, Swersky et al (2014)\\nestimate the asymptotic mean performance psat. Since the learning curves are\\ncombined with Bayesian optimisation, the uncertainty for a specific future\\nanchor is one of the required inputs for computing their acquisition func-\\ntion. In a rather thin evaluation, the approach was successfully applied to\\nOnline Latent Dirichlet Allocation, Logistic Regression, and Probabilistic\\nMatrix Factorization, considering one dataset per learner. While the paper\\nfocused on iteration-wise curves, the modelling technique can also be used for\\nsample-wise curves.\\nKlein et al (2017a) presented a similar approach dubbed FABOLAS. Simi-\\nlar to Freeze-thaw Bayesian optimisaion, a Gaussian process is used to model\\nSpringer Nature 2021 LATEX template\\nLearning Curves for Decision Making\\n51\\nthe learning curve of the learners across different hyperparameter configura-\\ntions. There are three main differences between the two approaches as far as\\nlearning curves are concerned. First, FABOLAS considers sample-wise curves,\\nwhile freeze-thaw Bayesian optimisation considers iteration-wise curves. Sec-\\nond, and related to this, FABOLAS uses a kernel different from freeze-thaw\\nBayesian optimisation to model the behaviour of the learning curve using the\\nGaussian process, which is defined by the relative dataset size in [0, 1] instead of\\nabsolute sizes. Third, FABOLAS tries to explicitly learn the complete learning\\ncurve, while freeze-thaw Bayesian optimisation focuses only on the saturation\\nperformance. Similar to freeze-thaw Bayesian optimisation, the uncertainty\\nabout the performance estimates of the learning curve is not explicitly used.\\nStill, the fact that a Gaussian process is fitted from the data allows one to\\nmake assertions about the certainty of the learning curve value at any point.\\nParallel to their work on FABOLAS, Klein et al (2017b) proposed an\\napproach to estimate both the mean and the noise (aleatoric uncertainty) of\\na learning curve through the notion of Bayesian neural networks. The neu-\\nral network predicts the parameters of a set of basis functions. These basis\\nfunctions incorporate prior knowledge into the network, which is necessary to\\nextrapolate away from the data. The main difference between this approach\\nand the aforementioned approaches is that this approach models the behaviour\\nof the learning curve through a neural network. This network has d + 1 input\\nunits (d for the algorithm description and one for the anchor), one output\\nunit for the estimated performance and, optionally, one output unit for the\\nestimate of the variance of the performance (which relates to aleatoric uncer-\\ntainty). To our knowledge, this approach and the learning curve extrapolation\\nproposed by Domhan et al (2015) are the only approaches in this area that\\nexplicitly model the variance of the performance of the learning curve (i.e.,\\nwhich can also be seen as noise). An important difference between the two is\\nthat Klein et al (2017b) assume heteroscedastic noise, i.e., noise that changes\\nwith both different hyperparameters and anchors, while Domhan et al (2015)\\nassume homoscedastic noise across anchor sizes (not across configurations,\\nbecause the model does not generalise over different configurations). While the\\napproach presented in the paper does not explicitly consider the uncertainty\\nabout the parameter estimates, the parameters are essentially sampled from\\na posterior distribution. Therefore, the uncertainty is at least implicitly avail-\\nable. However, it should be noted that the number of parameters describing\\nthe model here, namely the network weights, is potentially much larger than\\nin the approach taken by Domhan et al (2015).\\nWistuba and Pedapati (2019) propose to use biased matrix factorisation\\nto model C(·, ·). The approach is settled in the context of neural architecture\\nsearch. Knowledge from previous datasets and different architectures is used\\nto estimate the performance of new architectures on the target dataset, and\\nthis estimate is used to drive a Bayesian optimisation approach.\\nSpringer Nature 2021 LATEX template\\n52\\nLearning Curves for Decision Making\\nTable 1: Overview of the discussed learning curve approaches, ordered along\\nthe most general question they address, the learning curve type and the data\\nresources used (4 columns). In the header, ‘LC’ stands for learning curve, ‘DS’\\nstands for dataset, and ‘AL’ stands for algorithm. All of them use partial\\nempirical curves on the target dataset of the current learner(s). Estimate type:\\np,r,d are point, range and distribution estimates, respectively.\\nQuestion Type\\nLC other DS\\nDS Meta-Feat.\\nLC other AL\\nAL Meta-Feat.\\nUtility\\nEstimate Type\\nContributions\\npref > τ\\nobs.\\n✗✗✗✗✗\\np Van den Bosch (2004); Jamieson and Talwalkar (2016);\\nPetrak (2000); Zeng and Luo (2017)\\npref > τ\\nboth ✗✗✗✗✗\\np Li et al (2017)\\nπa∈A\\nobs.\\n✓✗✓✗✗\\np Leite and Brazdil (2005, 2007); van Rijn et al (2015)\\nπa∈A\\nobs.\\n✓✓✗✗✗\\np Leite and Brazdil (2008, 2010)\\nπa∈A\\nboth ✓✓✗✗✗\\np Ruhkopf et al (2023)\\nbsat\\niter.\\n✗✗✗✗✗\\np Bishop (1995)\\nbsat\\nobs.\\n✗✗✗✗✗\\np John and Langley (1996); Ng and Dash (2006); Provost\\net al (1999)\\nbsat\\nobs.\\n✓✗✗✗✗\\np Leite and Brazdil (2003, 2004)\\nplim\\nobs.\\n✗✗✗✗✗\\np Cortes et al (1994)\\nbu\\nsat\\nobs.\\n✗✗✗✗✗\\np Meek et al (2002)\\nbu\\nsat\\nobs.\\n✗✗✗✗✓p Weiss and Tian (2006, 2008)\\nC(a, dtr ) obs.\\n✓✗✗✗✗\\nr\\nChandrashekaran and Lane (2017)\\nC(a, dtr ) iter.\\n✗✗✓✓✗\\np Baker et al (2018)\\nC(a, ·)\\nobs.\\n✗✗✗✗✗\\np Sabharwal et al (2016)\\nC(a, ·)\\nobs.\\n✗✗✗✗✗\\nr\\nMohr and van Rijn (2023)\\nC(a, ·)\\niter.\\n✗✗✗✗✗\\np Cortes et al (1993)\\nC(a, ·)\\nobs.\\n✗✗✗✗✗\\np Boonyanunta and Zeephongsekul (2004); Frey and Fisher\\n(1999); Gu et al (2001); Hess and Wei (2010); John\\nand Langley (1996); Kolachina et al (2012); Richter and\\nKhoshgoftaar (2019); Singh (2005)\\nC(a, ·)\\nobs.\\n✗✗✗✗✗\\nr\\nFigueroa et al (2012); Koshute et al (2021); Mukherjee\\net al (2003)\\nC(a, ·)\\niter.\\n✗✗✗✗✗\\nd Domhan et al (2015)\\nC(a, ·)\\nobs.\\n✓✗✗✗✗\\np Cardona-Escobar et al (2017)\\nC(a, ·)\\nboth ✓✗✗✗✗\\nd Adriaensen et al (2023)\\nC(a, ·)\\nboth ✗✗✗✗✗\\nd Egele et al (2024)\\nC(a, ·)\\nobs.\\n✓✓✗✗✗\\np Kielh¨ofer et al (2024)\\nU(a, ·)\\nobs.\\n✗✗✗✗✓p Last (2007, 2009)\\nC(·, ·)\\nboth ✗✗✓✓✗\\nd Klein et al (2017b); Swersky et al (2014)\\nC(·, ·)\\nobs.\\n✗✗✓✓✗\\nd Klein et al (2017a)\\nC(·, ·)\\nboth ✓✗✓✓✗\\nd Wistuba and Pedapati (2019)\\n6 Summary and Open Research Directions\\nLearning curves have been a vital resource for decision making in machine\\nlearning for several decades, and they have gained significant attention over the\\nlast years. Learning curves have proven to be a suitable solution for different\\nSpringer Nature 2021 LATEX template\\nLearning Curves for Decision Making\\n53\\ntypes of decision-making situations, i.e., data acquisition, early stopping, and\\nearly discarding for model selection.\\nWe have provided a formal definition of various types of learning curves\\n(Sec. 2). There are two predominant types of learning curves in the machine\\nlearning literature, i.e.: the sample-wise curve (i.e., the type of learning curve\\nthat one obtains when giving a learner more training instances) and the\\niteration-wise curve (i.e., the type of learning curve that one obtains when\\nallowing an algorithm to process the data multiple times, see for example the\\nnumber of epochs of a neural network). While both types of learning curves\\nseem similar, they have distinct semantic meanings and characteristics. Both\\ntypes of learning curves can be extended to a utility curve, which considers the\\ncost of computational resources or data acquisition. Additionally, we have con-\\ntrasted these against other types of (learning) curves, such as feature curves,\\ncapacity curves, and curves obtained by data-centric models, such as active\\nlearning or curriculum learning.\\nWe have described the basic concepts of modelling a learning curve (Sec. 3).\\nThere are various parametric models that incorporate domain knowledge about\\nwhat we already know about the shape of learning curves (e.g., the three-\\nparameter inverse power law-model). Even when the performance of a learner\\nis observed at very few anchors, the learning curve can already be extrapo-\\nlated to make predictions about larger anchors. Additionally, one can decide\\nto also model a degree of uncertainty, either as a range estimate or as a\\ndistribution. We distinguish between two types of uncertainty, i.e., epistemic\\nuncertainty and aleatoric uncertainty, and relate these concepts to the lit-\\nerature on modelling learning curves. Typically, when uncertainty is being\\nmodelled, the epistemic uncertainty is being modelled, but in some cases, the\\naleatoric uncertainty is being modelled (see, e.g., Klein et al, 2017b).\\nWe have provided a unified framework for methods that utilise learn-\\ning curves for decision making in machine learning (Sec. 4). This framework\\ncategorises these methods along three axes: the decision situation that they\\naddress, the questions that can be addressed with learning curves, and the\\ndata resources that can be used to model the learning curves. Notably, Fig. 10\\nshows an overview of all questions that can be addressed by learning curves.\\nThere are various ways to address decision situations with learning curves; for\\nexample, questions about the saturation point of a given learner or whether\\na learner will perform better than another learner at a given amount of data.\\nThese questions can be further generalised, eventually ranging in complexity\\nfrom binary questions to questions that address how any learner behaves at\\nany budget.\\nWe have done an extensive literature survey, categorising all learning curve\\nmethods that we are aware of into this framework (Sec. 5). Table 1 shows an\\noverview of the methods we have discussed in this survey, contextualising them\\naccording to these criteria. This table can be seen as an extension of Fig. 12.\\nBased on this literature survey, we describe several directions for future work.\\nSpringer Nature 2021 LATEX template\\n54\\nLearning Curves for Decision Making\\nMore experimental databases for learning curve research to sup-\\nport the full complexity of learning curve methods. Doing relevant\\nresearch on learning curves requires extensive computing power. Exploring a\\nsample-wise curve inherently requires many models to restart the learning pro-\\ncess at different anchors, whereas exploring an iteration-wise curve is often\\ndone on neural networks that come with their distinct layers of complexity (see,\\ne.g., White et al, 2023). A common way to address this is by experimental\\ndatabases or surrogate benchmarks that store certain experimental results.\\nThis allows for fast experimentation and, therefore, faster development cycles.\\nWhile several of these experimental databases for learning curves exist (as\\noutlined in Sec. 2.2), these currently do not capture the full scale of learning\\ncurves resources or questions that can be answered using learning curves.\\nA quantitative benchmark on learning curve extrapolation meth-\\nods. Many models have been proposed to extrapolate learning curves and make\\npredictions about the performance of a learning at a higher budget (see, e.g.,\\nGu et al, 2001). However, these models have only been subject to limited com-\\nparison. While Gu et al (2001) compared various parametric models against\\neach other, and Kielh¨ofer et al (2024) compared a representative parametric\\nmodel against a representative metalearning model across many different set-\\ntings, more research is needed. Fig. 13 and Fig. 14 already show that, while\\nmany papers are aware of other methods and cite those methods (grey arrows),\\nonly very few actively compare against each other (blue arrows). Moreover,\\nmany of these learning curve models are used as a small component in a larger\\nsystem, e.g., an AutoML system. In such a case, the predictive performance\\nof the learning curve model might not even be measured, as eventually, one\\noften measures the quality of the complete system; in the case of an AutoML\\nsystem, the performance of the final selected model. Due to this modular\\nnature, improvements on the learning curve extrapolation method would then\\nbe orthogonal to improvements on the AutoML system.\\nTighter integration of learning curve extrapolation methods with\\nAutoML systems. We already noted a clear opportunity for AutoML sys-\\ntems in Sec. 4.1.3. In situations where multiple learners are being compared\\nagainst each other, the training set needs to be further split into an actual\\ntraining set and a validation set to select the best learner to be tested on the\\ntest set (which can only be seen once). The existence of this validation set\\nalready shows an opportunity for learning curve methods; while a learner is\\nbeing selected based on its performance after being trained on the split-off\\ntraining set, what is relevant is its performance after being trained on the orig-\\ninal training set (i.e., the split-off training set plus validation set). It is not\\nnecessarily the case that the same learner performs best on both. Learning\\ncurve extrapolation models can predict which learner will eventually perform\\nbest on an anchor of the size of the original training set.\\nMore learning curve methods that operate on low-level questions.\\nA prominent question that arises from the literature survey is whether sim-\\nple questions can be treated more simplistically. Fig. 10 shows four binary\\nSpringer Nature 2021 LATEX template\\nLearning Curves for Decision Making\\n55\\nquestions. While approaches aim to answer these questions, most do so by\\nimplicitly answering a more difficult question (see Table 1 and Fig. 12). Some\\nof these questions are really at the core of the discussed approaches. For exam-\\nple, will the learning curve intersect with the learning curve of the currently\\nbest model? Most approaches create a learning curve model for this, implic-\\nitly solving a much more difficult problem. While those models, if appropriate,\\nhave the potential to provide additional interesting insights, the question arises\\nwhether simpler approaches could reliably solve those problems while needing\\nmuch less online data. Approaches that remain faithful to this question level\\n(such as successive halving and hyperband) have proven effective and received\\nconsiderable attention. Additionally, we see a clear opportunity for incorpo-\\nrating uncertainty into methods that address the binary question, effectively\\nproviding the chance that a particular learner will be better than another\\nlearning at a given budget.\\nA learning curve method that makes use of all types of data\\nresources. Learning curve methods can make use of various resources to model\\nthe learning curve (see Fig 11). For example, the inverse power law model\\nuses the current learner’s learning curve on the same dataset. In contrast,\\nmetalearning models often also make use of learning curves of either the current\\nlearner or other learners on other datasets (see, e.g., Leite and Brazdil, 2005).\\nA reasonable assumption is that the methods that utilise more types of data\\nresources would be more accurate. Table 1 reveals that no learning curve model\\nutilises all types of data resources. Indeed, combining anchors across learners\\nand datasets might be a complex task, but when this is done successfully, it\\nwill enormously increase our understanding of learning curves.\\nAcknowledgements.\\nWe thank Pavel Brazdil, Isabelle Guyon, Aaron Klein, and Marco Loog for\\ntheir insightful discussions and feedback on this survey.\\nSpringer Nature 2021 LATEX template\\n56\\nLearning Curves for Decision Making\\nA Notation\\nTable 2 contains an overview of the notation used throughout this paper.\\nTable 2: Overview of notation.\\nterm\\ndescription\\nD\\nThe space of all possible datasets\\nd\\nAn instantiation of a dataset\\ndtr\\nThe instances from a given dataset based upon which a given hypothesis h\\nis trained\\nX\\nThe space of all possible input values for a given dataset d\\nY\\nThe space of all possible labels of a given dataset d\\nH\\nThe space that a given model or hypothesis h can take\\nh\\nModel or hypothesis h, induced based on a given train set dtr\\na\\nAn algorithm that given a training set dtr induces a hypothesis h\\nA\\nSet of all possible learners under consideration\\nRout\\nThe theoretical performance of a hypothesis under the true distribution of\\nthe data (note that this true distribution is typically unknown)\\nRin\\nThe empirical risk of a hypothesis under some sample d (i.e., a dataset) from\\nthe true distribution\\nC (a, n)\\ntrue mean performance of learner a when trained on n samples (related to\\nobservation learning curve)\\nC (a,n,t)\\ntrue mean performance of learner a when trained on n samples with t\\niterations (related to iteration learning curve)\\nf (a, b)\\nThe performance of a given learner a trained on a dataset of sample size b\\n(in case of observation curve) or iteration b (in case of iteration curve). In\\ncontrast to C. f (a, b) is a random variable with C as mean value\\nµa,b\\nThe mean of f (a, b)\\nσ2\\na,b\\nThe variance of f (a, b)\\nO\\nSet of performance observations from anchors of historic learning curves, pos-\\nsibly for different learners, e.g., O = {(a1, b1), (a2, b2), (a3, b3), . . .}, where\\nai are learners and bj are budgets.\\nˆf (a, b)\\nThe performance estimated by a learning curve model for learner a at budget\\nb. Maybe a point, range, or distributional estimate.\\nˆfa(b)\\nThe performance estimated by a learning curve model for learner a at budget\\nb if the model does not generalize across learners. Maybe a point, range, or\\ndistributional estimate.\\nn\\nanchor size, indicating the size of a subsample of the dataset\\nt\\nnumber of iterations, e.g., in the case of neural networks, the number of\\nepochs\\nb\\ngeneric symbol for an anchor, stands either for n or t, depending on the\\ncontext.\\nbsat\\nThe anchor size at which the performance of a learner saturates\\npsat\\nThe performance of the learner at the saturation point\\nbu\\nsat\\nThe anchor size at which the utility (a performance measure divided by a\\ncost measure) is maximized\\nbref\\nUsed in the framework for the binary question as a threshold on the budget\\npref\\nUsed in the framework for the binary question as a threshold on the\\nperformance\\nθ\\nThe parameters of a parametric function, for example, the parameters of the\\nIPL-model are (α, β, γ)\\nSpringer Nature 2021 LATEX template\\nLearning Curves for Decision Making\\n57\\nDeclarations\\nFunding. Felix Mohr was supported through the project ING-312-2023 from\\nUniversidad de La Sabana.\\nConflicts of interest. Not Applicable\\nEthics approval. Not Applicable\\nConsent to participate. Not Applicable\\nConsent for publication. Not Applicable\\nAvailability of data and material. Not Applicable\\nCode availability. Not Applicable\\nAuthors’ contributions. Both authors were involved in the redaction of all\\nparts of the document. F.M. had the lead in all sections; J.N.v.R. contributed\\nto all sections by actively writing and critically reviewing the text.\\nReferences\\nAdriaensen S, Rakotoarison H, M¨uller S, et al (2023) Efficient bayesian learning\\ncurve extrapolation using prior-data fitted networks. In: Advances in Neural\\nInformation Processing Systems 36, pp 19,858–19,886\\nAlwosheel A, van Cranenburgh S, Chorus CG (2018) Is your dataset big\\nenough? sample size requirements when using artificial neural networks for\\ndiscrete choice analysis. Journal of choice modelling 28:167–182\\nAmari S, Murata N (1993) Statistical theory of learning curves under entropic\\nloss criterion. Neural Computation 5(1):140–153\\nBaker B, Gupta O, Raskar R, et al (2018) Accelerating neural architecture\\nsearch using performance prediction. In: 6th International Conference on\\nLearning Representations, ICLR’18\\nBeleites C, Neugebauer U, Bocklitz T, et al (2013) Sample size planning for\\nclassification models. Analytica chimica acta 760:25–33\\nBifet A, Gavald`a R, Holmes G, et al (2018) Machine learning for data streams:\\nwith practical examples in MOA. MIT press\\nBishop C (1995) Regularization and complexity control in feed-forward\\nnetworks. In: Proceedings International Conference on Artificial Neural\\nNetworks ICANN’95, pp 141–148\\nBoonyanunta N, Zeephongsekul P (2004) Predicting the relationship between\\nthe size of training sample and the predictive power of classifiers. In:\\nKnowledge-Based Intelligent Information and Engineering Systems, 8th\\nInternational Conference, KES 2004. pp 529–535\\nSpringer Nature 2021 LATEX template\\n58\\nLearning Curves for Decision Making\\nBornschein J, Visin F, Osindero S (2020) Small data, big decisions: Model\\nselection in the small-data regime. In: Proceedings of the 37th International\\nConference on Machine Learning. pp 1035–1044\\nVan den Bosch A (2004) Wrapped progressive sampling search for optimizing\\nlearning algorithm parameters. In: Proceedings of the 16th Belgian-Dutch\\nConference on Artificial Intelligence, pp 219–226\\nBrazdil P, van Rijn JN, Soares C, et al (2022) Metalearning: Applications to\\nAutomated Machine Learning and Data Mining, 2nd edn. Springer\\nCardona-Escobar AF, Giraldo-Forero AF, Castro-Ospina AE, et al (2017)\\nEfficient hyperparameter optimization in convolutional neural networks\\nby learning curves prediction. In: Progress in Pattern Recognition, Image\\nAnalysis, Computer Vision, and Applications. pp 143–151\\nChandrashekaran A, Lane IR (2017) Speeding up hyper-parameter optimiza-\\ntion by extrapolation of learning curves using previous builds. In: Machine\\nLearning and Knowledge Discovery in Databases - European Conference,\\nECML PKDD 2017. pp 477–492\\nCho J, Lee K, Shin E, et al (2015) How much data is needed to train a medi-\\ncal image deep learning system to achieve necessary high accuracy? CoRR\\nabs/1511.06348. https://arxiv.org/abs/1511.06348\\nCortes C, Jackel LD, Solla SA, et al (1993) Learning curves: Asymptotic values\\nand rate of convergence. In: Advances in Neural Information Processing\\nSystems 6. pp 327–334\\nCortes C, Jackel LD, Chiang W (1994) Limits in learning machine accuracy\\nimposed by data quality. In: Advances in Neural Information Processing\\nSystems 7. pp 239–246\\nda Costa FG, Rios RA, de Mello RF (2016) Using dynamical systems tools\\nto detect concept drift in data streams. Expert Systems with Applications\\n60:39–50\\nDomhan T, Springenberg JT, Hutter F (2015) Speeding up automatic hyper-\\nparameter optimization of deep neural networks by extrapolation of learning\\ncurves. In: Proceedings of the Twenty-Fourth International Joint Conference\\non Artificial Intelligence, IJCAI 2015. pp 3460–3468\\nDomingos P, Hulten G (2000) Mining High-Speed Data Streams. In: Proceed-\\nings of the sixth ACM SIGKDD international conference on Knowledge\\ndiscovery and data mining, pp 71–80\\nSpringer Nature 2021 LATEX template\\nLearning Curves for Decision Making\\n59\\nDong X, Yang Y (2020) Nas-bench-201: Extending the scope of reproducible\\nneural architecture search. In: 8th International Conference on Learning\\nRepresentations, ICLR 2020\\nEgele R, Mohr F, Viering T, et al (2024) The unreasonable effectiveness of early\\ndiscarding after one epoch in neural network hyperparameter optimization.\\nNeurocomputing 597:127,964\\nEggensperger K, Lindauer M, Hoos HH, et al (2018) Efficient benchmarking\\nof algorithm configurators via model-based surrogates. Machine Learning\\n107(1):15–41\\nEggensperger K, M¨uller P, Mallik N, et al (2021) HPOBench: A collection\\nof reproducible multi-fidelity benchmark problems for HPO. In: Proceed-\\nings of the Neural Information Processing Systems Track on Datasets and\\nBenchmarks\\nFigueroa RL, Zeng-Treitler Q, Kandula S, et al (2012) Predicting sample size\\nrequired for classification performance. BMC Medical Informatics Decis Mak\\n12:8\\nFine T, Mukherjee S (1999) Parameter convergence and learning curves for\\nneural networks. Neural Comput 11(3):747–769\\nForman G, Cohen I (2004) Learning from little: Comparison of classifiers given\\nlittle training. In: Knowledge Discovery in Databases: PKDD 2004, 8th\\nEuropean Conference on Principles and Practice of Knowledge Discovery in\\nDatabases. pp 161–172\\nFrey LJ, Fisher DH (1999) Modeling decision tree performance with the power\\nlaw. In: Proceedings of the Seventh International Workshop on Artificial\\nIntelligence and Statistics, AISTATS 1999\\nF¨urnkranz J, Petrak J (2001) An evaluation of landmarking variants. In: Work-\\ning Notes of the ECML/PKDD 2000 Workshop on Integrating Aspects of\\nData Mining, Decision Support and Meta-Learning, pp 57–68\\nGkioxari G, Toshev A, Jaitly N (2016) Chained predictions using convolutional\\nneural networks. In: Leibe B, Matas J, Sebe N, et al (eds) Computer Vision\\n- ECCV 2016 - 14th European Conference. pp 728–743\\nGoodfellow I, Bengio Y, Courville A (2016) Deep Learning. MIT Press, http:\\n//www.deeplearningbook.org\\nGu B, Hu F, Liu H (2001) Modelling classification performance for large\\ndata sets. In: Advances in Web-Age Information Management, Second\\nInternational Conference, WAIM 2001. pp 317–328\\nSpringer Nature 2021 LATEX template\\n60\\nLearning Curves for Decision Making\\nHess KR, Wei C (2010) Learning curves in classification with microarray data.\\nSeminars in oncology 37(1):65–68\\nHollmann N, M¨uller S, Eggensperger K, et al (2023) Tabpfn: A transformer\\nthat solves small tabular classification problems in a second. In: The\\nEleventh International Conference on Learning Representations, ICLR 2023\\nHughes GF (1968) On the mean accuracy of statistical pattern recognizers.\\nIEEE Trans Inf Theory 14(1):55–63\\nH¨ullermeier E, Waegeman W (2021) Aleatoric and epistemic uncertainty\\nin machine learning: an introduction to concepts and methods. Machine\\nLearning 110(3):457–506\\nJamieson KG, Talwalkar A (2016) Non-stochastic best arm identification\\nand hyperparameter optimization. In: Proceedings of the 19th Interna-\\ntional Conference on Artificial Intelligence and Statistics, AISTATS 2016.\\npp 240–248\\nJohn GH, Langley P (1996) Static versus dynamic sampling for data min-\\ning. In: Proceedings of the Second International Conference on Knowledge\\nDiscovery and Data Mining (KDD-96). pp 367–370\\nKielh¨ofer L, Mohr F, van Rijn JN (2024) Learning curve extrapolation meth-\\nods across extrapolation settings. In: Advances in Intelligent Data Analysis\\nXXII. pp 145–157\\nKlein A, Falkner S, Bartels S, et al (2017a) Fast bayesian optimization of\\nmachine learning hyperparameters on large datasets. In: Proceedings of\\nthe 20th International Conference on Artificial Intelligence and Statistics,\\nAISTATS 2017. pp 528–536\\nKlein A, Falkner S, Springenberg JT, et al (2017b) Learning curve prediction\\nwith bayesian neural networks. In: 5th International Conference on Learning\\nRepresentations, ICLR’17\\nKolachina P, Cancedda N, Dymetman M, et al (2012) Prediction of learning\\ncurves in machine translation. In: Proceedings of the 50th Annual Meet-\\ning of the Association for Computational Linguistics, Proceedings of the\\nConference. pp 22–30\\nKoshute P, Zook J, McCulloh I (2021) Recommending training set sizes for\\nclassification. CoRR abs/2102.09382. https://arxiv.org/abs/2102.09382\\nLast M (2007) Predicting and optimizing classifier utility with the power law.\\nIn: Workshops Proceedings of the 7th IEEE International Conference on\\nData Mining (ICDM 2007). pp 219–224\\nSpringer Nature 2021 LATEX template\\nLearning Curves for Decision Making\\n61\\nLast M (2009) Improving data mining utility with projective sampling. In: Pro-\\nceedings of the 15th ACM SIGKDD International Conference on Knowledge\\nDiscovery and Data Mining. pp 487–496\\nLeite R, Brazdil P (2003) Improving progressive sampling via meta-learning.\\nIn: Progress in Artificial Intelligence, 11th Protuguese Conference on\\nArtificial Intelligence, EPIA 2003. pp 313–323\\nLeite R, Brazdil P (2004) Improving progressive sampling via meta-learning\\non learning curves. In: Machine Learning: ECML 2004, 15th European\\nConference on Machine Learning. pp 250–261\\nLeite R, Brazdil P (2005) Predicting relative performance of classifiers\\nfrom samples. In: Machine Learning, Proceedings of the Twenty-Second\\nInternational Conference (ICML 2005). pp 497–503\\nLeite R, Brazdil P (2007) An iterative process for building learning curves\\nand predicting relative performance of classifiers. In: Progress in Artificial\\nIntelligence, 13th Portuguese Conference on Aritficial Intelligence, EPIA\\n2007. pp 87–98\\nLeite R, Brazdil P (2008) Selecting classifiers using metalearning with sampling\\nlandmarks and data characterization. In: Proceedings of the 2nd Planning\\nto Learn Workshop (PlanLearn) at ICML/COLT/UAI, pp 35–41\\nLeite R, Brazdil P (2010) Active testing strategy to predict the best classi-\\nfication algorithm via sampling and metalearning. In: ECAI 2010 - 19th\\nEuropean Conference on Artificial Intelligence. pp 309–314\\nLi L, Jamieson KG, DeSalvo G, et al (2017) Hyperband: A novel bandit-based\\napproach to hyperparameter optimization. Journal of Machine Learning\\nResearch 18:185:1–185:52\\nLong D, Zhang S, Zhang Y (2020) Performance prediction based on neural\\narchitecture features. Cognitive Computation and Systems 2(2):80–83\\nLoog M, Duin RP (2012) The dipping phenomenon. In: Joint IAPR Interna-\\ntional Workshops on Statistical Techniques in Pattern Recognition (SPR)\\nand Structural and Syntactic Pattern Recognition (SSPR), pp 310–317\\nLoog M, Viering TJ, Mey A (2019) Minimizers of the empirical risk and risk\\nmonotonicity. In: Advances in Neural Information Processing Systems 32,\\npp 7476–7485\\nMao Z, Jung T, Lin C, et al (2016) Predicting EEG sample size required for\\nclassification calibration. In: Foundations of Augmented Cognition: Neuroer-\\ngonomics and Operational Neuroscience - 10th International Conference, AC\\nSpringer Nature 2021 LATEX template\\n62\\nLearning Curves for Decision Making\\n2016. pp 57–68\\nMeek C, Thiesson B, Heckerman D (2002) The learning-curve sampling method\\napplied to model-based clustering. Journal of Machine Learning Research\\n2:397–418\\nMhammedi Z, Husain H (2021) Risk-monotonicity in statistical learning. In:\\nAdvances in Neural Information Processing Systems 34, pp 10,732–10,744\\nMohr F, van Rijn JN (2021) Towards model selection using learning curve\\ncross-validation. In: 8th ICML Workshop on Automated Machine Learning\\n(AutoML)\\nMohr F, van Rijn JN (2023) Fast and informative model selection using\\nlearning curve cross-validation. IEEE Transactions on Pattern Analysis and\\nMachine Intelligence 45(8):9669–9680\\nMohr F, Viering TJ, Loog M, et al (2022) LCDB 1.0: An extensive learning\\ncurves database for classification tasks. In: Machine Learning and Knowledge\\nDiscovery in Databases - European Conference, ECML PKDD. pp 3–19\\nMørch NJS, Hansen LK, Strother SC, et al (1997) Nonlinear versus lin-\\near models in functional neuroimaging: Learning curves and generalization\\ncrossover. In: Information Processing in Medical Imaging, 15th International\\nConference, IPMI’97. pp 259–270\\nMukherjee S, Tamayo P, Rogers S, et al (2003) Estimating dataset size require-\\nments for classifying DNA microarray data. Journal of Computational\\nBiology 10(2):119–142\\nMurata N, Yoshizawa S, Amari S (1992) Learning curves, model selection\\nand complexity of neural networks. In: Advances in Neural Information\\nProcessing Systems 5. pp 607–614\\nNakkiran P, Kaplun G, Bansal Y, et al (2020) Deep double descent: Where big-\\nger models and more data hurt. In: 8th International Conference on Learning\\nRepresentations, ICLR’20\\nNakkiran P, Venkat P, Kakade SM, et al (2021) Optimal regularization\\ncan mitigate double descent. In: 9th International Conference on Learning\\nRepresentations, ICLR 2021\\nNg AY, Jordan MI (2001) On discriminative vs. generative classifiers: A\\ncomparison of logistic regression and naive bayes. In: Advances in Neural\\nInformation Processing Systems 14. pp 841–848\\nSpringer Nature 2021 LATEX template\\nLearning Curves for Decision Making\\n63\\nNg W, Dash M (2006) An evaluation of progressive sampling for imbal-\\nanced data sets. In: Workshops Proceedings of the 6th IEEE International\\nConference on Data Mining (ICDM 2006). pp 657–661\\nOyedare T, Park JJ (2019) Estimating the required training dataset size for\\ntransmitter classification using deep learning. In: 2019 IEEE International\\nSymposium on Dynamic Spectrum Access Networks, DySPAN 2019. pp 1–10\\nPerlich C, Provost FJ, Simonoff JS (2003) Tree induction vs. logistic regression:\\nA learning-curve analysis. Journal of Machine Learning Research 4:211–255\\nPetrak J (2000) Fast subsampling performance estimates for classification\\nalgorithm selection. In: Proceedings of the ECML-00 Workshop on Meta-\\nLearning: Building Automatic Advice Strategies for Model Selection and\\nMethod Combination, pp 3–14\\nPfisterer F, Schneider L, Moosbauer J, et al (2022) YAHPO gym - an efficient\\nmulti-objective multi-fidelity benchmark for hyperparameter optimization.\\nIn: International Conference on Automated Machine Learning, AutoML. pp\\n3/1–39\\nProvost FJ, Jensen DD, Oates T (1999) Efficient progressive sampling. In: Pro-\\nceedings of the Fifth ACM SIGKDD International Conference on Knowledge\\nDiscovery and Data Mining. pp 23–32\\nRichter AN, Khoshgoftaar TM (2019) Approximating learning curves for\\nimbalanced big data with limited labels. In: 31st IEEE International\\nConference on Tools with Artificial Intelligence, ICTAI 2019. pp 237–242\\nvan Rijn JN, Abdulrahman SM, Brazdil P, et al (2015) Fast algorithm selection\\nusing learning curves. In: Advances in Intelligent Data Analysis XIV. pp\\n298–309\\nRuhkopf T, Mohan A, Deng D, et al (2023) Masif: Meta-learned algo-\\nrithm selection using implicit fidelity information. Transactions on Machine\\nLearning Research 2023\\nSabharwal A, Samulowitz H, Tesauro G (2016) Selecting near-optimal learners\\nvia incremental data allocation. In: Proceedings of the AAAI Conference on\\nArtificial Intelligence\\nSarkar A, Guo J, Siegmund N, et al (2015) Cost-efficient sampling for per-\\nformance prediction of configurable systems (T). In: 30th IEEE/ACM\\nInternational Conference on Automated Software Engineering, ASE 2015.\\npp 342–352\\nSpringer Nature 2021 LATEX template\\n64\\nLearning Curves for Decision Making\\nSettles B (2009) Active learning literature survey. Tech. rep., University of\\nWisconsin\\nSeung HS, Sompolinsky H, Tishby N (1992) Statistical mechanics of learning\\nfrom examples. Physical Review A 45(8):6056\\nSiems J, Zimmer L, Zela A, et al (2020) Nas-bench-301 and the case for sur-\\nrogate benchmarks for neural architecture search. CoRR abs/2008.09777.\\nhttps://arxiv.org/abs/2008.09777\\nSingh S (2005) Modeling performance of different classification methods: devi-\\nation from the power law. Project Report, Department of Computer Science,\\nVanderbilt University, USA\\nStrang B, van der Putten P, van Rijn JN, et al (2018) Don’t rule out simple\\nmodels prematurely: A large scale benchmark comparing linear and non-\\nlinear classifiers in openml. In: Advances in Intelligent Data Analysis XVII.\\npp 303–315\\nSwersky K, Snoek J, Adams RP (2014) Freeze-thaw bayesian optimization.\\nCoRR abs/1406.3896. https://arxiv.org/abs/1406.3896\\nTomanek K (2010) Resource-aware annotation through active learning. PhD\\nthesis, Dortmund University of Technology\\nVallet F, Cailton JG, Refregier P (1989) Linear and nonlinear extension of the\\npseudo-inverse solution for learning boolean functions. EPL (Europhysics\\nLetters) 9(4):315\\nViering TJ, Loog M (2023) The shape of learning curves: A review. IEEE\\nTransactions on Pattern Analysis and Machine Intelligence 45(6):7799–7819\\nViering TJ, Mey A, Loog M (2020) Making learners (more) monotone. In:\\nAdvances in Intelligent Data Analysis XVIII. pp 535–547\\nWaltz M, Fu K (1965) A heuristic approach to reinforcement learning control\\nsystems. IEEE Transactions on Automatic Control 10(4):390–398\\nWang X, Chen Y, Zhu W (2022) A survey on curriculum learning. IEEE\\nTransactions on Pattern Analysis and Machine Intelligence 44(9):4555–4576\\nWeiss GM, Provost FJ (2003) Learning when training data are costly: The\\neffect of class distribution on tree induction. Journal of Artificial Intelligence\\nResearch 19:315–354\\nWeiss GM, Tian Y (2006) Maximizing classifier utility when training data is\\ncostly. SIGKDD Explorations 8(2):31–38\\nSpringer Nature 2021 LATEX template\\nLearning Curves for Decision Making\\n65\\nWeiss GM, Tian Y (2008) Maximizing classifier utility when there are data\\nacquisition and modeling costs. Data Mining and Knowledge Discovery\\n17(2):253–282\\nWhite C, Safari M, Sukthanker R, et al (2023) Neural architecture search:\\nInsights from 1000 papers. CoRR abs/2301.08727. https://arxiv.org/abs/\\n2301.08727\\nWistuba M, Pedapati T (2019) Inductive transfer for neural architecture\\noptimization. CoRR abs/1903.03536. https://arxiv.org/abs/1903.03536\\nZeng X, Luo G (2017) Progressive sampling-based bayesian optimization for\\nefficient and automatic machine learning model selection. Health Informa-\\ntion Science and Systems 5(1):2\\n')]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c740f4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs=ArxivLoader(query=\"attention is all u need\", max_results=2).load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1268091",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import WikipediaLoader\n",
    "doc=WikipediaLoader(query=\"generative ai\", max_results=2).load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3538c1d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"hello\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37af5245",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d5d86b07",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "fbd344a2",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValidationError",
     "evalue": "1 validation error for GitHubIssuesLoader\nrepo\n  Field required [type=missing, input_value={'repo_path': 'langchain-...iYOAxJFNWZA7F3koP9kDEm'}, input_type=dict]\n    For further information visit https://errors.pydantic.dev/2.12/v/missing",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValidationError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[24], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mlangchain_community\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdocument_loaders\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m GitHubIssuesLoader\n\u001b[1;32m----> 2\u001b[0m loader \u001b[38;5;241m=\u001b[39m \u001b[43mGitHubIssuesLoader\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrepo_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlangchain-ai/langchain-community\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m      4\u001b[0m \u001b[43m)\u001b[49m\n\u001b[0;32m      5\u001b[0m docs \u001b[38;5;241m=\u001b[39m loader\u001b[38;5;241m.\u001b[39mload()    \n",
      "File \u001b[1;32mc:\\xampp\\htdocs\\programming\\python\\genai_evn\\lib\\site-packages\\pydantic\\main.py:250\u001b[0m, in \u001b[0;36mBaseModel.__init__\u001b[1;34m(self, **data)\u001b[0m\n\u001b[0;32m    248\u001b[0m \u001b[38;5;66;03m# `__tracebackhide__` tells pytest and some other tools to omit this function from tracebacks\u001b[39;00m\n\u001b[0;32m    249\u001b[0m __tracebackhide__ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m--> 250\u001b[0m validated_self \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__pydantic_validator__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalidate_python\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mself_instance\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    251\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m validated_self:\n\u001b[0;32m    252\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m    253\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mA custom validator is returning a value other than `self`.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    254\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mReturning anything other than `self` from a top level model validator isn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt supported when validating via `__init__`.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    255\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSee the `model_validator` docs (https://docs.pydantic.dev/latest/concepts/validators/#model-validators) for more details.\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m    256\u001b[0m         stacklevel\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m,\n\u001b[0;32m    257\u001b[0m     )\n",
      "\u001b[1;31mValidationError\u001b[0m: 1 validation error for GitHubIssuesLoader\nrepo\n  Field required [type=missing, input_value={'repo_path': 'langchain-...iYOAxJFNWZA7F3koP9kDEm'}, input_type=dict]\n    For further information visit https://errors.pydantic.dev/2.12/v/missing"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import GitHubIssuesLoader\n",
    "loader = GitHubIssuesLoader(\n",
    "    repo_path=\"langchain-ai/langchain-community\",\n",
    ")\n",
    "docs = loader.load()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3c8e561e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import GithubFileLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "87dee20c",
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = GithubFileLoader(\n",
    "    repo=\"langchain-ai/langchain\",  # the repo name\n",
    "    branch=\"master\",  # the branch name\n",
    "    #access_token=ACCESS_TOKEN,\n",
    "    github_api_url=\"https://api.github.com\",\n",
    "    file_filter=lambda file_path: file_path.endswith(\n",
    "        \".md\"\n",
    "    ),  # load all markdowns files.\n",
    ")\n",
    "documents = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d9474335",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'path': '.devcontainer/README.md', 'sha': '8f1b9e4330347921dd0af656f7602dc1cbd776f8', 'source': 'https://api.github.com/langchain-ai/langchain/blob/master/.devcontainer/README.md'}, page_content=\"# Dev container\\n\\nThis project includes a [dev container](https://containers.dev/), which lets you use a container as a full-featured dev environment.\\n\\nYou can use the dev container configuration in this folder to build and run the app without needing to install any of its tools locally! You can use it in [GitHub Codespaces](https://github.com/features/codespaces) or the [VS Code Dev Containers extension](https://marketplace.visualstudio.com/items?itemName=ms-vscode-remote.remote-containers).\\n\\n## GitHub Codespaces\\n\\n[![Open in GitHub Codespaces](https://github.com/codespaces/badge.svg)](https://codespaces.new/langchain-ai/langchain)\\n\\nYou may use the button above, or follow these steps to open this repo in a Codespace:\\n\\n1. Click the **Code** drop-down menu at the top of <https://github.com/langchain-ai/langchain>.\\n1. Click on the **Codespaces** tab.\\n1. Click **Create codespace on master**.\\n\\nFor more info, check out the [GitHub documentation](https://docs.github.com/en/free-pro-team@latest/github/developing-online-with-codespaces/creating-a-codespace#creating-a-codespace).\\n\\n## VS Code Dev Containers\\n\\n[![Open in Dev Containers](https://img.shields.io/static/v1?label=Dev%20Containers&message=Open&color=blue&logo=visualstudiocode)](https://vscode.dev/redirect?url=vscode://ms-vscode-remote.remote-containers/cloneInVolume?url=https://github.com/langchain-ai/langchain)\\n\\n> [!NOTE]\\n> If you click the link above you will open the main repo (`langchain-ai/langchain`) and *not* your local cloned repo. This is fine if you only want to run and test the library, but if you want to contribute you can use the link below and replace with your username and cloned repo name:\\n\\n```txt\\nhttps://vscode.dev/redirect?url=vscode://ms-vscode-remote.remote-containers/cloneInVolume?url=https://github.com/&lt;YOUR_USERNAME&gt;/&lt;YOUR_CLONED_REPO_NAME&gt;\\n```\\n\\nThen you will have a local cloned repo where you can contribute and then create pull requests.\\n\\nIf you already have VS Code and Docker installed, you can use the button above to get started. This will use VSCode to automatically install the Dev Containers extension if needed, clone the source code into a container volume, and spin up a dev container for use.\\n\\nAlternatively you can also follow these steps to open this repo in a container using the VS Code Dev Containers extension:\\n\\n1. If this is your first time using a development container, please ensure your system meets the pre-reqs (i.e. have Docker installed) in the [getting started steps](https://aka.ms/vscode-remote/containers/getting-started).\\n\\n2. Open a locally cloned copy of the code:\\n\\n   - Fork and Clone this repository to your local filesystem.\\n   - Press <kbd>F1</kbd> and select the **Dev Containers: Open Folder in Container...** command.\\n   - Select the cloned copy of this folder, wait for the container to start, and try things out!\\n\\nYou can learn more in the [Dev Containers documentation](https://code.visualstudio.com/docs/devcontainers/containers).\\n\\n## Tips and tricks\\n\\n- If you are working with the same repository folder in a container and Windows, you'll want consistent line endings (otherwise you may see hundreds of changes in the SCM view). The `.gitattributes` file in the root of this repo will disable line ending conversion and should prevent this. See [tips and tricks](https://code.visualstudio.com/docs/devcontainers/tips-and-tricks#_resolving-git-line-ending-issues-in-containers-resulting-in-many-modified-files) for more info.\\n- If you'd like to review the contents of the image used in this dev container, you can check it out in the [devcontainers/images](https://github.com/devcontainers/images/tree/main/src/python) repo.\\n\"),\n",
       " Document(metadata={'path': '.github/PULL_REQUEST_TEMPLATE.md', 'sha': '41095e85344a503bdfd305a6e72619f03660a710', 'source': 'https://api.github.com/langchain-ai/langchain/blob/master/.github/PULL_REQUEST_TEMPLATE.md'}, page_content='(Replace this entire block of text)\\n\\nRead the full contributing guidelines: https://docs.langchain.com/oss/python/contributing/overview\\n\\nThank you for contributing to LangChain! Follow these steps to have your pull request considered as ready for review.\\n\\n1. PR title: Should follow the format: TYPE(SCOPE): DESCRIPTION\\n\\n  - Examples:\\n    - fix(anthropic): resolve flag parsing error\\n    - feat(core): add multi-tenant support\\n    - test(openai): update API usage tests\\n  - Allowed TYPE and SCOPE values: https://github.com/langchain-ai/langchain/blob/master/.github/workflows/pr_lint.yml#L15-L33\\n\\n2. PR description:\\n\\n  - Write 1-2 sentences summarizing the change.\\n  - If this PR addresses a specific issue, please include \"Fixes #ISSUE_NUMBER\" in the description to automatically close the issue when the PR is merged.\\n  - If there are any breaking changes, please clearly describe them.\\n  - If this PR depends on another PR being merged first, please include \"Depends on #PR_NUMBER\" inthe description.\\n\\n3. Run `make format`, `make lint` and `make test` from the root of the package(s) you\\'ve modified.\\n\\n  - We will not consider a PR unless these three are passing in CI.\\n\\nAdditional guidelines:\\n\\n  - We ask that if you use generative AI for your contribution, you include a disclaimer.\\n  - PRs should not touch more than one package unless absolutely necessary.\\n  - Do not update the `uv.lock` files unless or add dependencies to `pyproject.toml` files (even optional ones) unless you have explicit permission to do so by a maintainer.\\n'),\n",
       " Document(metadata={'path': 'AGENTS.md', 'sha': '897dcb2e88d5b884909b205e10be228c597507b4', 'source': 'https://api.github.com/langchain-ai/langchain/blob/master/AGENTS.md'}, page_content='# Global development guidelines for the LangChain monorepo\\n\\nThis document provides context to understand the LangChain Python project and assist with development.\\n\\n## Project architecture and context\\n\\n### Monorepo structure\\n\\nThis is a Python monorepo with multiple independently versioned packages that use `uv`.\\n\\n```txt\\nlangchain/\\n├── libs/\\n│   ├── core/             # `langchain-core` primitives and base abstractions\\n│   ├── langchain/        # `langchain-classic` (legacy, no new features)\\n│   ├── langchain_v1/     # Actively maintained `langchain` package\\n│   ├── partners/         # Third-party integrations\\n│   │   ├── openai/       # OpenAI models and embeddings\\n│   │   ├── anthropic/    # Anthropic (Claude) integration\\n│   │   ├── ollama/       # Local model support\\n│   │   └── ... (other integrations maintained by the LangChain team)\\n│   ├── text-splitters/   # Document chunking utilities\\n│   ├── standard-tests/   # Shared test suite for integrations\\n│   ├── model-profiles/   # Model configuration profiles\\n│   └── cli/              # Command-line interface tools\\n├── .github/              # CI/CD workflows and templates\\n├── .vscode/              # VSCode IDE standard settings and recommended extensions\\n└── README.md             # Information about LangChain\\n```\\n\\n- **Core layer** (`langchain-core`): Base abstractions, interfaces, and protocols. Users should not need to know about this layer directly.\\n- **Implementation layer** (`langchain`): Concrete implementations and high-level public utilities\\n- **Integration layer** (`partners/`): Third-party service integrations. Note that this monorepo is not exhaustive of all LangChain integrations; some are maintained in separate repos, such as `langchain-ai/langchain-google` and `langchain-ai/langchain-aws`. Usually these repos are cloned at the same level as this monorepo, so if needed, you can refer to their code directly by navigating to `../langchain-google/` from this monorepo.\\n- **Testing layer** (`standard-tests/`): Standardized integration tests for partner integrations\\n\\n### Development tools & commands**\\n\\n- `uv` – Fast Python package installer and resolver (replaces pip/poetry)\\n- `make` – Task runner for common development commands. Feel free to look at the `Makefile` for available commands and usage patterns.\\n- `ruff` – Fast Python linter and formatter\\n- `mypy` – Static type checking\\n- `pytest` – Testing framework\\n\\nThis monorepo uses `uv` for dependency management. Local development uses editable installs: `[tool.uv.sources]`\\n\\nEach package in `libs/` has its own `pyproject.toml` and `uv.lock`.\\n\\n```bash\\n# Run unit tests (no network)\\nmake test\\n\\n# Run specific test file\\nuv run --group test pytest tests/unit_tests/test_specific.py\\n```\\n\\n```bash\\n# Lint code\\nmake lint\\n\\n# Format code\\nmake format\\n\\n# Type checking\\nuv run --group lint mypy .\\n```\\n\\n#### Key config files\\n\\n- pyproject.toml: Main workspace configuration with dependency groups\\n- uv.lock: Locked dependencies for reproducible builds\\n- Makefile: Development tasks\\n\\n#### Commit standards\\n\\nSuggest PR titles that follow Conventional Commits format. Refer to .github/workflows/pr_lint for allowed types and scopes.\\n\\n#### Pull request guidelines\\n\\n- Always add a disclaimer to the PR description mentioning how AI agents are involved with the contribution.\\n- Describe the \"why\" of the changes, why the proposed solution is the right one. Limit prose.\\n- Highlight areas of the proposed changes that require careful review.\\n\\n## Core development principles\\n\\n### Maintain stable public interfaces\\n\\nCRITICAL: Always attempt to preserve function signatures, argument positions, and names for exported/public methods. Do not make breaking changes.\\n\\n**Before making ANY changes to public APIs:**\\n\\n- Check if the function/class is exported in `__init__.py`\\n- Look for existing usage patterns in tests and examples\\n- Use keyword-only arguments for new parameters: `*, new_param: str = \"default\"`\\n- Mark experimental features clearly with docstring warnings (using MkDocs Material admonitions, like `!!! warning`)\\n\\nAsk: \"Would this change break someone\\'s code if they used it last week?\"\\n\\n### Code quality standards\\n\\nAll Python code MUST include type hints and return types.\\n\\n```python title=\"Example\"\\ndef filter_unknown_users(users: list[str], known_users: set[str]) -> list[str]:\\n    \"\"\"Single line description of the function.\\n\\n    Any additional context about the function can go here.\\n\\n    Args:\\n        users: List of user identifiers to filter.\\n        known_users: Set of known/valid user identifiers.\\n\\n    Returns:\\n        List of users that are not in the known_users set.\\n    \"\"\"\\n```\\n\\n- Use descriptive, self-explanatory variable names.\\n- Follow existing patterns in the codebase you\\'re modifying\\n- Attempt to break up complex functions (>20 lines) into smaller, focused functions where it makes sense\\n\\n### Testing requirements\\n\\nEvery new feature or bugfix MUST be covered by unit tests.\\n\\n- Unit tests: `tests/unit_tests/` (no network calls allowed)\\n- Integration tests: `tests/integration_tests/` (network calls permitted)\\n- We use `pytest` as the testing framework; if in doubt, check other existing tests for examples.\\n- The testing file structure should mirror the source code structure.\\n\\n**Checklist:**\\n\\n- [ ] Tests fail when your new logic is broken\\n- [ ] Happy path is covered\\n- [ ] Edge cases and error conditions are tested\\n- [ ] Use fixtures/mocks for external dependencies\\n- [ ] Tests are deterministic (no flaky tests)\\n- [ ] Does the test suite fail if your new logic is broken?\\n\\n### Security and risk assessment\\n\\n- No `eval()`, `exec()`, or `pickle` on user-controlled input\\n- Proper exception handling (no bare `except:`) and use a `msg` variable for error messages\\n- Remove unreachable/commented code before committing\\n- Race conditions or resource leaks (file handles, sockets, threads).\\n- Ensure proper resource cleanup (file handles, connections)\\n\\n### Documentation standards\\n\\nUse Google-style docstrings with Args section for all public functions.\\n\\n```python title=\"Example\"\\ndef send_email(to: str, msg: str, *, priority: str = \"normal\") -> bool:\\n    \"\"\"Send an email to a recipient with specified priority.\\n\\n    Any additional context about the function can go here.\\n\\n    Args:\\n        to: The email address of the recipient.\\n        msg: The message body to send.\\n        priority: Email priority level.\\n\\n    Returns:\\n        `True` if email was sent successfully, `False` otherwise.\\n\\n    Raises:\\n        InvalidEmailError: If the email address format is invalid.\\n        SMTPConnectionError: If unable to connect to email server.\\n    \"\"\"\\n```\\n\\n- Types go in function signatures, NOT in docstrings\\n  - If a default is present, DO NOT repeat it in the docstring unless there is post-processing or it is set conditionally.\\n- Focus on \"why\" rather than \"what\" in descriptions\\n- Document all parameters, return values, and exceptions\\n- Keep descriptions concise but clear\\n- Ensure American English spelling (e.g., \"behavior\", not \"behaviour\")\\n\\n## Additional resources\\n\\n- **Documentation:** https://docs.langchain.com/oss/python/langchain/overview and source at https://github.com/langchain-ai/docs or `../docs/`. Prefer the local install and use file search tools for best results. If needed, use the docs MCP server as defined in `.mcp.json` for programmatic access.\\n- **Contributing Guide:** [`.github/CONTRIBUTING.md`](https://docs.langchain.com/oss/python/contributing/overview)\\n'),\n",
       " Document(metadata={'path': 'CLAUDE.md', 'sha': '897dcb2e88d5b884909b205e10be228c597507b4', 'source': 'https://api.github.com/langchain-ai/langchain/blob/master/CLAUDE.md'}, page_content='# Global development guidelines for the LangChain monorepo\\n\\nThis document provides context to understand the LangChain Python project and assist with development.\\n\\n## Project architecture and context\\n\\n### Monorepo structure\\n\\nThis is a Python monorepo with multiple independently versioned packages that use `uv`.\\n\\n```txt\\nlangchain/\\n├── libs/\\n│   ├── core/             # `langchain-core` primitives and base abstractions\\n│   ├── langchain/        # `langchain-classic` (legacy, no new features)\\n│   ├── langchain_v1/     # Actively maintained `langchain` package\\n│   ├── partners/         # Third-party integrations\\n│   │   ├── openai/       # OpenAI models and embeddings\\n│   │   ├── anthropic/    # Anthropic (Claude) integration\\n│   │   ├── ollama/       # Local model support\\n│   │   └── ... (other integrations maintained by the LangChain team)\\n│   ├── text-splitters/   # Document chunking utilities\\n│   ├── standard-tests/   # Shared test suite for integrations\\n│   ├── model-profiles/   # Model configuration profiles\\n│   └── cli/              # Command-line interface tools\\n├── .github/              # CI/CD workflows and templates\\n├── .vscode/              # VSCode IDE standard settings and recommended extensions\\n└── README.md             # Information about LangChain\\n```\\n\\n- **Core layer** (`langchain-core`): Base abstractions, interfaces, and protocols. Users should not need to know about this layer directly.\\n- **Implementation layer** (`langchain`): Concrete implementations and high-level public utilities\\n- **Integration layer** (`partners/`): Third-party service integrations. Note that this monorepo is not exhaustive of all LangChain integrations; some are maintained in separate repos, such as `langchain-ai/langchain-google` and `langchain-ai/langchain-aws`. Usually these repos are cloned at the same level as this monorepo, so if needed, you can refer to their code directly by navigating to `../langchain-google/` from this monorepo.\\n- **Testing layer** (`standard-tests/`): Standardized integration tests for partner integrations\\n\\n### Development tools & commands**\\n\\n- `uv` – Fast Python package installer and resolver (replaces pip/poetry)\\n- `make` – Task runner for common development commands. Feel free to look at the `Makefile` for available commands and usage patterns.\\n- `ruff` – Fast Python linter and formatter\\n- `mypy` – Static type checking\\n- `pytest` – Testing framework\\n\\nThis monorepo uses `uv` for dependency management. Local development uses editable installs: `[tool.uv.sources]`\\n\\nEach package in `libs/` has its own `pyproject.toml` and `uv.lock`.\\n\\n```bash\\n# Run unit tests (no network)\\nmake test\\n\\n# Run specific test file\\nuv run --group test pytest tests/unit_tests/test_specific.py\\n```\\n\\n```bash\\n# Lint code\\nmake lint\\n\\n# Format code\\nmake format\\n\\n# Type checking\\nuv run --group lint mypy .\\n```\\n\\n#### Key config files\\n\\n- pyproject.toml: Main workspace configuration with dependency groups\\n- uv.lock: Locked dependencies for reproducible builds\\n- Makefile: Development tasks\\n\\n#### Commit standards\\n\\nSuggest PR titles that follow Conventional Commits format. Refer to .github/workflows/pr_lint for allowed types and scopes.\\n\\n#### Pull request guidelines\\n\\n- Always add a disclaimer to the PR description mentioning how AI agents are involved with the contribution.\\n- Describe the \"why\" of the changes, why the proposed solution is the right one. Limit prose.\\n- Highlight areas of the proposed changes that require careful review.\\n\\n## Core development principles\\n\\n### Maintain stable public interfaces\\n\\nCRITICAL: Always attempt to preserve function signatures, argument positions, and names for exported/public methods. Do not make breaking changes.\\n\\n**Before making ANY changes to public APIs:**\\n\\n- Check if the function/class is exported in `__init__.py`\\n- Look for existing usage patterns in tests and examples\\n- Use keyword-only arguments for new parameters: `*, new_param: str = \"default\"`\\n- Mark experimental features clearly with docstring warnings (using MkDocs Material admonitions, like `!!! warning`)\\n\\nAsk: \"Would this change break someone\\'s code if they used it last week?\"\\n\\n### Code quality standards\\n\\nAll Python code MUST include type hints and return types.\\n\\n```python title=\"Example\"\\ndef filter_unknown_users(users: list[str], known_users: set[str]) -> list[str]:\\n    \"\"\"Single line description of the function.\\n\\n    Any additional context about the function can go here.\\n\\n    Args:\\n        users: List of user identifiers to filter.\\n        known_users: Set of known/valid user identifiers.\\n\\n    Returns:\\n        List of users that are not in the known_users set.\\n    \"\"\"\\n```\\n\\n- Use descriptive, self-explanatory variable names.\\n- Follow existing patterns in the codebase you\\'re modifying\\n- Attempt to break up complex functions (>20 lines) into smaller, focused functions where it makes sense\\n\\n### Testing requirements\\n\\nEvery new feature or bugfix MUST be covered by unit tests.\\n\\n- Unit tests: `tests/unit_tests/` (no network calls allowed)\\n- Integration tests: `tests/integration_tests/` (network calls permitted)\\n- We use `pytest` as the testing framework; if in doubt, check other existing tests for examples.\\n- The testing file structure should mirror the source code structure.\\n\\n**Checklist:**\\n\\n- [ ] Tests fail when your new logic is broken\\n- [ ] Happy path is covered\\n- [ ] Edge cases and error conditions are tested\\n- [ ] Use fixtures/mocks for external dependencies\\n- [ ] Tests are deterministic (no flaky tests)\\n- [ ] Does the test suite fail if your new logic is broken?\\n\\n### Security and risk assessment\\n\\n- No `eval()`, `exec()`, or `pickle` on user-controlled input\\n- Proper exception handling (no bare `except:`) and use a `msg` variable for error messages\\n- Remove unreachable/commented code before committing\\n- Race conditions or resource leaks (file handles, sockets, threads).\\n- Ensure proper resource cleanup (file handles, connections)\\n\\n### Documentation standards\\n\\nUse Google-style docstrings with Args section for all public functions.\\n\\n```python title=\"Example\"\\ndef send_email(to: str, msg: str, *, priority: str = \"normal\") -> bool:\\n    \"\"\"Send an email to a recipient with specified priority.\\n\\n    Any additional context about the function can go here.\\n\\n    Args:\\n        to: The email address of the recipient.\\n        msg: The message body to send.\\n        priority: Email priority level.\\n\\n    Returns:\\n        `True` if email was sent successfully, `False` otherwise.\\n\\n    Raises:\\n        InvalidEmailError: If the email address format is invalid.\\n        SMTPConnectionError: If unable to connect to email server.\\n    \"\"\"\\n```\\n\\n- Types go in function signatures, NOT in docstrings\\n  - If a default is present, DO NOT repeat it in the docstring unless there is post-processing or it is set conditionally.\\n- Focus on \"why\" rather than \"what\" in descriptions\\n- Document all parameters, return values, and exceptions\\n- Keep descriptions concise but clear\\n- Ensure American English spelling (e.g., \"behavior\", not \"behaviour\")\\n\\n## Additional resources\\n\\n- **Documentation:** https://docs.langchain.com/oss/python/langchain/overview and source at https://github.com/langchain-ai/docs or `../docs/`. Prefer the local install and use file search tools for best results. If needed, use the docs MCP server as defined in `.mcp.json` for programmatic access.\\n- **Contributing Guide:** [`.github/CONTRIBUTING.md`](https://docs.langchain.com/oss/python/contributing/overview)\\n'),\n",
       " Document(metadata={'path': 'README.md', 'sha': '35e9019fb8321f003818b9f12176992ca9739d33', 'source': 'https://api.github.com/langchain-ai/langchain/blob/master/README.md'}, page_content='<div align=\"center\">\\n  <a href=\"https://www.langchain.com/\">\\n    <picture>\\n      <source media=\"(prefers-color-scheme: light)\" srcset=\".github/images/logo-dark.svg\">\\n      <source media=\"(prefers-color-scheme: dark)\" srcset=\".github/images/logo-light.svg\">\\n      <img alt=\"LangChain Logo\" src=\".github/images/logo-dark.svg\" width=\"80%\">\\n    </picture>\\n  </a>\\n</div>\\n\\n<div align=\"center\">\\n  <h3>The platform for reliable agents.</h3>\\n</div>\\n\\n<div align=\"center\">\\n  <a href=\"https://opensource.org/licenses/MIT\" target=\"_blank\"><img src=\"https://img.shields.io/pypi/l/langchain\" alt=\"PyPI - License\"></a>\\n  <a href=\"https://pypistats.org/packages/langchain\" target=\"_blank\"><img src=\"https://img.shields.io/pepy/dt/langchain\" alt=\"PyPI - Downloads\"></a>\\n  <a href=\"https://pypi.org/project/langchain/#history\" target=\"_blank\"><img src=\"https://img.shields.io/pypi/v/langchain?label=%20\" alt=\"Version\"></a>\\n  <a href=\"https://vscode.dev/redirect?url=vscode://ms-vscode-remote.remote-containers/cloneInVolume?url=https://github.com/langchain-ai/langchain\" target=\"_blank\"><img src=\"https://img.shields.io/static/v1?label=Dev%20Containers&message=Open&color=blue&logo=visualstudiocode\" alt=\"Open in Dev Containers\"></a>\\n  <a href=\"https://codespaces.new/langchain-ai/langchain\" target=\"_blank\"><img src=\"https://github.com/codespaces/badge.svg\" alt=\"Open in Github Codespace\" title=\"Open in Github Codespace\" width=\"150\" height=\"20\"></a>\\n  <a href=\"https://codspeed.io/langchain-ai/langchain\" target=\"_blank\"><img src=\"https://img.shields.io/endpoint?url=https://codspeed.io/badge.json\" alt=\"CodSpeed Badge\"></a>\\n  <a href=\"https://twitter.com/langchainai\" target=\"_blank\"><img src=\"https://img.shields.io/twitter/url/https/twitter.com/langchainai.svg?style=social&label=Follow%20%40LangChainAI\" alt=\"Twitter / X\"></a>\\n</div>\\n\\nLangChain is a framework for building agents and LLM-powered applications. It helps you chain together interoperable components and third-party integrations to simplify AI application development – all while future-proofing decisions as the underlying technology evolves.\\n\\n```bash\\npip install langchain\\n```\\n\\nIf you\\'re looking for more advanced customization or agent orchestration, check out [LangGraph](https://docs.langchain.com/oss/python/langgraph/overview), our framework for building controllable agent workflows.\\n\\n---\\n\\n**Documentation**:\\n\\n- [docs.langchain.com](https://docs.langchain.com/oss/python/langchain/overview) – Comprehensive documentation, including conceptual overviews and guides\\n- [reference.langchain.com/python](https://reference.langchain.com/python) – API reference docs for LangChain packages\\n\\n**Discussions**: Visit the [LangChain Forum](https://forum.langchain.com) to connect with the community and share all of your technical questions, ideas, and feedback.\\n\\n> [!NOTE]\\n> Looking for the JS/TS library? Check out [LangChain.js](https://github.com/langchain-ai/langchainjs).\\n\\n## Why use LangChain?\\n\\nLangChain helps developers build applications powered by LLMs through a standard interface for models, embeddings, vector stores, and more.\\n\\nUse LangChain for:\\n\\n- **Real-time data augmentation**. Easily connect LLMs to diverse data sources and external/internal systems, drawing from LangChain\\'s vast library of integrations with model providers, tools, vector stores, retrievers, and more.\\n- **Model interoperability**. Swap models in and out as your engineering team experiments to find the best choice for your application\\'s needs. As the industry frontier evolves, adapt quickly – LangChain\\'s abstractions keep you moving without losing momentum.\\n- **Rapid prototyping**. Quickly build and iterate on LLM applications with LangChain\\'s modular, component-based architecture. Test different approaches and workflows without rebuilding from scratch, accelerating your development cycle.\\n- **Production-ready features**. Deploy reliable applications with built-in support for monitoring, evaluation, and debugging through integrations like LangSmith. Scale with confidence using battle-tested patterns and best practices.\\n- **Vibrant community and ecosystem**. Leverage a rich ecosystem of integrations, templates, and community-contributed components. Benefit from continuous improvements and stay up-to-date with the latest AI developments through an active open-source community.\\n- **Flexible abstraction layers**. Work at the level of abstraction that suits your needs - from high-level chains for quick starts to low-level components for fine-grained control. LangChain grows with your application\\'s complexity.\\n\\n## LangChain ecosystem\\n\\nWhile the LangChain framework can be used standalone, it also integrates seamlessly with any LangChain product, giving developers a full suite of tools when building LLM applications.\\n\\nTo improve your LLM application development, pair LangChain with:\\n\\n- [LangGraph](https://docs.langchain.com/oss/python/langgraph/overview) – Build agents that can reliably handle complex tasks with LangGraph, our low-level agent orchestration framework. LangGraph offers customizable architecture, long-term memory, and human-in-the-loop workflows – and is trusted in production by companies like LinkedIn, Uber, Klarna, and GitLab.\\n- [Integrations](https://docs.langchain.com/oss/python/integrations/providers/overview) – List of LangChain integrations, including chat & embedding models, tools & toolkits, and more\\n- [LangSmith](https://www.langchain.com/langsmith) – Helpful for agent evals and observability. Debug poor-performing LLM app runs, evaluate agent trajectories, gain visibility in production, and improve performance over time.\\n- [LangSmith Deployment](https://docs.langchain.com/langsmith/deployments) – Deploy and scale agents effortlessly with a purpose-built deployment platform for long-running, stateful workflows. Discover, reuse, configure, and share agents across teams – and iterate quickly with visual prototyping in [LangSmith Studio](https://docs.langchain.com/langsmith/studio).\\n- [Deep Agents](https://github.com/langchain-ai/deepagents) *(new!)* – Build agents that can plan, use subagents, and leverage file systems for complex tasks\\n\\n## Additional resources\\n\\n- [API Reference](https://reference.langchain.com/python) – Detailed reference on navigating base packages and integrations for LangChain.\\n- [Contributing Guide](https://docs.langchain.com/oss/python/contributing/overview) – Learn how to contribute to LangChain projects and find good first issues.\\n- [Code of Conduct](https://github.com/langchain-ai/langchain/blob/master/.github/CODE_OF_CONDUCT.md) – Our community guidelines and standards for participation.\\n'),\n",
       " Document(metadata={'path': 'libs/README.md', 'sha': '1bfcae4e1d9e9826dfe3c1c282c3da08509ee868', 'source': 'https://api.github.com/langchain-ai/langchain/blob/master/libs/README.md'}, page_content='# Packages\\n\\n> [!IMPORTANT]\\n> [**View all LangChain integrations packages**](https://docs.langchain.com/oss/python/integrations/providers)\\n\\nThis repository is structured as a monorepo, with various packages located in this `libs/` directory. Packages to note in this directory include:\\n\\n```txt\\ncore/             # Core primitives and abstractions for langchain\\nlangchain/        # langchain-classic\\nlangchain_v1/     # langchain\\npartners/         # Certain third-party providers integrations (see below)\\nstandard-tests/   # Standardized tests for integrations\\ntext-splitters/   # Text splitter utilities\\n```\\n\\n(Each package contains its own `README.md` file with specific details about that package.)\\n\\n## Integrations (`partners/`)\\n\\nThe `partners/` directory contains a small subset of third-party provider integrations that are maintained directly by the LangChain team. These include, but are not limited to:\\n\\n* [OpenAI](https://pypi.org/project/langchain-openai/)\\n* [Anthropic](https://pypi.org/project/langchain-anthropic/)\\n* [Ollama](https://pypi.org/project/langchain-ollama/)\\n* [DeepSeek](https://pypi.org/project/langchain-deepseek/)\\n* [xAI](https://pypi.org/project/langchain-xai/)\\n* and more\\n\\nMost integrations have been moved to their own repositories for improved versioning, dependency management, collaboration, and testing. This includes packages from popular providers such as [Google](https://github.com/langchain-ai/langchain-google) and [AWS](https://github.com/langchain-ai/langchain-aws). Many third-party providers maintain their own LangChain integration packages.\\n\\nFor a full list of all LangChain integrations, please refer to the [LangChain Integrations documentation](https://docs.langchain.com/oss/python/integrations/providers).\\n'),\n",
       " Document(metadata={'path': 'libs/cli/DOCS.md', 'sha': 'd08ba956aa5e23956c42e76ebc65cad34eb342ca', 'source': 'https://api.github.com/langchain-ai/langchain/blob/master/libs/cli/DOCS.md'}, page_content=\"# `langchain`\\n\\n**Usage**:\\n\\n```console\\n$ langchain [OPTIONS] COMMAND [ARGS]...\\n```\\n\\n**Options**:\\n\\n* `--help`: Show this message and exit.\\n* `-v, --version`: Print current CLI version.\\n\\n**Commands**:\\n\\n* `app`: Manage LangChain apps\\n* `serve`: Start the LangServe app, whether it's a...\\n* `template`: Develop installable templates.\\n\\n## `langchain app`\\n\\nManage LangChain apps\\n\\n**Usage**:\\n\\n```console\\n$ langchain app [OPTIONS] COMMAND [ARGS]...\\n```\\n\\n**Options**:\\n\\n* `--help`: Show this message and exit.\\n\\n**Commands**:\\n\\n* `add`: Adds the specified template to the current...\\n* `new`: Create a new LangServe application.\\n* `remove`: Removes the specified package from the...\\n* `serve`: Starts the LangServe app.\\n\\n### `langchain app add`\\n\\nAdds the specified template to the current LangServe app.\\n\\ne.g.:\\nlangchain app add extraction-openai-functions\\nlangchain app add git+ssh://git@github.com/efriis/simple-pirate.git\\n\\n**Usage**:\\n\\n```console\\n$ langchain app add [OPTIONS] [DEPENDENCIES]...\\n```\\n\\n**Arguments**:\\n\\n* `[DEPENDENCIES]...`: The dependency to add\\n\\n**Options**:\\n\\n* `--api-path TEXT`: API paths to add\\n* `--project-dir PATH`: The project directory\\n* `--repo TEXT`: Install templates from a specific github repo instead\\n* `--branch TEXT`: Install templates from a specific branch\\n* `--help`: Show this message and exit.\\n\\n### `langchain app new`\\n\\nCreate a new LangServe application.\\n\\n**Usage**:\\n\\n```console\\n$ langchain app new [OPTIONS] NAME\\n```\\n\\n**Arguments**:\\n\\n* `NAME`: The name of the folder to create  [required]\\n\\n**Options**:\\n\\n* `--package TEXT`: Packages to seed the project with\\n* `--help`: Show this message and exit.\\n\\n### `langchain app remove`\\n\\nRemoves the specified package from the current LangServe app.\\n\\n**Usage**:\\n\\n```console\\n$ langchain app remove [OPTIONS] API_PATHS...\\n```\\n\\n**Arguments**:\\n\\n* `API_PATHS...`: The API paths to remove  [required]\\n\\n**Options**:\\n\\n* `--help`: Show this message and exit.\\n\\n### `langchain app serve`\\n\\nStarts the LangServe app.\\n\\n**Usage**:\\n\\n```console\\n$ langchain app serve [OPTIONS]\\n```\\n\\n**Options**:\\n\\n* `--port INTEGER`: The port to run the server on\\n* `--host TEXT`: The host to run the server on\\n* `--app TEXT`: The app to run, e.g. `app.server:app`\\n* `--help`: Show this message and exit.\\n\\n## `langchain serve`\\n\\nStart the LangServe app, whether it's a template or an app.\\n\\n**Usage**:\\n\\n```console\\n$ langchain serve [OPTIONS]\\n```\\n\\n**Options**:\\n\\n* `--port INTEGER`: The port to run the server on\\n* `--host TEXT`: The host to run the server on\\n* `--help`: Show this message and exit.\\n\\n## `langchain template`\\n\\nDevelop installable templates.\\n\\n**Usage**:\\n\\n```console\\n$ langchain template [OPTIONS] COMMAND [ARGS]...\\n```\\n\\n**Options**:\\n\\n* `--help`: Show this message and exit.\\n\\n**Commands**:\\n\\n* `new`: Creates a new template package.\\n* `serve`: Starts a demo app for this template.\\n\\n### `langchain template new`\\n\\nCreates a new template package.\\n\\n**Usage**:\\n\\n```console\\n$ langchain template new [OPTIONS] NAME\\n```\\n\\n**Arguments**:\\n\\n* `NAME`: The name of the folder to create  [required]\\n\\n**Options**:\\n\\n* `--with-poetry / --no-poetry`: Don't run poetry install  [default: no-poetry]\\n* `--help`: Show this message and exit.\\n\\n### `langchain template serve`\\n\\nStarts a demo app for this template.\\n\\n**Usage**:\\n\\n```console\\n$ langchain template serve [OPTIONS]\\n```\\n\\n**Options**:\\n\\n* `--port INTEGER`: The port to run the server on\\n* `--host TEXT`: The host to run the server on\\n* `--help`: Show this message and exit.\\n\"),\n",
       " Document(metadata={'path': 'libs/cli/README.md', 'sha': '7c29748e9548e21f557344ef754ead3357334e4e', 'source': 'https://api.github.com/langchain-ai/langchain/blob/master/libs/cli/README.md'}, page_content='# langchain-cli\\n\\n[![PyPI - Version](https://img.shields.io/pypi/v/langchain-cli?label=%20)](https://pypi.org/project/langchain-cli/#history)\\n[![PyPI - License](https://img.shields.io/pypi/l/langchain-cli)](https://opensource.org/licenses/MIT)\\n[![PyPI - Downloads](https://img.shields.io/pepy/dt/langchain-cli)](https://pypistats.org/packages/langchain-cli)\\n[![Twitter](https://img.shields.io/twitter/url/https/twitter.com/langchainai.svg?style=social&label=Follow%20%40LangChainAI)](https://twitter.com/langchainai)\\n\\n## Quick Install\\n\\n```bash\\npip install langchain-cli\\n```\\n\\n## 🤔 What is this?\\n\\nThis package implements the official CLI for LangChain. Right now, it is most useful for getting started with LangChain Templates!\\n\\n## 📖 Documentation\\n\\n[CLI Docs](https://github.com/langchain-ai/langchain/blob/master/libs/cli/DOCS.md)\\n\\n## 📕 Releases & Versioning\\n\\nSee our [Releases](https://docs.langchain.com/oss/python/release-policy) and [Versioning](https://docs.langchain.com/oss/python/versioning) policies.\\n\\n## 💁 Contributing\\n\\nAs an open-source project in a rapidly developing field, we are extremely open to contributions, whether it be in the form of a new feature, improved infrastructure, or better documentation.\\n\\nFor detailed information on how to contribute, see the [Contributing Guide](https://docs.langchain.com/oss/python/contributing/overview).\\n'),\n",
       " Document(metadata={'path': 'libs/cli/langchain_cli/integration_template/README.md', 'sha': '15741c62f85a530e4405cba155a935e437af0433', 'source': 'https://api.github.com/langchain-ai/langchain/blob/master/libs/cli/langchain_cli/integration_template/README.md'}, page_content='# __package_name__\\n\\nThis package contains the LangChain integration with __ModuleName__\\n\\n## Installation\\n\\n```bash\\npip install -U __package_name__\\n```\\n\\nAnd you should configure credentials by setting the following environment variables:\\n\\n* TODO: fill this out\\n\\n## Chat Models\\n\\n`Chat__ModuleName__` class exposes chat models from __ModuleName__.\\n\\n```python\\nfrom __module_name__ import Chat__ModuleName__\\n\\nmodel = Chat__ModuleName__()\\nmodel.invoke(\"Sing a ballad of LangChain.\")\\n```\\n\\n## Embeddings\\n\\n`__ModuleName__Embeddings` class exposes embeddings from __ModuleName__.\\n\\n```python\\nfrom __module_name__ import __ModuleName__Embeddings\\n\\nembeddings = __ModuleName__Embeddings()\\nembeddings.embed_query(\"What is the meaning of life?\")\\n```\\n\\n## LLMs\\n\\n`__ModuleName__LLM` class exposes LLMs from __ModuleName__.\\n\\n```python\\nfrom __module_name__ import __ModuleName__LLM\\n\\nmodel = __ModuleName__LLM()\\nmodel.invoke(\"The meaning of life is\")\\n```\\n'),\n",
       " Document(metadata={'path': 'libs/cli/langchain_cli/namespaces/migrate/.grit/patterns/_test_replace_imports.md', 'sha': '8642f0ea8aff126024d0811d3b19cc0d17264c41', 'source': 'https://api.github.com/langchain-ai/langchain/blob/master/libs/cli/langchain_cli/namespaces/migrate/.grit/patterns/_test_replace_imports.md'}, page_content='# Testing the replace_imports migration\\n\\nThis runs the v0.2 migration with a desired set of rules.\\n\\n```grit\\nlanguage python\\n\\nlangchain_all_migrations()\\n```\\n\\n## Single import\\n\\nBefore:\\n\\n```python\\nfrom langchain.chat_models import ChatOpenAI\\n```\\n\\nAfter:\\n\\n```python\\nfrom langchain_community.chat_models import ChatOpenAI\\n```\\n\\n## Community to partner\\n\\n```python\\nfrom langchain_community.chat_models import ChatOpenAI\\n```\\n\\n```python\\nfrom langchain_openai import ChatOpenAI\\n```\\n\\n## Noop\\n\\nThis file should not match at all.\\n\\n```python\\nfrom foo import ChatOpenAI\\n```\\n\\n## Mixed imports\\n\\n```python\\nfrom langchain_community.chat_models import ChatOpenAI, ChatAnthropic, foo\\n```\\n\\n```python\\nfrom langchain_community.chat_models import foo\\n\\nfrom langchain_openai import ChatOpenAI\\n\\nfrom langchain_anthropic import ChatAnthropic\\n\\n```\\n'),\n",
       " Document(metadata={'path': 'libs/cli/langchain_cli/package_template/README.md', 'sha': 'c0c98eb6844f0de8d42cffe05e56552ab00b6ce7', 'source': 'https://api.github.com/langchain-ai/langchain/blob/master/libs/cli/langchain_cli/package_template/README.md'}, page_content='# __package_name__\\n\\nTODO: What does this package do\\n\\n## Environment Setup\\n\\nTODO: What environment variables need to be set (if any)\\n\\n## Usage\\n\\nTo use this package, you should first have the LangChain CLI installed:\\n\\n```shell\\npip install -U langchain-cli\\n```\\n\\nTo create a new LangChain project and install this as the only package, you can do:\\n\\n```shell\\nlangchain app new my-app --package __package_name__\\n```\\n\\nIf you want to add this to an existing project, you can just run:\\n\\n```shell\\nlangchain app add __package_name__\\n```\\n\\nAnd add the following code to your `server.py` file:\\n\\n```python\\n__app_route_code__\\n```\\n\\n(Optional) Let\\'s now configure LangSmith.\\nLangSmith will help us trace, monitor and debug LangChain applications.\\nYou can sign up for LangSmith [here](https://smith.langchain.com/).\\nIf you don\\'t have access, you can skip this section\\n\\n```shell\\nexport LANGSMITH_TRACING=true\\nexport LANGSMITH_API_KEY=<your-api-key>\\nexport LANGSMITH_PROJECT=<your-project>  # if not specified, defaults to \"default\"\\n```\\n\\nIf you are inside this directory, then you can spin up a LangServe instance directly by:\\n\\n```shell\\nlangchain serve\\n```\\n\\nThis will start the FastAPI app with a server is running locally at\\n[http://localhost:8000](http://localhost:8000)\\n\\nWe can see all templates at [http://127.0.0.1:8000/docs](http://127.0.0.1:8000/docs)\\nWe can access the playground at [http://127.0.0.1:8000/__package_name__/playground](http://127.0.0.1:8000/__package_name__/playground)\\n\\nWe can access the template from code with:\\n\\n```python\\nfrom langserve.client import RemoteRunnable\\n\\nrunnable = RemoteRunnable(\"http://localhost:8000/__package_name__\")\\n```\\n'),\n",
       " Document(metadata={'path': 'libs/cli/langchain_cli/project_template/README.md', 'sha': 'd9b3f59e5509a28f0d51fcafbd1e0977ef643980', 'source': 'https://api.github.com/langchain-ai/langchain/blob/master/libs/cli/langchain_cli/project_template/README.md'}, page_content='# __app_name__\\n\\n## Installation\\n\\nInstall the LangChain CLI if you haven\\'t yet\\n\\n```bash\\npip install -U langchain-cli\\n```\\n\\n## Adding packages\\n\\n```bash\\n# adding packages from\\n# https://github.com/langchain-ai/langchain/tree/master/templates\\nlangchain app add $PROJECT_NAME\\n\\n# adding custom GitHub repo packages\\nlangchain app add --repo $OWNER/$REPO\\n# or with whole git string (supports other git providers):\\n# langchain app add git+https://github.com/hwchase17/chain-of-verification\\n\\n# with a custom api mount point (defaults to `/{package_name}`)\\nlangchain app add $PROJECT_NAME --api_path=/my/custom/path/rag\\n```\\n\\nNote: you remove packages by their api path\\n\\n```bash\\nlangchain app remove my/custom/path/rag\\n```\\n\\n## Setup LangSmith (Optional)\\n\\nLangSmith will help us trace, monitor and debug LangChain applications.\\nYou can sign up for LangSmith [here](https://smith.langchain.com/).\\nIf you don\\'t have access, you can skip this section\\n\\n```shell\\nexport LANGSMITH_TRACING=true\\nexport LANGSMITH_API_KEY=<your-api-key>\\nexport LANGSMITH_PROJECT=<your-project>  # if not specified, defaults to \"default\"\\n```\\n\\n## Launch LangServe\\n\\n```bash\\nlangchain serve\\n```\\n\\n## Running in Docker\\n\\nThis project folder includes a Dockerfile that allows you to easily build and host your LangServe app.\\n\\n### Building the Image\\n\\nTo build the image, you simply:\\n\\n```shell\\ndocker build . -t my-langserve-app\\n```\\n\\nIf you tag your image with something other than `my-langserve-app`,\\nnote it for use in the next step.\\n\\n### Running the Image Locally\\n\\nTo run the image, you\\'ll need to include any environment variables\\nnecessary for your application.\\n\\nIn the below example, we inject the `OPENAI_API_KEY` environment\\nvariable with the value set in my local environment\\n(`$OPENAI_API_KEY`)\\n\\nWe also expose port 8080 with the `-p 8080:8080` option.\\n\\n```shell\\ndocker run -e OPENAI_API_KEY=$OPENAI_API_KEY -p 8080:8080 my-langserve-app\\n```\\n'),\n",
       " Document(metadata={'path': 'libs/core/README.md', 'sha': 'ee35e758adde1472b9ae6267017c03cefa6e7fd4', 'source': 'https://api.github.com/langchain-ai/langchain/blob/master/libs/core/README.md'}, page_content=\"# 🦜🍎️ LangChain Core\\n\\n[![PyPI - Version](https://img.shields.io/pypi/v/langchain-core?label=%20)](https://pypi.org/project/langchain-core/#history)\\n[![PyPI - License](https://img.shields.io/pypi/l/langchain-core)](https://opensource.org/licenses/MIT)\\n[![PyPI - Downloads](https://img.shields.io/pepy/dt/langchain-core)](https://pypistats.org/packages/langchain-core)\\n[![Twitter](https://img.shields.io/twitter/url/https/twitter.com/langchainai.svg?style=social&label=Follow%20%40LangChainAI)](https://twitter.com/langchainai)\\n\\nLooking for the JS/TS version? Check out [LangChain.js](https://github.com/langchain-ai/langchainjs).\\n\\nTo help you ship LangChain apps to production faster, check out [LangSmith](https://smith.langchain.com).\\n[LangSmith](https://smith.langchain.com) is a unified developer platform for building, testing, and monitoring LLM applications.\\n\\n## Quick Install\\n\\n```bash\\npip install langchain-core\\n```\\n\\n## 🤔 What is this?\\n\\nLangChain Core contains the base abstractions that power the LangChain ecosystem.\\n\\nThese abstractions are designed to be as modular and simple as possible.\\n\\nThe benefit of having these abstractions is that any provider can implement the required interface and then easily be used in the rest of the LangChain ecosystem.\\n\\n## ⛰️ Why build on top of LangChain Core?\\n\\nThe LangChain ecosystem is built on top of `langchain-core`. Some of the benefits:\\n\\n- **Modularity**: We've designed Core around abstractions that are independent of each other, and not tied to any specific model provider.\\n- **Stability**: We are committed to a stable versioning scheme, and will communicate any breaking changes with advance notice and version bumps.\\n- **Battle-tested**: Core components have the largest install base in the LLM ecosystem, and are used in production by many companies.\\n\\n## 📖 Documentation\\n\\nFor full documentation, see the [API reference](https://reference.langchain.com/python/langchain_core/). For conceptual guides, tutorials, and examples on using LangChain, see the [LangChain Docs](https://docs.langchain.com/oss/python/langchain/overview).\\n\\n## 📕 Releases & Versioning\\n\\nSee our [Releases](https://docs.langchain.com/oss/python/release-policy) and [Versioning](https://docs.langchain.com/oss/python/versioning) policies.\\n\\n## 💁 Contributing\\n\\nAs an open-source project in a rapidly developing field, we are extremely open to contributions, whether it be in the form of a new feature, improved infrastructure, or better documentation.\\n\\nFor detailed information on how to contribute, see the [Contributing Guide](https://docs.langchain.com/oss/python/contributing/overview).\\n\"),\n",
       " Document(metadata={'path': 'libs/langchain/README.md', 'sha': 'db2519567016b45ce92b531a25b2284b7f663c27', 'source': 'https://api.github.com/langchain-ai/langchain/blob/master/libs/langchain/README.md'}, page_content='# 🦜️🔗 LangChain Classic\\n\\n[![PyPI - Version](https://img.shields.io/pypi/v/langchain-classic?label=%20)](https://pypi.org/project/langchain-classic/#history)\\n[![PyPI - License](https://img.shields.io/pypi/l/langchain-classic)](https://opensource.org/licenses/MIT)\\n[![PyPI - Downloads](https://img.shields.io/pepy/dt/langchain-classic)](https://pypistats.org/packages/langchain-classic)\\n[![Twitter](https://img.shields.io/twitter/url/https/twitter.com/langchainai.svg?style=social&label=Follow%20%40LangChainAI)](https://twitter.com/langchainai)\\n\\nLooking for the JS/TS version? Check out [LangChain.js](https://github.com/langchain-ai/langchainjs).\\n\\nTo help you ship LangChain apps to production faster, check out [LangSmith](https://smith.langchain.com).\\n[LangSmith](https://smith.langchain.com) is a unified developer platform for building, testing, and monitoring LLM applications.\\n\\n## Quick Install\\n\\n```bash\\npip install langchain-classic\\n```\\n\\n## 🤔 What is this?\\n\\nLegacy chains, `langchain-community` re-exports, indexing API, deprecated functionality, and more.\\n\\nIn most cases, you should be using the main [`langchain`](https://pypi.org/project/langchain/) package.\\n\\n## 📖 Documentation\\n\\nFor full documentation, see the [API reference](https://reference.langchain.com/python/langchain_classic). For conceptual guides, tutorials, and examples on using LangChain, see the [LangChain Docs](https://docs.langchain.com/oss/python/langchain/overview).\\n\\n## 📕 Releases & Versioning\\n\\nSee our [Releases](https://docs.langchain.com/oss/python/release-policy) and [Versioning](https://docs.langchain.com/oss/python/versioning) policies.\\n\\n## 💁 Contributing\\n\\nAs an open-source project in a rapidly developing field, we are extremely open to contributions, whether it be in the form of a new feature, improved infrastructure, or better documentation.\\n\\nFor detailed information on how to contribute, see the [Contributing Guide](https://docs.langchain.com/oss/python/contributing/overview).\\n'),\n",
       " Document(metadata={'path': 'libs/langchain/tests/README.md', 'sha': '533abea532375f704d9f164025a5d8b51a345a91', 'source': 'https://api.github.com/langchain-ai/langchain/blob/master/libs/langchain/tests/README.md'}, page_content='# LangChain Tests\\n\\n[This guide has moved to the docs](https://python.langchain.com/docs/contributing/testing)\\n'),\n",
       " Document(metadata={'path': 'libs/langchain_v1/README.md', 'sha': '514e3ecdc2b214741cdc7b1407e9eacb15d3b7e4', 'source': 'https://api.github.com/langchain-ai/langchain/blob/master/libs/langchain_v1/README.md'}, page_content='# 🦜️🔗 LangChain\\n\\n[![PyPI - Version](https://img.shields.io/pypi/v/langchain?label=%20)](https://pypi.org/project/langchain/#history)\\n[![PyPI - License](https://img.shields.io/pypi/l/langchain)](https://opensource.org/licenses/MIT)\\n[![PyPI - Downloads](https://img.shields.io/pepy/dt/langchain)](https://pypistats.org/packages/langchain)\\n[![Twitter](https://img.shields.io/twitter/url/https/twitter.com/langchainai.svg?style=social&label=Follow%20%40LangChainAI)](https://twitter.com/langchainai)\\n\\nLooking for the JS/TS version? Check out [LangChain.js](https://github.com/langchain-ai/langchainjs).\\n\\nTo help you ship LangChain apps to production faster, check out [LangSmith](https://smith.langchain.com).\\n[LangSmith](https://smith.langchain.com) is a unified developer platform for building, testing, and monitoring LLM applications.\\n\\n## Quick Install\\n\\n```bash\\npip install langchain\\n```\\n\\n## 🤔 What is this?\\n\\nLangChain is the easiest way to start building agents and applications powered by LLMs. With under 10 lines of code, you can connect to OpenAI, Anthropic, Google, and [more](https://docs.langchain.com/oss/python/integrations/providers/overview). LangChain provides a pre-built agent architecture and model integrations to help you get started quickly and seamlessly incorporate LLMs into your agents and applications.\\n\\nWe recommend you use LangChain if you want to quickly build agents and autonomous applications. Use [LangGraph](https://docs.langchain.com/oss/python/langgraph/overview), our low-level agent orchestration framework and runtime, when you have more advanced needs that require a combination of deterministic and agentic workflows, heavy customization, and carefully controlled latency.\\n\\nLangChain [agents](https://docs.langchain.com/oss/python/langchain/agents) are built on top of LangGraph in order to provide durable execution, streaming, human-in-the-loop, persistence, and more. (You do not need to know LangGraph for basic LangChain agent usage.)\\n\\n## 📖 Documentation\\n\\nFor full documentation, see the [API reference](https://reference.langchain.com/python/langchain/langchain/). For conceptual guides, tutorials, and examples on using LangChain, see the [LangChain Docs](https://docs.langchain.com/oss/python/langchain/overview).\\n\\n## 📕 Releases & Versioning\\n\\nSee our [Releases](https://docs.langchain.com/oss/python/release-policy) and [Versioning](https://docs.langchain.com/oss/python/versioning) policies.\\n\\n## 💁 Contributing\\n\\nAs an open-source project in a rapidly developing field, we are extremely open to contributions, whether it be in the form of a new feature, improved infrastructure, or better documentation.\\n\\nFor detailed information on how to contribute, see the [Contributing Guide](https://docs.langchain.com/oss/python/contributing/overview).\\n'),\n",
       " Document(metadata={'path': 'libs/model-profiles/README.md', 'sha': '42500a447530e50d64235f250fad39a47352442e', 'source': 'https://api.github.com/langchain-ai/langchain/blob/master/libs/model-profiles/README.md'}, page_content='# 🦜\\U0001faaa langchain-model-profiles\\n\\n[![PyPI - Version](https://img.shields.io/pypi/v/langchain-model-profiles?label=%20)](https://pypi.org/project/langchain-model-profiles/#history)\\n[![PyPI - License](https://img.shields.io/pypi/l/langchain-model-profiles)](https://opensource.org/licenses/MIT)\\n[![PyPI - Downloads](https://img.shields.io/pepy/dt/langchain-model-profiles)](https://pypistats.org/packages/langchain-model-profiles)\\n[![Twitter](https://img.shields.io/twitter/url/https/twitter.com/langchainai.svg?style=social&label=Follow%20%40LangChainAI)](https://twitter.com/langchainai)\\n\\n> [!WARNING]\\n> This package is currently in development and the API is subject to change.\\n\\nCLI tool for updating model profile data in LangChain integration packages.\\n\\n## Quick Install\\n\\n```bash\\npip install langchain-model-profiles\\n```\\n\\n## 🤔 What is this?\\n\\n`langchain-model-profiles` is a CLI tool for fetching and updating model capability data from [models.dev](https://github.com/sst/models.dev) for use in LangChain integration packages.\\n\\nLangChain chat models expose a `.profile` field that provides programmatic access to model capabilities such as context window sizes, supported modalities, tool calling, structured output, and more. This CLI tool helps maintainers keep that data up-to-date.\\n\\n## Data sources\\n\\nThis package is built on top of the excellent work by the [models.dev](https://github.com/sst/models.dev) project, an open source initiative that provides model capability data.\\n\\nLangChain model profiles augment the data from models.dev with some additional fields. We intend to keep this aligned with the upstream project as it evolves.\\n\\n## 📖 Documentation\\n\\nFor full documentation, see the [API reference](https://reference.langchain.com/python/langchain_model_profiles/). For conceptual guides, tutorials, and examples on using LangChain, see the [LangChain Docs](https://docs.langchain.com/oss/python/langchain/overview).\\n\\n## Usage\\n\\nUpdate model profile data for a specific provider:\\n\\n```bash\\nlangchain-profiles refresh --provider anthropic --data-dir ./langchain_anthropic/data\\n```\\n\\nThis downloads the latest model data from models.dev, merges it with any augmentations defined in `profile_augmentations.toml`, and generates a `profiles.py` file.\\n'),\n",
       " Document(metadata={'path': 'libs/partners/README.md', 'sha': '73ba9682239a9181926a1554c18acc71853a0456', 'source': 'https://api.github.com/langchain-ai/langchain/blob/master/libs/partners/README.md'}, page_content='# FAQ\\n\\nLooking for an integration not listed here? Check out the [integrations documentation](https://docs.langchain.com/oss/python/integrations/providers) and the [note](../README.md) in the `libs/` README about third-party maintained packages.\\n\\n## Integration docs\\n\\nFor full documentation, see the [primary](https://docs.langchain.com/oss/python/integrations/providers/overview) and [API reference](https://reference.langchain.com/python/integrations/) docs for integrations.\\n'),\n",
       " Document(metadata={'path': 'libs/partners/anthropic/README.md', 'sha': '35889d8f790db5dd2b9f2c4e26cd1bdf751cc878', 'source': 'https://api.github.com/langchain-ai/langchain/blob/master/libs/partners/anthropic/README.md'}, page_content=\"# langchain-anthropic\\n\\n[![PyPI - Version](https://img.shields.io/pypi/v/langchain-anthropic?label=%20)](https://pypi.org/project/langchain-anthropic/#history)\\n[![PyPI - License](https://img.shields.io/pypi/l/langchain-anthropic)](https://opensource.org/licenses/MIT)\\n[![PyPI - Downloads](https://img.shields.io/pepy/dt/langchain-anthropic)](https://pypistats.org/packages/langchain-anthropic)\\n[![Twitter](https://img.shields.io/twitter/url/https/twitter.com/langchainai.svg?style=social&label=Follow%20%40LangChainAI)](https://twitter.com/langchainai)\\n\\nLooking for the JS/TS version? Check out [LangChain.js](https://github.com/langchain-ai/langchainjs).\\n\\n## Quick Install\\n\\n```bash\\npip install langchain-anthropic\\n```\\n\\n## 🤔 What is this?\\n\\nThis package contains the LangChain integration for Anthropic's generative models.\\n\\n## 📖 Documentation\\n\\nFor full documentation, see the [API reference](https://reference.langchain.com/python/integrations/langchain_anthropic/). For conceptual guides, tutorials, and examples on using these classes, see the [LangChain Docs](https://docs.langchain.com/oss/python/integrations/providers/anthropic).\\n\\n## 📕 Releases & Versioning\\n\\nSee our [Releases](https://docs.langchain.com/oss/python/release-policy) and [Versioning](https://docs.langchain.com/oss/python/versioning) policies.\\n\\n## 💁 Contributing\\n\\nAs an open-source project in a rapidly developing field, we are extremely open to contributions, whether it be in the form of a new feature, improved infrastructure, or better documentation.\\n\\nFor detailed information on how to contribute, see the [Contributing Guide](https://docs.langchain.com/oss/python/contributing/overview).\\n\"),\n",
       " Document(metadata={'path': 'libs/partners/chroma/README.md', 'sha': '960727432a08cb630ae5eb38ac862a5d904e65ca', 'source': 'https://api.github.com/langchain-ai/langchain/blob/master/libs/partners/chroma/README.md'}, page_content='# langchain-chroma\\n\\n[![PyPI - Version](https://img.shields.io/pypi/v/langchain-chroma?label=%20)](https://pypi.org/project/langchain-chroma/#history)\\n[![PyPI - License](https://img.shields.io/pypi/l/langchain-chroma)](https://opensource.org/licenses/MIT)\\n[![PyPI - Downloads](https://img.shields.io/pepy/dt/langchain-chroma)](https://pypistats.org/packages/langchain-chroma)\\n[![Twitter](https://img.shields.io/twitter/url/https/twitter.com/langchainai.svg?style=social&label=Follow%20%40LangChainAI)](https://twitter.com/langchainai)\\n\\nLooking for the JS/TS version? Check out [LangChain.js](https://github.com/langchain-ai/langchainjs).\\n\\n## Quick Install\\n\\n```bash\\npip install langchain-chroma\\n```\\n\\n## 🤔 What is this?\\n\\nThis package contains the LangChain integration with Chroma.\\n\\n## 📖 Documentation\\n\\nView the [documentation](https://docs.langchain.com/oss/python/integrations/providers/chroma) for more details.\\n'),\n",
       " Document(metadata={'path': 'libs/partners/deepseek/README.md', 'sha': '805bb63b457d5c382f93c3bc70427a320d6eaa72', 'source': 'https://api.github.com/langchain-ai/langchain/blob/master/libs/partners/deepseek/README.md'}, page_content='# langchain-deepseek\\n\\n[![PyPI - Version](https://img.shields.io/pypi/v/langchain-deepseek?label=%20)](https://pypi.org/project/langchain-deepseek/#history)\\n[![PyPI - License](https://img.shields.io/pypi/l/langchain-deepseek)](https://opensource.org/licenses/MIT)\\n[![PyPI - Downloads](https://img.shields.io/pepy/dt/langchain-deepseek)](https://pypistats.org/packages/langchain-deepseek)\\n[![Twitter](https://img.shields.io/twitter/url/https/twitter.com/langchainai.svg?style=social&label=Follow%20%40LangChainAI)](https://twitter.com/langchainai)\\n\\nLooking for the JS/TS version? Check out [LangChain.js](https://github.com/langchain-ai/langchainjs).\\n\\n## Quick Install\\n\\n```bash\\npip install langchain-deepseek\\n```\\n\\n## 🤔 What is this?\\n\\nThis package contains the LangChain integration with DeepSeek.\\n\\n## 📖 Documentation\\n\\nFor full documentation, see the [API reference](https://reference.langchain.com/python/integrations/langchain_deepseek/). For conceptual guides, tutorials, and examples on using these classes, see the [LangChain Docs](https://docs.langchain.com/oss/python/integrations/providers/deepseek).\\n\\n## 📕 Releases & Versioning\\n\\nSee our [Releases](https://docs.langchain.com/oss/python/release-policy) and [Versioning](https://docs.langchain.com/oss/python/versioning) policies.\\n\\n## 💁 Contributing\\n\\nAs an open-source project in a rapidly developing field, we are extremely open to contributions, whether it be in the form of a new feature, improved infrastructure, or better documentation.\\n\\nFor detailed information on how to contribute, see the [Contributing Guide](https://docs.langchain.com/oss/python/contributing/overview).\\n'),\n",
       " Document(metadata={'path': 'libs/partners/exa/README.md', 'sha': '4e242f1d700a173c38d505bcbe9ba56338b9fda3', 'source': 'https://api.github.com/langchain-ai/langchain/blob/master/libs/partners/exa/README.md'}, page_content='# langchain-exa\\n\\n[![PyPI - Version](https://img.shields.io/pypi/v/langchain-exa?label=%20)](https://pypi.org/project/langchain-exa/#history)\\n[![PyPI - License](https://img.shields.io/pypi/l/langchain-exa)](https://opensource.org/licenses/MIT)\\n[![PyPI - Downloads](https://img.shields.io/pepy/dt/langchain-exa)](https://pypistats.org/packages/langchain-exa)\\n[![Twitter](https://img.shields.io/twitter/url/https/twitter.com/langchainai.svg?style=social&label=Follow%20%40LangChainAI)](https://twitter.com/langchainai)\\n\\nLooking for the JS/TS version? Check out [LangChain.js](https://github.com/langchain-ai/langchainjs).\\n\\n## Quick Install\\n\\n```bash\\npip install langchain-exa\\n```\\n\\n## 🤔 What is this?\\n\\nThis package contains the LangChain integration with Exa.\\n\\n## 📖 Documentation\\n\\nView the [documentation](https://docs.langchain.com/oss/python/integrations/providers/exa_search) for more details.\\n'),\n",
       " Document(metadata={'path': 'libs/partners/fireworks/README.md', 'sha': 'c234702e40115c666ce9ae78decc1df29cf57379', 'source': 'https://api.github.com/langchain-ai/langchain/blob/master/libs/partners/fireworks/README.md'}, page_content='# langchain-fireworks\\n\\n[![PyPI - Version](https://img.shields.io/pypi/v/langchain-fireworks?label=%20)](https://pypi.org/project/langchain-fireworks/#history)\\n[![PyPI - License](https://img.shields.io/pypi/l/langchain-fireworks)](https://opensource.org/licenses/MIT)\\n[![PyPI - Downloads](https://img.shields.io/pepy/dt/langchain-fireworks)](https://pypistats.org/packages/langchain-fireworks)\\n[![Twitter](https://img.shields.io/twitter/url/https/twitter.com/langchainai.svg?style=social&label=Follow%20%40LangChainAI)](https://twitter.com/langchainai)\\n\\nLooking for the JS/TS version? Check out [LangChain.js](https://github.com/langchain-ai/langchainjs).\\n\\n## Quick Install\\n\\n```bash\\npip install langchain-fireworks\\n```\\n\\n## 🤔 What is this?\\n\\nThis is the partner package for tying Fireworks.ai and LangChain. Fireworks really strive to provide good support for LangChain use cases, so if you run into any issues please let us know. You can reach out to us [in our Discord channel](https://discord.com/channels/1137072072808472616/)\\n\\n## 📖 Documentation\\n\\nFor full documentation, see the [API reference](https://reference.langchain.com/python/integrations/langchain_fireworks/). For conceptual guides, tutorials, and examples on using these classes, see the [LangChain Docs](https://docs.langchain.com/oss/python/integrations/providers/fireworks).\\n\\n## 📕 Releases & Versioning\\n\\nSee our [Releases](https://docs.langchain.com/oss/python/release-policy) and [Versioning](https://docs.langchain.com/oss/python/versioning) policies.\\n\\n## 💁 Contributing\\n\\nAs an open-source project in a rapidly developing field, we are extremely open to contributions, whether it be in the form of a new feature, improved infrastructure, or better documentation.\\n\\nFor detailed information on how to contribute, see the [Contributing Guide](https://docs.langchain.com/oss/python/contributing/overview).\\n'),\n",
       " Document(metadata={'path': 'libs/partners/groq/README.md', 'sha': '4d336229388503d42d4dd0f01d73a0b5421e62e5', 'source': 'https://api.github.com/langchain-ai/langchain/blob/master/libs/partners/groq/README.md'}, page_content='# langchain-groq\\n\\n[![PyPI - Version](https://img.shields.io/pypi/v/langchain-groq?label=%20)](https://pypi.org/project/langchain-groq/#history)\\n[![PyPI - License](https://img.shields.io/pypi/l/langchain-groq)](https://opensource.org/licenses/MIT)\\n[![PyPI - Downloads](https://img.shields.io/pepy/dt/langchain-groq)](https://pypistats.org/packages/langchain-groq)\\n[![Twitter](https://img.shields.io/twitter/url/https/twitter.com/langchainai.svg?style=social&label=Follow%20%40LangChainAI)](https://twitter.com/langchainai)\\n\\nLooking for the JS/TS version? Check out [LangChain.js](https://github.com/langchain-ai/langchainjs).\\n\\n## Quick Install\\n\\n```bash\\npip install langchain-groq\\n```\\n\\n## 📖 Documentation\\n\\nFor full documentation, see the [API reference](https://reference.langchain.com/python/integrations/langchain_groq/). For conceptual guides, tutorials, and examples on using these classes, see the [LangChain Docs](https://docs.langchain.com/oss/python/integrations/providers/groq).\\n\\n## 📕 Releases & Versioning\\n\\nSee our [Releases](https://docs.langchain.com/oss/python/release-policy) and [Versioning](https://docs.langchain.com/oss/python/versioning) policies.\\n\\n## 💁 Contributing\\n\\nAs an open-source project in a rapidly developing field, we are extremely open to contributions, whether it be in the form of a new feature, improved infrastructure, or better documentation.\\n\\nFor detailed information on how to contribute, see the [Contributing Guide](https://docs.langchain.com/oss/python/contributing/overview).\\n'),\n",
       " Document(metadata={'path': 'libs/partners/huggingface/README.md', 'sha': 'a7a4784e59e41368394a25278b126034f98f7522', 'source': 'https://api.github.com/langchain-ai/langchain/blob/master/libs/partners/huggingface/README.md'}, page_content='# langchain-huggingface\\n\\n[![PyPI - Version](https://img.shields.io/pypi/v/langchain-huggingface?label=%20)](https://pypi.org/project/langchain-huggingface/#history)\\n[![PyPI - License](https://img.shields.io/pypi/l/langchain-huggingface)](https://opensource.org/licenses/MIT)\\n[![PyPI - Downloads](https://img.shields.io/pepy/dt/langchain-huggingface)](https://pypistats.org/packages/langchain-huggingface)\\n[![Twitter](https://img.shields.io/twitter/url/https/twitter.com/langchainai.svg?style=social&label=Follow%20%40LangChainAI)](https://twitter.com/langchainai)\\n\\nLooking for the JS/TS version? Check out [LangChain.js](https://github.com/langchain-ai/langchainjs).\\n\\n## Quick Install\\n\\n```bash\\npip install langchain-huggingface\\n```\\n\\n## 🤔 What is this?\\n\\nThis package contains the LangChain integrations for Hugging Face related classes.\\n\\n## 📖 Documentation\\n\\nFor full documentation, see the [API reference](https://reference.langchain.com/python/integrations/langchain_huggingface/). For conceptual guides, tutorials, and examples on using these classes, see the [LangChain Docs](https://docs.langchain.com/oss/python/integrations/providers/huggingface).\\n\\n## 📕 Releases & Versioning\\n\\nSee our [Releases](https://docs.langchain.com/oss/python/release-policy) and [Versioning](https://docs.langchain.com/oss/python/versioning) policies.\\n\\n## 💁 Contributing\\n\\nAs an open-source project in a rapidly developing field, we are extremely open to contributions, whether it be in the form of a new feature, improved infrastructure, or better documentation.\\n\\nFor detailed information on how to contribute, see the [Contributing Guide](https://docs.langchain.com/oss/python/contributing/overview).\\n'),\n",
       " Document(metadata={'path': 'libs/partners/mistralai/README.md', 'sha': '6e3c3023f87aa83693d08d5f758ad89846d6e81b', 'source': 'https://api.github.com/langchain-ai/langchain/blob/master/libs/partners/mistralai/README.md'}, page_content='# langchain-mistralai\\n\\n[![PyPI - Version](https://img.shields.io/pypi/v/langchain-mistralai?label=%20)](https://pypi.org/project/langchain-mistralai/#history)\\n[![PyPI - License](https://img.shields.io/pypi/l/langchain-mistralai)](https://opensource.org/licenses/MIT)\\n[![PyPI - Downloads](https://img.shields.io/pepy/dt/langchain-mistralai)](https://pypistats.org/packages/langchain-mistralai)\\n[![Twitter](https://img.shields.io/twitter/url/https/twitter.com/langchainai.svg?style=social&label=Follow%20%40LangChainAI)](https://twitter.com/langchainai)\\n\\nLooking for the JS/TS version? Check out [LangChain.js](https://github.com/langchain-ai/langchainjs).\\n\\n## Quick Install\\n\\n```bash\\npip install langchain-mistralai\\n```\\n\\n## 📖 Documentation\\n\\nFor full documentation, see the [API reference](https://reference.langchain.com/python/integrations/langchain_mistralai/). For conceptual guides, tutorials, and examples on using these classes, see the [LangChain Docs](https://docs.langchain.com/oss/python/integrations/providers/mistralai).\\n\\n## 📕 Releases & Versioning\\n\\nSee our [Releases](https://docs.langchain.com/oss/python/release-policy) and [Versioning](https://docs.langchain.com/oss/python/versioning) policies.\\n\\n## 💁 Contributing\\n\\nAs an open-source project in a rapidly developing field, we are extremely open to contributions, whether it be in the form of a new feature, improved infrastructure, or better documentation.\\n\\nFor detailed information on how to contribute, see the [Contributing Guide](https://docs.langchain.com/oss/python/contributing/overview).\\n'),\n",
       " Document(metadata={'path': 'libs/partners/nomic/README.md', 'sha': '47d36b6d9567e06b3e8fb509abc3e689591fd77c', 'source': 'https://api.github.com/langchain-ai/langchain/blob/master/libs/partners/nomic/README.md'}, page_content='# langchain-nomic\\n\\n[![PyPI - Version](https://img.shields.io/pypi/v/langchain-nomic?label=%20)](https://pypi.org/project/langchain-nomic/#history)\\n[![PyPI - License](https://img.shields.io/pypi/l/langchain-nomic)](https://opensource.org/licenses/MIT)\\n[![PyPI - Downloads](https://img.shields.io/pepy/dt/langchain-nomic)](https://pypistats.org/packages/langchain-nomic)\\n[![Twitter](https://img.shields.io/twitter/url/https/twitter.com/langchainai.svg?style=social&label=Follow%20%40LangChainAI)](https://twitter.com/langchainai)\\n\\nLooking for the JS/TS version? Check out [LangChain.js](https://github.com/langchain-ai/langchainjs).\\n\\n## Quick Install\\n\\n```bash\\npip install langchain-nomic\\n```\\n\\n## 🤔 What is this?\\n\\nThis package contains the LangChain integration with Nomic\\n\\n## 📖 Documentation\\n\\nView the [documentation](https://docs.langchain.com/oss/python/integrations/providers/nomic) for more details.\\n'),\n",
       " Document(metadata={'path': 'libs/partners/ollama/README.md', 'sha': '1f728523175c555604c6ff79c019197a8cda58bf', 'source': 'https://api.github.com/langchain-ai/langchain/blob/master/libs/partners/ollama/README.md'}, page_content='# langchain-ollama\\n\\n[![PyPI - Version](https://img.shields.io/pypi/v/langchain-ollama?label=%20)](https://pypi.org/project/langchain-ollama/#history)\\n[![PyPI - License](https://img.shields.io/pypi/l/langchain-ollama)](https://opensource.org/licenses/MIT)\\n[![PyPI - Downloads](https://img.shields.io/pepy/dt/langchain-ollama)](https://pypistats.org/packages/langchain-ollama)\\n[![Twitter](https://img.shields.io/twitter/url/https/twitter.com/langchainai.svg?style=social&label=Follow%20%40LangChainAI)](https://twitter.com/langchainai)\\n\\nLooking for the JS/TS version? Check out [LangChain.js](https://github.com/langchain-ai/langchainjs).\\n\\n## Quick Install\\n\\n```bash\\npip install langchain-ollama\\n```\\n\\n## 🤔 What is this?\\n\\nThis package contains the LangChain integration with Ollama\\n\\n## 📖 Documentation\\n\\nFor full documentation, see the [API reference](https://reference.langchain.com/python/integrations/langchain_ollama/). For conceptual guides, tutorials, and examples on using these classes, see the [LangChain Docs](https://docs.langchain.com/oss/python/integrations/providers/ollama).\\n\\n## 📕 Releases & Versioning\\n\\nSee our [Releases](https://docs.langchain.com/oss/python/release-policy) and [Versioning](https://docs.langchain.com/oss/python/versioning) policies.\\n\\n## 💁 Contributing\\n\\nAs an open-source project in a rapidly developing field, we are extremely open to contributions, whether it be in the form of a new feature, improved infrastructure, or better documentation.\\n\\nFor detailed information on how to contribute, see the [Contributing Guide](https://docs.langchain.com/oss/python/contributing/overview).\\n'),\n",
       " Document(metadata={'path': 'libs/partners/openai/README.md', 'sha': 'b2ccecfd5eb58cec7a5f8600eda19d03102e2780', 'source': 'https://api.github.com/langchain-ai/langchain/blob/master/libs/partners/openai/README.md'}, page_content='# langchain-openai\\n\\n[![PyPI - Version](https://img.shields.io/pypi/v/langchain-openai?label=%20)](https://pypi.org/project/langchain-openai/#history)\\n[![PyPI - License](https://img.shields.io/pypi/l/langchain-openai)](https://opensource.org/licenses/MIT)\\n[![PyPI - Downloads](https://img.shields.io/pepy/dt/langchain-openai)](https://pypistats.org/packages/langchain-openai)\\n[![Twitter](https://img.shields.io/twitter/url/https/twitter.com/langchainai.svg?style=social&label=Follow%20%40LangChainAI)](https://twitter.com/langchainai)\\n\\nLooking for the JS/TS version? Check out [LangChain.js](https://github.com/langchain-ai/langchainjs).\\n\\n## Quick Install\\n\\n```bash\\npip install langchain-openai\\n```\\n\\n## 🤔 What is this?\\n\\nThis package contains the LangChain integrations for OpenAI through their `openai` SDK.\\n\\n## 📖 Documentation\\n\\nFor full documentation, see the [API reference](https://reference.langchain.com/python/integrations/langchain_openai/). For conceptual guides, tutorials, and examples on using these classes, see the [LangChain Docs](https://docs.langchain.com/oss/python/integrations/providers/openai).\\n\\n## 📕 Releases & Versioning\\n\\nSee our [Releases](https://docs.langchain.com/oss/python/release-policy) and [Versioning](https://docs.langchain.com/oss/python/versioning) policies.\\n\\n## 💁 Contributing\\n\\nAs an open-source project in a rapidly developing field, we are extremely open to contributions, whether it be in the form of a new feature, improved infrastructure, or better documentation.\\n\\nFor detailed information on how to contribute, see the [Contributing Guide](https://docs.langchain.com/oss/python/contributing/overview).\\n'),\n",
       " Document(metadata={'path': 'libs/partners/perplexity/README.md', 'sha': '70839f717d26847cc9e622afae33f49155caf929', 'source': 'https://api.github.com/langchain-ai/langchain/blob/master/libs/partners/perplexity/README.md'}, page_content='# langchain-perplexity\\n\\n[![PyPI - Version](https://img.shields.io/pypi/v/langchain-perplexity?label=%20)](https://pypi.org/project/langchain-perplexity/#history)\\n[![PyPI - License](https://img.shields.io/pypi/l/langchain-perplexity)](https://opensource.org/licenses/MIT)\\n[![PyPI - Downloads](https://img.shields.io/pepy/dt/langchain-perplexity)](https://pypistats.org/packages/langchain-perplexity)\\n[![Twitter](https://img.shields.io/twitter/url/https/twitter.com/langchainai.svg?style=social&label=Follow%20%40LangChainAI)](https://twitter.com/langchainai)\\n\\nLooking for the JS/TS version? Check out [LangChain.js](https://github.com/langchain-ai/langchainjs).\\n\\n## Quick Install\\n\\n```bash\\npip install langchain-perplexity\\n```\\n\\n## 🤔 What is this?\\n\\nThis package contains the LangChain integration with Perplexity.\\n\\n## 📖 Documentation\\n\\nFor full documentation, see the [API reference](https://reference.langchain.com/python/integrations/langchain_perplexity/). For conceptual guides, tutorials, and examples on using these classes, see the [LangChain Docs](https://docs.langchain.com/oss/python/integrations/providers/perplexity).\\n\\n## 📕 Releases & Versioning\\n\\nSee our [Releases](https://docs.langchain.com/oss/python/release-policy) and [Versioning](https://docs.langchain.com/oss/python/versioning) policies.\\n\\n## 💁 Contributing\\n\\nAs an open-source project in a rapidly developing field, we are extremely open to contributions, whether it be in the form of a new feature, improved infrastructure, or better documentation.\\n\\nFor detailed information on how to contribute, see the [Contributing Guide](https://docs.langchain.com/oss/python/contributing/overview).\\n'),\n",
       " Document(metadata={'path': 'libs/partners/prompty/README.md', 'sha': '62a9f047ad7e85cdffb885166a7d08d9ef3cca13', 'source': 'https://api.github.com/langchain-ai/langchain/blob/master/libs/partners/prompty/README.md'}, page_content=\"# langchain-prompty\\n\\n[![PyPI - Version](https://img.shields.io/pypi/v/langchain-prompty?label=%20)](https://pypi.org/project/langchain-prompty/#history)\\n[![PyPI - License](https://img.shields.io/pypi/l/langchain-prompty)](https://opensource.org/licenses/MIT)\\n[![PyPI - Downloads](https://img.shields.io/pepy/dt/langchain-prompty)](https://pypistats.org/packages/langchain-prompty)\\n[![Twitter](https://img.shields.io/twitter/url/https/twitter.com/langchainai.svg?style=social&label=Follow%20%40LangChainAI)](https://twitter.com/langchainai)\\n\\nLooking for the JS/TS version? Check out [LangChain.js](https://github.com/langchain-ai/langchainjs).\\n\\n## Quick Install\\n\\n```bash\\npip install langchain-prompty\\n```\\n\\n## 🤔 What is this?\\n\\nThis package contains the LangChain integration with Microsoft Prompty.\\n\\n## 📖 Documentation\\n\\nView the [documentation](https://docs.langchain.com/oss/python/integrations/providers/microsoft) for more details.\\n\\n## Usage\\n\\nUse the `create_chat_prompt` function to load `prompty` file as prompt.\\n\\n```python\\nfrom langchain_prompty import create_chat_prompt\\n\\nprompt = create_chat_prompt('<your .prompty file path>')\\n```\\n\\nThen you can use the prompt for next steps.\\n\\nHere is an example .prompty file:\\n\\n```prompty\\n---\\nname: Basic Prompt\\ndescription: A basic prompt that uses the GPT-3 chat API to answer questions\\nauthors:\\n  - author_1\\n  - author_2\\nmodel:\\n  api: chat\\n  configuration:\\n    azure_deployment: gpt-35-turbo\\nsample:\\n  firstName: Jane\\n  lastName: Doe\\n  question: What is the meaning of life?\\n  chat_history: []\\n---\\nsystem:\\nYou are an AI assistant who helps people find information.\\nAs the assistant, you answer questions briefly, succinctly,\\nand in a personable manner using markdown and even add some personal flair with appropriate emojis.\\n\\n{% for item in chat_history %}\\n{{item.role}}:\\n{{item.content}}\\n{% endfor %}\\n\\n\\nuser:\\n{{input}}\\n\\n```\\n\"),\n",
       " Document(metadata={'path': 'libs/partners/qdrant/README.md', 'sha': 'd78c888d9c65c8dacaff98701ce923ff9840657c', 'source': 'https://api.github.com/langchain-ai/langchain/blob/master/libs/partners/qdrant/README.md'}, page_content='# langchain-qdrant\\n\\n[![PyPI - Version](https://img.shields.io/pypi/v/langchain-qdrant?label=%20)](https://pypi.org/project/langchain-qdrant/#history)\\n[![PyPI - License](https://img.shields.io/pypi/l/langchain-qdrant)](https://opensource.org/licenses/MIT)\\n[![PyPI - Downloads](https://img.shields.io/pepy/dt/langchain-qdrant)](https://pypistats.org/packages/langchain-qdrant)\\n[![Twitter](https://img.shields.io/twitter/url/https/twitter.com/langchainai.svg?style=social&label=Follow%20%40LangChainAI)](https://twitter.com/langchainai)\\n\\nLooking for the JS/TS version? Check out [LangChain.js](https://github.com/langchain-ai/langchainjs).\\n\\n## Quick Install\\n\\n```bash\\npip install langchain-qdrant\\n```\\n\\n## 🤔 What is this?\\n\\nThis package contains the LangChain integration with [Qdrant](https://qdrant.tech/).\\n\\n## 📖 Documentation\\n\\nView the [documentation](https://docs.langchain.com/oss/python/integrations/providers/qdrant) for more details.\\n'),\n",
       " Document(metadata={'path': 'libs/partners/xai/README.md', 'sha': '72c5efc416f26d692faa127062f8b3c1b9bd0f88', 'source': 'https://api.github.com/langchain-ai/langchain/blob/master/libs/partners/xai/README.md'}, page_content='# langchain-xai\\n\\n[![PyPI - Version](https://img.shields.io/pypi/v/langchain-xai?label=%20)](https://pypi.org/project/langchain-xai/#history)\\n[![PyPI - License](https://img.shields.io/pypi/l/langchain-xai)](https://opensource.org/licenses/MIT)\\n[![PyPI - Downloads](https://img.shields.io/pepy/dt/langchain-xai)](https://pypistats.org/packages/langchain-xai)\\n[![Twitter](https://img.shields.io/twitter/url/https/twitter.com/langchainai.svg?style=social&label=Follow%20%40LangChainAI)](https://twitter.com/langchainai)\\n\\nLooking for the JS/TS version? Check out [LangChain.js](https://github.com/langchain-ai/langchainjs).\\n\\n## Quick Install\\n\\n```bash\\npip install langchain-xai\\n```\\n\\n## 🤔 What is this?\\n\\nThis package contains the LangChain integrations for [xAI](https://x.ai/) through their [APIs](https://console.x.ai).\\n\\n## 📖 Documentation\\n\\nFor full documentation, see the [API reference](https://reference.langchain.com/python/integrations/langchain_xai/). For conceptual guides, tutorials, and examples on using these classes, see the [LangChain Docs](https://docs.langchain.com/oss/python/integrations/providers/xai).\\n\\n## 📕 Releases & Versioning\\n\\nSee our [Releases](https://docs.langchain.com/oss/python/release-policy) and [Versioning](https://docs.langchain.com/oss/python/versioning) policies.\\n\\n## 💁 Contributing\\n\\nAs an open-source project in a rapidly developing field, we are extremely open to contributions, whether it be in the form of a new feature, improved infrastructure, or better documentation.\\n\\nFor detailed information on how to contribute, see the [Contributing Guide](https://docs.langchain.com/oss/python/contributing/overview).\\n'),\n",
       " Document(metadata={'path': 'libs/standard-tests/README.md', 'sha': '0f8b79eae76d4207fb5fcfa39a4b9575869095fc', 'source': 'https://api.github.com/langchain-ai/langchain/blob/master/libs/standard-tests/README.md'}, page_content='# 🦜️🔗 langchain-tests\\n\\n[![PyPI - Version](https://img.shields.io/pypi/v/langchain-tests?label=%20)](https://pypi.org/project/langchain-tests/#history)\\n[![PyPI - License](https://img.shields.io/pypi/l/langchain-tests)](https://opensource.org/licenses/MIT)\\n[![PyPI - Downloads](https://img.shields.io/pepy/dt/langchain-tests)](https://pypistats.org/packages/langchain-tests)\\n[![Twitter](https://img.shields.io/twitter/url/https/twitter.com/langchainai.svg?style=social&label=Follow%20%40LangChainAI)](https://twitter.com/langchainai)\\n\\nLooking for the JS/TS version? Check out [LangChain.js](https://github.com/langchain-ai/langchainjs).\\n\\n## Quick Install\\n\\n```bash\\npip install langchain-tests\\n```\\n\\n## 🤔 What is this?\\n\\nThis is a testing library for LangChain integrations. It contains the base classes for a standard set of tests.\\n\\n## 📖 Documentation\\n\\nFor full documentation, see the [API reference](https://reference.langchain.com/python/langchain_tests/).\\n\\n## 📕 Releases & Versioning\\n\\nSee our [Releases](https://docs.langchain.com/oss/python/release-policy) and [Versioning](https://docs.langchain.com/oss/python/versioning) policies.\\n\\nWe encourage pinning your version to a specific version in order to avoid breaking your CI when we publish new tests. We recommend upgrading to the latest version periodically to make sure you have the latest tests.\\n\\nNot pinning your version will ensure you always have the latest tests, but it may also break your CI if we introduce tests that your integration doesn\\'t pass.\\n\\n## 💁 Contributing\\n\\nAs an open-source project in a rapidly developing field, we are extremely open to contributions, whether it be in the form of a new feature, improved infrastructure, or better documentation.\\n\\nFor detailed information on how to contribute, see the [Contributing Guide](https://docs.langchain.com/oss/python/contributing/overview).\\n\\n## Usage\\n\\nTo add standard tests to an integration package (e.g., for a chat model), you need to create\\n\\n1. A unit test class that inherits from `ChatModelUnitTests`\\n2. An integration test class that inherits from `ChatModelIntegrationTests`\\n\\n`tests/unit_tests/test_standard.py`:\\n\\n```python\\n\"\"\"Standard LangChain interface tests\"\"\"\\n\\nfrom typing import Type\\n\\nimport pytest\\nfrom langchain_core.language_models import BaseChatModel\\nfrom langchain_tests.unit_tests import ChatModelUnitTests\\n\\nfrom langchain_parrot_chain import ChatParrotChain\\n\\n\\nclass TestParrotChainStandard(ChatModelUnitTests):\\n    @pytest.fixture\\n    def chat_model_class(self) -> Type[BaseChatModel]:\\n        return ChatParrotChain\\n```\\n\\n`tests/integration_tests/test_standard.py`:\\n\\n```python\\n\"\"\"Standard LangChain interface tests\"\"\"\\n\\nfrom typing import Type\\n\\nimport pytest\\nfrom langchain_core.language_models import BaseChatModel\\nfrom langchain_tests.integration_tests import ChatModelIntegrationTests\\n\\nfrom langchain_parrot_chain import ChatParrotChain\\n\\n\\nclass TestParrotChainStandard(ChatModelIntegrationTests):\\n    @pytest.fixture\\n    def chat_model_class(self) -> Type[BaseChatModel]:\\n        return ChatParrotChain\\n```\\n\\n## Reference\\n\\nThe following fixtures are configurable in the test classes. Anything not marked\\nas required is optional.\\n\\n- `chat_model_class` (required): The class of the chat model to be tested\\n- `chat_model_params`: The keyword arguments to pass to the chat model constructor\\n- `chat_model_has_tool_calling`: Whether the chat model can call tools. By default, this is set to `hasattr(chat_model_class, \\'bind_tools)`\\n- `chat_model_has_structured_output`: Whether the chat model can structured output. By default, this is set to `hasattr(chat_model_class, \\'with_structured_output\\')`\\n'),\n",
       " Document(metadata={'path': 'libs/text-splitters/README.md', 'sha': '293eebeca66d50279067cc70ffce1d99ced43949', 'source': 'https://api.github.com/langchain-ai/langchain/blob/master/libs/text-splitters/README.md'}, page_content=\"# 🦜✂️ LangChain Text Splitters\\n\\n[![PyPI - Version](https://img.shields.io/pypi/v/langchain-text-splitters?label=%20)](https://pypi.org/project/langchain-text-splitters/#history)\\n[![PyPI - License](https://img.shields.io/pypi/l/langchain-text-splitters)](https://opensource.org/licenses/MIT)\\n[![PyPI - Downloads](https://img.shields.io/pepy/dt/langchain-text-splitters)](https://pypistats.org/packages/langchain-text-splitters)\\n[![Twitter](https://img.shields.io/twitter/url/https/twitter.com/langchainai.svg?style=social&label=Follow%20%40LangChainAI)](https://twitter.com/langchainai)\\n\\nLooking for the JS/TS version? Check out [LangChain.js](https://github.com/langchain-ai/langchainjs).\\n\\n## Quick Install\\n\\n```bash\\npip install langchain-text-splitters\\n```\\n\\n## 🤔 What is this?\\n\\nLangChain Text Splitters contains utilities for splitting into chunks a wide variety of text documents.\\n\\n## 📖 Documentation\\n\\nFor full documentation, see the [API reference](https://reference.langchain.com/python/langchain_text_splitters/).\\n\\n## 📕 Releases & Versioning\\n\\nSee our [Releases](https://docs.langchain.com/oss/python/release-policy) and [Versioning](https://docs.langchain.com/oss/python/versioning) policies.\\n\\nWe encourage pinning your version to a specific version in order to avoid breaking your CI when we publish new tests. We recommend upgrading to the latest version periodically to make sure you have the latest tests.\\n\\nNot pinning your version will ensure you always have the latest tests, but it may also break your CI if we introduce tests that your integration doesn't pass.\\n\\n## 💁 Contributing\\n\\nAs an open-source project in a rapidly developing field, we are extremely open to contributions, whether it be in the form of a new feature, improved infrastructure, or better documentation.\\n\\nFor detailed information on how to contribute, see the [Contributing Guide](https://docs.langchain.com/oss/python/contributing/overview).\\n\")]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "8a3c463b",
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = GithubFileLoader(\n",
    "    repo=\"shafqatameen/opencv\",  # the repo name\n",
    "    branch=\"main\",  # the branch name\n",
    "    #access_token=ACCESS_TOKEN,\n",
    "    github_api_url=\"https://api.github.com\",\n",
    "    file_filter=lambda file_path: file_path.endswith(\n",
    "        \".md\"\n",
    "    ),  # load all markdowns files.\n",
    ")\n",
    "documents = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "9be65f38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "print(len(documents))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "d52dc0f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'path': 'README.md', 'sha': 'd25fc863cd46a04e5abb8adf9b2e2d2ff82d59a7', 'source': 'https://api.github.com/shafqatameen/opencv/blob/main/README.md'}, page_content='hello its my first time doing the text on it \\n')]"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "c466a197",
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = GithubFileLoader(\n",
    "    repo=\"shafqatameen/opencv\",  # the repo name\n",
    "    branch=\"main\",  # the branch name\n",
    "    #access_token=ACCESS_TOKEN,\n",
    "    github_api_url=\"https://api.github.com\",\n",
    "    file_filter=lambda file_path: file_path.endswith(\n",
    "        \".py\"\n",
    "    ),  # load all markdowns files.\n",
    ")\n",
    "documents = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "d4d19f50",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'path': '0demo.py', 'sha': 'cec0601486bcbe8f86c2f7b6198702b51f533646', 'source': 'https://api.github.com/shafqatameen/opencv/blob/main/0demo.py'}, page_content='import cv2\\n\\n# Load pre-trained Haarcascade model\\nface_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + \"haarcascade_fullbody.xml\")\\n\\n# Read image\\nimg = cv2.imread(\"images/park.jpg\")\\ngray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\\n\\n# Detect faces\\nfaces = face_cascade.detectMultiScale(gray, scaleFactor=1.1, minNeighbors=5, minSize=(30, 30))\\n\\n# Draw rectangles around faces\\nfor (x, y, w, h) in faces:\\n    cv2.rectangle(img, (x, y), (x + w, y + h), (255, 0, 0), 2)\\n\\n# Show output\\ncv2.imshow(\"Detected Faces\", img)\\ncv2.waitKey(0)\\ncv2.destroyAllWindows()\\n'),\n",
       " Document(metadata={'path': 'SECTION_1_BASICS/0demo.py', 'sha': '751a6f201ba693717e30e6dd7a0985079c69c0d2', 'source': 'https://api.github.com/shafqatameen/opencv/blob/main/SECTION_1_BASICS/0demo.py'}, page_content='import cv2 as cv\\n\\n#resiszng the frame of live video, its working for all videos,live and images\\ndef resizedFrame(frame,scale=75):\\n    width=int(frame.shape[1]*scale/100)\\n    height=int(frame.shape[0]*scale/100)\\n    dim=(width,height)\\n    return cv.resize(frame,dim,interpolation=cv.INTER_AREA)\\n\\'\\'\\'def changeRes(width,height):\\n    #only for live video another method for live video\\n    capture.set(3,width)\\n    capture.set(4,height)\\'\\'\\'\\n# Open the default camera (0), or use 1, 2 for external cameras\\ncapture = cv.VideoCapture(0)\\n\\n\\n# Check if the camera opened successfully\\nif not capture.isOpened():\\n    print(\"Error: Could not open camera.\")\\n    exit()\\n\\nwhile True:\\n    isTrue, frame = capture.read()\\n    if not isTrue:  # Break if frame is not read (camera error)\\n        print(\"Error reading frame.\")\\n        break\\n    #change image to other type\\n    converted_color=cv.cvtColor(frame,cv.COLOR_BGR2YUV)\\n    # Resize the frame\\n    resized_frame=resizedFrame(converted_color,scale=110)\\n    cv.imshow(\\'Webcam\\', resized_frame)\\n\\n    # Press \\'d\\' to exit\\n    if cv.waitKey(20) & 0xFF == ord(\\'d\\'):\\n        break\\n\\n# Release resources\\ncapture.release()\\ncv.destroyAllWindows()\\n'),\n",
       " Document(metadata={'path': 'SECTION_1_BASICS/10contours.py', 'sha': '3c608c3d69a2601a85e4c9e0c64f2faf4f18038c', 'source': 'https://api.github.com/shafqatameen/opencv/blob/main/SECTION_1_BASICS/10contours.py'}, page_content=\"import cv2 as cv\\nimport numpy as np\\n\\n# Read the image\\nimg=cv.imread('images/cats.jpg')\\ncv.imshow('cats',img)\\n#create blank image\\nblank=np.zeros(img.shape, dtype='uint8')\\ncv.imshow('Blank',blank)\\n\\n# Convert to gray scale\\ngray=cv.cvtColor(img,cv.COLOR_BGR2GRAY)\\ncv.imshow('gray',gray)\\n\\n# Blur the image\\nblur=cv.GaussianBlur(gray,(5,5),cv.BORDER_DEFAULT)\\ncv.imshow('blur',blur)\\n\\n# Canny edge detection\\ncanny =cv.Canny(blur,125,175)\\ncv.imshow('canny',canny)\\n\\n# Thresholding\\nret,thresh = cv.threshold(gray,125,255,cv.THRESH_BINARY)\\ncv.imshow('thresh',thresh)\\n\\n#contouurs\\ncontours, hierarchies=cv.findContours(canny,cv.RETR_LIST,cv.CHAIN_APPROX_SIMPLE)\\nprint(f'{len(contours) } contours found')\\n\\n# Draw the contours\\ncv.drawContours(blank,contours,-1,(90,90,90),1)\\ncv.imshow('contours drawn',blank)\\n\\ncv.waitKey(0)\"),\n",
       " Document(metadata={'path': 'SECTION_1_BASICS/1create_img.py', 'sha': '650a3eee89d2b6309bf7271fedeb5f589effd0a4', 'source': 'https://api.github.com/shafqatameen/opencv/blob/main/SECTION_1_BASICS/1create_img.py'}, page_content=\"import cv2 as cv\\nimport numpy as np\\n\\n# Create the image using numpy\\nblack_img = np.zeros((500,500),dtype=np.uint8)\\nwhite_img = np.ones((500,500),dtype=np.uint8)*255\\n\\n#show the image on window\\ncv.imshow('Black Image',black_img)\\ncv.imshow('White Image',white_img)\\ncv.waitKey(0)\"),\n",
       " Document(metadata={'path': 'SECTION_1_BASICS/2readimg.py', 'sha': '1a9f6ab97cc1c3483c20598173aa0a142b19ae8a', 'source': 'https://api.github.com/shafqatameen/opencv/blob/main/SECTION_1_BASICS/2readimg.py'}, page_content='import cv2 as cv\\n\\nimg= cv.imread(\"images/cat.jpg\")\\n\\ncv.imshow(\\'window ka naam cat image\\',img)\\ncv.waitKey(0)\\ncv.destroyAllWindows()\\n\\n#reading an img greater than the screen size\\nlarge_img=cv.imread(\"images/cat_large.jpg\")\\ncv.imshow(\\'it a large image\\',large_img)\\ncv.waitKey(0)\\n\\n'),\n",
       " Document(metadata={'path': 'SECTION_1_BASICS/3readvideocam.py', 'sha': 'cb16c9a287f181175cc2f32ed985735ff15024ba', 'source': 'https://api.github.com/shafqatameen/opencv/blob/main/SECTION_1_BASICS/3readvideocam.py'}, page_content='import cv2 as cv\\n\\n# Open the default camera (0), or use 1, 2 for external cameras\\ncapture = cv.VideoCapture(0)\\n\\n# Check if the camera opened successfully\\nif not capture.isOpened():\\n    print(\"Error: Could not open camera.\")\\n    exit()\\n\\nwhile True:\\n    isTrue, frame = capture.read()\\n\\n    if not isTrue:  # Break if frame is not read (camera error)\\n        print(\"Error reading frame.\")\\n        break\\n\\n    cv.imshow(\\'Webcam\\', frame)\\n\\n    # Press \\'d\\' to exit\\n    if cv.waitKey(1) & 0xFF == ord(\\'d\\'):\\n        break\\n\\n# Release resources\\ncapture.release()\\ncv.destroyAllWindows()\\n'),\n",
       " Document(metadata={'path': 'SECTION_1_BASICS/4readvideo.py', 'sha': '91209bfe48f9f0bbac4a2d2ba3aba5af344df125', 'source': 'https://api.github.com/shafqatameen/opencv/blob/main/SECTION_1_BASICS/4readvideo.py'}, page_content=\"import cv2 as cv\\ncapture =cv.VideoCapture('videos/dog.mp4')\\nif not capture.isOpened():\\n    print('Error: Could not open video.')\\n    exit()\\nwhile True:\\n    isTrue, frame = capture.read()\\n    if not isTrue:\\n        print('End of video or Error reading video.')\\n        break\\n    cv.imshow('dog video', frame)\\n    if cv.waitKey(33)==ord('z'):\\n        break\\ncapture.release()\\ncv.destroyAllWindows()\"),\n",
       " Document(metadata={'path': 'SECTION_1_BASICS/5imgconversion.py', 'sha': '614bc99451a8caca86d33b043127e20de0fedac2', 'source': 'https://api.github.com/shafqatameen/opencv/blob/main/SECTION_1_BASICS/5imgconversion.py'}, page_content=\"import cv2 as cv\\nimport numpy as np\\n# Reading the image\\nimg= cv.imread('images/cat.jpg')\\ncv.imshow('Cat', img)\\n# Converting to grayscale\\ngrayimg= cv.cvtColor(img, cv.COLOR_BGR2GRAY)\\ncv.imshow('Gray cat', grayimg)\\n#converting to hsv\\nhsvimg= cv.cvtColor(img, cv.COLOR_BGR2HSV)\\ncv.imshow('hue saturation value', hsvimg)\\n#converting to lab\\nlabimg=cv.cvtColor(img,cv.COLOR_BGR2LAB)\\ncv.imshow('lab',labimg)\\n#converting to rgb\\nrgbimg=cv.cvtColor(img,cv.COLOR_BGR2RGB)\\ncv.imshow('rgb',rgbimg)\\n#converting to hls\\nhlsimg=cv.cvtColor(img,cv.COLOR_BGR2HLS)\\ncv.imshow('hUE lightness saturation',hlsimg)\\n#converting to xyz\\nxyzimg=cv.cvtColor(img,cv.COLOR_BGR2XYZ)\\ncv.imshow('x y z',xyzimg)\\n#displaying the single channels\\ncv.imshow('blue ',img[:,:,0])\\ncv.imshow('Green ',img[:,:,1])\\ncv.imshow('Red ',img[:,:,2])\\n\\ncv.waitKey(0)\"),\n",
       " Document(metadata={'path': 'SECTION_1_BASICS/6resizeframe.py', 'sha': 'dce1d2a565453e4e406dff4b9f2b6c114f8c4aa2', 'source': 'https://api.github.com/shafqatameen/opencv/blob/main/SECTION_1_BASICS/6resizeframe.py'}, page_content=\"import cv2 as cv\\n\\n# resizing frame of video or image\\ndef rescaleFrame(frame,scale=75):\\n    width=int(frame.shape[1]*scale//100)\\n    height=int(frame.shape[0]*scale//100)\\n    dimensions=(width,height)\\n    return cv.resize(frame,dimensions,interpolation=cv.INTER_AREA)\\nframe=cv.imread('images/cat_large.jpg')\\ncv.imshow('Cat',frame)\\nrescaled_frame=rescaleFrame(frame,25)\\ncv.imshow('Cat reduced to 75%',rescaled_frame)\\ncv.waitKey(0)\\n\\n'''import cv2 as cv\\n\\ndef rescaleFrame(frame, scale=0.75):\\n    # Calculate the new dimensions based on the scale\\n    width = int(frame.shape[1] * scale)\\n    height = int(frame.shape[0] * scale)\\n    dimensions = (width, height)\\n\\n    # Resize the frame using the calculated dimensions\\n    return cv.resize(frame, dimensions, interpolation=cv.INTER_AREA)\\n\\n# Example usage:\\n# Assuming 'frame' is an image or video frame loaded using OpenCV\\nframe = cv.imread('images/cat.jpg')\\nresized_frame = rescaleFrame(frame, scale=0.5)\\ncv.imshow('Resized Frame', resized_frame)\\ncv.waitKey(0)'''\"),\n",
       " Document(metadata={'path': 'SECTION_1_BASICS/6resizeframe2.py', 'sha': '4a791a67dba78480f8f302a1946408ee20d38b5d', 'source': 'https://api.github.com/shafqatameen/opencv/blob/main/SECTION_1_BASICS/6resizeframe2.py'}, page_content='import cv2 as cv\\n#resiszng the frame of video \\ndef resizedFrame(frame,scale=75):\\n    width=int(frame.shape[1]*scale/100)\\n    height=int(frame.shape[0]*scale/100)\\n    dim=(width,height)\\n    return cv.resize(frame,dim,interpolation=cv.INTER_AREA)\\ncapture= cv.VideoCapture(\"videos/dog.mp4\")\\nwhile True:\\n    isTrue,frame=capture.read()\\n    if not isTrue:\\n        print(\\'Can not read the video\\')\\n        break\\n    cv.imshow(\\'Video\\',frame)\\n    frame_resized=resizedFrame(frame,scale=50)\\n    cv.imshow(\\'Resized video\\',frame_resized)\\n\\n    if cv.waitKey(20)&0xFF==ord(\\'d\\'):\\n        break\\ncapture.release()\\ncv.destroyAllWindows()'),\n",
       " Document(metadata={'path': 'SECTION_1_BASICS/6resizeframeCam3.py', 'sha': '4ea05613d4a95098cb945acbeb9b44222fd2d34e', 'source': 'https://api.github.com/shafqatameen/opencv/blob/main/SECTION_1_BASICS/6resizeframeCam3.py'}, page_content='import cv2 as cv\\n\\n#resiszng the frame of live video, its working for all videos,live and images\\ndef resizedFrame(frame,scale=75):\\n    width=int(frame.shape[1]*scale/100)\\n    height=int(frame.shape[0]*scale/100)\\n    dim=(width,height)\\n    return cv.resize(frame,dim,interpolation=cv.INTER_AREA)\\n\\'\\'\\'def changeRes(width,height):\\n    #only for live video another method for live video\\n    capture.set(3,width)\\n    capture.set(4,height)\\'\\'\\'\\n# Open the default camera (0), or use 1, 2 for external cameras\\ncapture = cv.VideoCapture(0)\\n\\n\\n# Check if the camera opened successfully\\nif not capture.isOpened():\\n    print(\"Error: Could not open camera.\")\\n    exit()\\n\\nwhile True:\\n    isTrue, frame = capture.read()\\n    if not isTrue:  # Break if frame is not read (camera error)\\n        print(\"Error reading frame.\")\\n        break\\n    # Resize the frame\\n    resized_frame=resizedFrame(frame,scale=20)\\n    cv.imshow(\\'Webcam\\', resized_frame)\\n\\n    # Press \\'d\\' to exit\\n    if cv.waitKey(20) & 0xFF == ord(\\'d\\'):\\n        break\\n\\n# Release resources\\ncapture.release()\\ncv.destroyAllWindows()\\n'),\n",
       " Document(metadata={'path': 'SECTION_1_BASICS/7shapes.py', 'sha': '3eebf7dd8820be17895889725d8400366e4f629b', 'source': 'https://api.github.com/shafqatameen/opencv/blob/main/SECTION_1_BASICS/7shapes.py'}, page_content='import cv2 as cv\\nimport numpy as np\\n\\n# Create the image using numpy\\nblank=np.zeros((500,500,3),dtype=\"uint8\")\\ncv.imshow(\\'Blank Image\\',blank)\\n\\n# 1. Paint the image a certain color\\nblank[0:500,0:250]=100,25,10\\ncv.imshow(\"green\",blank)\\n\\n#draw the rectangle\\ncv.rectangle(blank,pt1=(0,0),pt2=(250,250),color=(0,0,200),thickness=-1)\\ncv.imshow(\"Rectangle\",blank)\\n\\n#draw the circle\\ncv.circle(blank,(blank.shape[1]//2,blank.shape[0]//2),100,(200,0,220),thickness=-1)\\ncv.imshow(\"Circle\",blank)\\n\\n#draw the text\\ncv.putText(blank,\"hello bhai\",(0,int(0.75*blank.shape[0])),cv.FONT_HERSHEY_TRIPLEX,2,(20,255,0),1)\\ncv.imshow(\"Text\",blank)\\n\\n#draw the line\\ncv.line(blank,(0,0),(500,500),(255,255,255),3)\\ncv.imshow(\"Line\",blank)\\ncv.waitKey(0)'),\n",
       " Document(metadata={'path': 'SECTION_1_BASICS/8basic_fxns.py', 'sha': '0415f51320b41c73908af12ae37862ff0e02ec65', 'source': 'https://api.github.com/shafqatameen/opencv/blob/main/SECTION_1_BASICS/8basic_fxns.py'}, page_content=\"import cv2 as cv\\n\\nimg=cv.imread('images/park.jpg')\\ncv.imshow('Park',img)\\n\\n# 1. Convert to grayscale\\ngray=cv.cvtColor(img,cv.COLOR_BGR2GRAY)\\ncv.imshow('Gray',gray)\\n\\n# 2. Blur the image\\nblur=cv.GaussianBlur(img,(3,3),cv.BORDER_DEFAULT)\\ncv.imshow('Blur',blur)\\n\\n# 3. Edge Cascade\\ncanny=cv.Canny(blur,125,175)\\ncv.imshow('Canny Edges',canny)\\n\\n# 4. Dilating the image\\ndilated=cv.dilate(canny,(3,3),iterations=1)\\ncv.imshow('Dilated',dilated)\\n\\n# 5. Eroding\\neroded=cv.erode(dilated,(3,3),iterations=1)\\ncv.imshow('Eroded',eroded)\\ncv.waitKey(0)\\n\"),\n",
       " Document(metadata={'path': 'SECTION_1_BASICS/9transformation.py', 'sha': 'b962cce136bcadd7060352eaee5732a73fc38c95', 'source': 'https://api.github.com/shafqatameen/opencv/blob/main/SECTION_1_BASICS/9transformation.py'}, page_content='import cv2 as cv\\nimport numpy as np\\n\\n#load and read the image\\nimg=cv.imread(\"images/park.jpg\")\\ncv.imshow(\\'Park\\',img)\\n\\n# transformation functions\\ndef translate(img,x,y):\\n    transMat=np.float32([[1,0,x],[0,1,y]])\\n    dim=(img.shape[1],img.shape[0])\\n    return cv.warpAffine(img,transMat,dim)\\n\\n\\'\\'\\'x: Moves the image horizontally.\\n+x → Shifts right.\\n-x → Shifts left.\\n   y: Moves the image vertically.\\n+y → Shifts down.\\n-y → Shifts up.           \\'\\'\\'\\n\\ntranslated=translate(img,-100,-100)\\ncv.imshow(\"window\",translated)\\n\\n# Rotation\\ndef rotate(img,angle,rotPoint=None):\\n    (height,width)=img.shape[:2]\\n    if rotPoint is None:\\n        rotPoint=(width//2,height//2)\\n    rotMat=cv.getRotationMatrix2D(rotPoint,angle,1.0)\\n    dim=(width,height)\\n    return cv.warpAffine(img,rotMat,dim)\\n\\nrotated=rotate(img,180)\\ncv.imshow(\\'rotated image\\',rotated)\\n\\n# Resizing\\nresized=cv.resize(img,(1000,1000),interpolation=cv.INTER_CUBIC)\\ncv.imshow(\\'Resized\\',resized)\\n\\n#flip\\n\\'\\'\\'flipCode: Defines the flipping direction:\\n0  → Flip vertically (upside down).\\n1  → Flip horizontally (mirror image).\\n-1 → Flip both vertically & horizontally (180° rotation).\\'\\'\\'\\nfliped=cv.flip(img,-1)\\ncv.imshow(\\'Fliped\\',fliped)\\n\\n#cropping\\ncropped=img[0:img.shape[0]//2,0:img.shape[1]//2]\\ncv.imshow(\\'Cropped\\',cropped)\\n\\n\\ncv.waitKey(0)\\n'),\n",
       " Document(metadata={'path': 'SECTION_2_ADVANCE/0demo.py', 'sha': '3442eb609272aabe1d102a2210465626213d7f0f', 'source': 'https://api.github.com/shafqatameen/opencv/blob/main/SECTION_2_ADVANCE/0demo.py'}, page_content=\"import cv2 as cv\\nimport numpy as np\\n\\nimg = cv.imread('images/park.jpg')\\ngray = cv.cvtColor(img, cv.COLOR_BGR2GRAY)\\n\\nlap_float = cv.Laplacian(gray, cv.CV_64F)\\nlap_abs = np.absolute(lap_float)\\nlap_uint8 = np.uint8(lap_abs)\\n\\ncv.imshow('Laplacian Float64', lap_float / lap_float.max())  # Normalize for display\\ncv.imshow('Laplacian Absolute', lap_abs / lap_abs.max())      # Normalize for display\\ncv.imshow('Laplacian uint8', lap_uint8)\\n\\ncv.waitKey(0)\\ncv.destroyAllWindows()\\n\"),\n",
       " Document(metadata={'path': 'SECTION_2_ADVANCE/11matlab_cv2_imshow.py', 'sha': '24b94e29f5d266cf0de16bfe71446b7cea96cb90', 'source': 'https://api.github.com/shafqatameen/opencv/blob/main/SECTION_2_ADVANCE/11matlab_cv2_imshow.py'}, page_content='import cv2 as cv\\nimport matplotlib.pyplot as plt\\nfrom pathlib import Path as path\\nimport numpy as np\\n\\n#arranging the file using pathlib\\nimage=sorted(path(\"images\").glob(\\'*\\'))\\nprint(image)\\n#default bgr img\\nimg=cv.imread(image[3])\\n#cv.imshow(\"img->bgr\", img)\\n#converted into rgb img\\nimg_rgb=cv.cvtColor(img,cv.COLOR_BGR2RGB)\\n#cv.imshow(\"img->rbg\", img_rgb)\\n\\ncmbined_img=np.hstack((img_rgb,img))\\ncv.imshow(\"stack of img: img_rgb,img\", cmbined_img)\\n\\n#ushing stack display hsv, gray,lab,hls\\nhsv_img=cv.cvtColor(img,cv.COLOR_BGR2HSV)\\nlab_img=cv.cvtColor(img,cv.COLOR_BGR2LAB)\\nhls_img=cv.cvtColor(img,cv.COLOR_BGR2HLS)\\n\\nstack_img=np.hstack((hsv_img,lab_img,hls_img))\\n\\ncv.imshow(\"stack of imgs(cv)\", stack_img)\\n\\n# Display each image in a separate Matplotlib window\\n#using hstack in matplotlib for img_rgb and img_bgr\\nplt.figure(\"stack of img : img_rgb,img\")\\nplt.imshow(cmbined_img)\\nplt.title(\"stack of img : img_rgb,img\")\\n\\n\\n#using hstack in matplotlib for hsv,gray,lab,hls\\nplt.figure(\"stack of imgs(plt)\")\\nplt.imshow(stack_img)               #need image in rgb only(plt )\\nplt.title(\"stack of imgs(plt)\")\\nplt.show()\\n\\ncv.waitKey(0)\\nif cv.waitKey(0)& 0xFF ==ord(\\'q\\'):\\n    cv.destroyAllWindows()\\n\\n'),\n",
       " Document(metadata={'path': 'SECTION_2_ADVANCE/12split_merge.py', 'sha': '8d46385906e12866e01cd174b29bac6b9f546b32', 'source': 'https://api.github.com/shafqatameen/opencv/blob/main/SECTION_2_ADVANCE/12split_merge.py'}, page_content='import cv2 as  cv\\nimport numpy as np\\nfrom pathlib import Path\\n\\n#path of images\\nimage=sorted(Path(\"images\").glob(\\'*\\'))[2]\\n\\n#load img\\nimg=cv.imread(str(image))\\n#show img\\ncv.imshow(\"bgr img\",img)\\n\\nb , g , r=cv.split(img)\\nstack_rgb=np.hstack((b,g,r))\\ncv.imshow(\"stack of img\",stack_rgb,)\\n\\n#shape of img and  b g r imgs\\nprint(img.shape)\\nprint(b.shape)\\nprint(g.shape)\\nprint(r.shape)\\n\\n#merge \\nmerged=cv.merge([b,g,r])\\ncv.imshow(\"merged img\", merged)\\n\\nblank=np.zeros(img.shape[:2],dtype=\"uint8\")\\nblue=cv.merge([b,blank,blank])\\ngreen=cv.merge([blank,g,blank])\\nred=cv.merge([blank,blank,r])\\n\\nstack_colors=np.hstack((blue,green,red))\\ncv.imshow(\"stack of colors\",stack_colors)\\n\\ncv.waitKey(0)\\nif cv.waitkey(0) &0xFF==ord(\\'q\\'):\\n    cv.destroyAllWindows'),\n",
       " Document(metadata={'path': 'SECTION_2_ADVANCE/13smoothing.py', 'sha': '94978703569d53c2c2135916a44f4ec4285e60f0', 'source': 'https://api.github.com/shafqatameen/opencv/blob/main/SECTION_2_ADVANCE/13smoothing.py'}, page_content='import cv2 as cv\\nimport numpy as np\\n\\n#load img\\nimage=cv.imread(\"images/cats.jpg\")\\ncv.imshow(\"cat\",image)\\nimg=cv.resize(image,(300,300))\\ncv.imshow(\"resized cat\",img)\\n#average blur\\nimg_blur=cv.blur(img,(3,3))\\ncv.imshow(\"blur\",img_blur)\\n\\n#Gaussian blur\\nimg_gauss_blur=cv.GaussianBlur(img,(3,3),0)\\ncv.imshow(\"Gaussian blur\",img_gauss_blur)\\n\\n#median blur\\nimg_median=cv.medianBlur(img,3)\\ncv.imshow(\"median blur\",img_median)\\n\\n#bilateral blur\\nimg_bilateral=cv.bilateralFilter(img,3,75,75)\\ncv.imshow(\"bilateral blur\",img_bilateral)\\n\\n#hstack\\nstacked=np.hstack((img,img_blur,img_gauss_blur,img_median,img_bilateral))\\ncv.imshow(\"stacked (img,img_blur,img_gauss_blur,img_median,img_bilateral)\",stacked)\\n\\ncv.waitKey(0)'),\n",
       " Document(metadata={'path': 'SECTION_2_ADVANCE/14bitwise.py', 'sha': 'f15d8f75d26aeebbd1602141230e995fc3d49476', 'source': 'https://api.github.com/shafqatameen/opencv/blob/main/SECTION_2_ADVANCE/14bitwise.py'}, page_content='import cv2 as cv\\nimport numpy as np\\n\\n#create blank img\\nblank=np.zeros((400,400),dtype=\"uint8\")\\nrectangle=cv.rectangle(blank.copy(),(30,30),(370,370),255,-1)\\ncircle=cv.circle(blank.copy(), (200,200), 200,255,-1)\\n\\ncv.imshow(\"Rectangle\",rectangle)\\ncv.imshow(\"Circle\",circle)\\n\\n#bitwise AND\\nbitwise_and=cv.bitwise_and(rectangle,circle)\\ncv.imshow(\"Bitwise AND\",bitwise_and)\\n\\n#bitwise OR\\nbitwise_or=cv.bitwise_or(rectangle,circle)\\ncv.imshow(\"Bitwise OR\",bitwise_or)\\n\\n#bitwise XOR\\nbitwise_xor=cv.bitwise_xor(rectangle,circle)\\ncv.imshow(\"Bitwise XOR\",bitwise_xor)\\n\\n#bitwise NOT\\nbitwise_not=cv.bitwise_not(rectangle,circle)\\ncv.imshow(\"Bitwise NOT\",bitwise_not)\\n\\n\\ncv.waitKey(0)'),\n",
       " Document(metadata={'path': 'SECTION_2_ADVANCE/15masking.py', 'sha': '7f3be8c52e216d452ef1e303dc00bd70abe1d9bd', 'source': 'https://api.github.com/shafqatameen/opencv/blob/main/SECTION_2_ADVANCE/15masking.py'}, page_content='import cv2 as cv\\nimport numpy as np\\n\\n#load and read the image\\nimg=cv.imread(\"images/park.jpg\")\\ncv.imshow(\\'Park\\',img)\\n\\n#BLANK\\nblank=np.zeros(img.shape[:2],dtype=\"uint8\")\\ncv.imshow(\"Blank\",blank)\\nmask=cv.rectangle(blank.copy(),(0,0),(img.shape[1]//2,img.shape[0]//2),255,-1)\\ncv.imshow(\"mask\",mask)\\n\\nmasked=cv.bitwise_and(img,img,mask=mask)\\ncv.imshow(\"masked\",masked)\\ncv.waitKey(0)'),\n",
       " Document(metadata={'path': 'SECTION_2_ADVANCE/16histogram1_gray.py', 'sha': '83eb64b019bc85a206ccc1cb0313e62d1c671f5f', 'source': 'https://api.github.com/shafqatameen/opencv/blob/main/SECTION_2_ADVANCE/16histogram1_gray.py'}, page_content='import cv2 as cv\\nimport matplotlib.pyplot as plt\\nimport numpy as np\\n\\n#load and read the img\\nimg=cv.imread(\"images/park.jpg\")\\ncv.imshow(\\'Park\\',img)\\n\\n#conver the img into gray\\ngray=cv.cvtColor(img,cv.COLOR_BGR2GRAY)\\ncv.imshow(\\'gray\\',gray)\\n\\n\\n\\nblank=np.zeros(img.shape[:2],dtype=\"uint8\")\\nmask=cv.rectangle(blank.copy(),(0,0),(img.shape[1]//2,img.shape[0]//2),255,-1)\\nmasked=cv.bitwise_and(img,img,mask=mask)\\ncv.imshow(\"masked\",masked)\\n\\n#grayscale histogram\\ngray_hist=cv.calcHist([gray],[0],None,[256],[0,256])\\n#histogram of masked image\\nmasked_gray_hist=cv.calcHist([gray],[0],mask,[256],[0,256])\\n\\n\\n#plot the histogram\\nplt.figure()\\nplt.title(\"Grayscale Histogram\")\\nplt.xlabel(\"Bins\")\\nplt.ylabel(\"# of pixels\")\\nplt.plot(gray_hist)\\nplt.xlim([0,256])\\n\\n#plot the histogram for the masked image\\nplt.figure()\\nplt.title(\"Masked Grayscale Histogram\")\\nplt.xlabel(\"Bins\")\\nplt.ylabel(\"# of pixels\")\\nplt.plot(masked_gray_hist)\\nplt.xlim([0,256])\\nplt.show()\\n\\n\\n\\ncv.waitKey(0)'),\n",
       " Document(metadata={'path': 'SECTION_2_ADVANCE/16histogram2_color.py', 'sha': 'bff589bc14b5c7305d4ca867965dac874eca3845', 'source': 'https://api.github.com/shafqatameen/opencv/blob/main/SECTION_2_ADVANCE/16histogram2_color.py'}, page_content='import cv2 as cv\\nimport numpy as np\\nimport matplotlib.pyplot as plt\\n\\nimg=cv.imread(\\'images/cats.jpg\\')\\n\\n#masked image\\nblank=np.zeros(img.shape[:2],dtype=\"uint8\")\\nmask=cv.rectangle(blank.copy(),(0,0),(img.shape[1]//2,img.shape[0]//2),255,-1)\\nmasked=cv.bitwise_and(img,img,mask=mask)\\ncv.imshow(\"masked\",masked)\\n\\n#color histogram for color image\\ncolors=(\\'b\\',\\'g\\',\\'r\\')\\nfor i,col in enumerate(colors):\\n    hist_color=cv.calcHist([img],[i],None,[256],[0,256])\\n    plt.plot(hist_color,color=col)\\n    plt.xlim([0,256])    \\nplt.show()\\n\\n#color histogram for masked image\\ncolors=(\\'b\\',\\'g\\',\\'r\\')\\nfor i,col in enumerate(colors):\\n    hist_color=cv.calcHist([masked],[i],None,[256],[0,256])\\n    plt.plot(hist_color,color=col)\\n    plt.xlim([0,256])   \\nplt.show()\\n'),\n",
       " Document(metadata={'path': 'SECTION_2_ADVANCE/17threshold.py', 'sha': '07415a7925f2da2d5c363e2a8363ac54538419d6', 'source': 'https://api.github.com/shafqatameen/opencv/blob/main/SECTION_2_ADVANCE/17threshold.py'}, page_content=\"import cv2 as cv\\n\\nimg = cv.imread('images/park.jpg')\\ngray = cv.cvtColor(img, cv.COLOR_BGR2GRAY)\\n\\n# Simple threshold\\nthreshold , thresh = cv.threshold(gray, 150,255,cv.THRESH_BINARY)\\ncv.imshow('Simple Threshold', thresh)\\n\\n# Inverse threshold\\nthreshold,thresh= cv.threshold(gray,150,255,cv.THRESH_BINARY_INV)\\ncv.imshow('Inverse Threshold', thresh)\\n\\n\\n# Adaptive threshold\\nadp_thresh=cv.adaptiveThreshold(gray,255,cv.ADAPTIVE_THRESH_MEAN_C,cv.THRESH_BINARY,11,1)\\ncv.imshow('Adaptive Threshold', adp_thresh)\\n\\ncv.waitKey(0)\\n\"),\n",
       " Document(metadata={'path': 'SECTION_2_ADVANCE/18edge.py', 'sha': '5e07b2d67bec704bb710ab981b677df1763b2efb', 'source': 'https://api.github.com/shafqatameen/opencv/blob/main/SECTION_2_ADVANCE/18edge.py'}, page_content=\"import cv2 as cv\\nimport numpy as np\\n\\nimg=cv.imread('images/park.jpg')\\ncv.imshow('Park',img)\\ngray=cv.cvtColor(img,cv.COLOR_BGR2GRAY)\\ncv.imshow('Gray',gray)\\n\\n#laplacian edge detection\\nlap=cv.Laplacian(gray,cv.CV_64F)\\nabs_lap=np.absolute(lap)\\nlap_uint8=np.uint8(abs_lap)\\n#all_lap=np.hstack((lap_uint8,lap,abs_lap))#needs all in same data type\\ncv.imshow('Laplacian',lap_uint8)\\n\\n\\n#sobel edge detection\\nsobelx=cv.Sobel(gray,cv.CV_64F,1,0)\\nsobely=cv.Sobel(gray,cv.CV_64F,0,1)\\ncombined_sobel=cv.bitwise_and(sobelx,sobely)\\ncombined=np.hstack((combined_sobel,sobelx,sobely))\\ncv.imshow('Sobel',combined)\\n\\n#canny edge detection\\ncanny = cv.Canny(gray,125,175)\\ncv.imshow('Canny',canny)\\n\\ncv.waitKey(0)\"),\n",
       " Document(metadata={'path': 'SECTION_3_Faces/19haar_face_dection.py', 'sha': '3f79aeaa6ae1e29f684f895a8ef06124e47e3264', 'source': 'https://api.github.com/shafqatameen/opencv/blob/main/SECTION_3_Faces/19haar_face_dection.py'}, page_content='import cv2 as cv\\n#load and read the image\\nimg=cv.imread(\"images/lady.jpg\")\\ncv.imshow(\"lady\",img)\\n#conver into gray\\ngray_img=cv.cvtColor(img,cv.COLOR_BGR2GRAY)\\ncv.imshow(\"gray\",gray_img)\\n#haar cascade\\nhaar_cascade=cv.CascadeClassifier(\"SECTION_3_Faces/haar_face.xml\")\\n\\n#faces_rect=haar_cascade.detectMultiScale(gray_img,scaleFactor=1.1,minNeighbors=4)\\nfaces_rect=haar_cascade.detectMultiScale(gray_img,1.1,4)\\n#dectect the face\\nfor x,y,w,h in faces_rect:\\n    cv.rectangle(img,(x,y),(x+w,y+h),(0,255,0),3)\\ncv.imshow(\"face\",img)\\nprint(f\\'number of faces found={len(faces_rect)}\\')\\ncv.waitKey(0)'),\n",
       " Document(metadata={'path': 'SECTION_3_Faces/19haar_face_dection2.py', 'sha': '34efecdd740379bc15df72ad5d3281938a621fd3', 'source': 'https://api.github.com/shafqatameen/opencv/blob/main/SECTION_3_Faces/19haar_face_dection2.py'}, page_content='import cv2 as cv\\n#img_path=\\'\\'\\'C:\\\\xampp\\\\htdocs\\\\programming\\\\opencv\\\\images\\\\group2.jpg\\'\\'\\'\\n\\n#load and read the image\\nimg=cv.imread(\"images/group1.jpg\")\\n\\n# Check if image is loaded\\ncv.imshow(\"group2\",img)\\n\\n#conver into gray\\ngray_img=cv.cvtColor(img,cv.COLOR_BGR2GRAY)\\ncv.imshow(\"gray\",gray_img)\\n\\n#haar cascade\\nhaar_cascade=cv.CascadeClassifier(\"SECTION_3_Faces/haar_face.xml\")\\n\\n#faces_rect=haar_cascade.detectMultiScale(gray_img,scaleFactor=1.1,minNeighbors=4)\\nfaces_rect=haar_cascade.detectMultiScale(gray_img,1.1,1)\\n\\n#dectect the face\\nfor x,y,w,h in faces_rect:\\n    cv.rectangle(img,(x,y),(x+w,y+h),(0,255,0),3)\\ncv.imshow(\"face\",img)\\nprint(f\\'number of faces found={len(faces_rect)}\\')\\ncv.waitKey(0)'),\n",
       " Document(metadata={'path': 'SECTION_3_Faces/20face_dection.py', 'sha': '6e0475cd12fb36ff3127f5ff972644ee2d296e7e', 'source': 'https://api.github.com/shafqatameen/opencv/blob/main/SECTION_3_Faces/20face_dection.py'}, page_content='# pylint:disable=no-member\\nimport cv2 as cv\\nimport numpy as np\\nfrom pathlib import Path\\n\\npeople = [\\'Ben Afflek\\', \\'Elton John\\', \\'Jerry Seinfield\\', \\'Madonna\\', \\'Mindy Kaling\\']\\nDIR = Path(\"C:/xampp/htdocs/programming/opencv/SECTION_3_Faces/train\")\\nhaar_cascade = cv.CascadeClassifier(str(Path(\"SECTION_3_Faces/haar_face.xml\")))\\n\\nfeatures = []\\nlabels = []\\n\\ndef create_train():\\n    for person in people:\\n        path = DIR / person\\n        label = people.index(person)\\n\\n        for img_path in path.iterdir():\\n            img_array = cv.imread(str(img_path))\\n            \\n            if img_array is None:\\n                continue  # Skip unreadable images\\n\\n            gray = cv.cvtColor(img_array, cv.COLOR_BGR2GRAY)\\n            faces_rect = haar_cascade.detectMultiScale(gray, scaleFactor=1.1, minNeighbors=4)\\n\\n            for (x, y, w, h) in faces_rect:\\n                face_roi = gray[y:y+h, x:x+w]\\n                print(f\"face_roi: {face_roi.shape}\")\\n                features.append(face_roi)\\n                print(f\"features: {len(features)}\")\\n                labels.append(label)\\n                print(f\\'Label: {label}, {img_path}\\')\\n                \\n\\ncreate_train()\\n\\nprint(f\"Training complete ✅\\\\nTotal faces: {len(features)}\\\\nTotal labels: {len(labels)}\")\\n\\nfeatures = np.array(features, dtype=\\'object\\')\\nlabels = np.array(labels)\\n\\nface_recognizer = cv.face.LBPHFaceRecognizer_create()\\nface_recognizer.train(features, labels)\\nface_recognizer.save(\"SECTION_3_Faces/face_trained.yml\")\\nnp.save(\"SECTION_3_Faces/features.npy\", features)\\nnp.save(\"SECTION_3_Faces/labels.npy\", labels)\\n\\n\\n\\n\\n\\'\\'\\'import os \\nimport cv2 as cv\\nimport numpy as np\\n\\npeople=[\\'Ben Afflek\\', \\'Elton John\\', \\'Jerry Seinfield\\', \\'Madonna\\', \\'Mindy Kaling\\']\\nDIC=r\"C:/xampp/htdocs/programming/opencv\\\\SECTION_3_Faces/train\"\\np=[]\\nfor i  in os.listdir(r\"C:/xampp/htdocs/programming/opencv\\\\SECTION_3_Faces/train\"):\\n    p.append(i)\\n\\nprint(p)\\n\\n\\nfeatures=[]\\nlabels=[]\\nhaar_cascade=cv.CascadeClassifier(\"SECTION_3_Faces/haar_face.xml\")\\n\\ndef create_train():\\n    for person in people:\\n        path=os.path.join(DIC,person)\\n        label=people.index(person)  \\n        for img in os.listdir(path):\\n            img_path=os.path.join(path,img)\\n            img_array=cv.imread(img_path)\\n            gray=cv.cvtColor(img_array,cv.COLOR_BGR2GRAY)\\n            faces_rect=haar_cascade.detectMultiScale(gray,1.1,1)\\n            for (x,y,w,h) in faces_rect:\\n                faces_roi=gray[y:y+h,x:x+w]\\n                features.append(faces_roi)\\n                labels.append(label)\\n\\ncreate_train()\\n\\nprint(f\\'length of features={len(features)}\\')\\nprint(f\\'length of labels={len(labels)}\\')\\n\\nfeatures=np.array(features,dtype=\"object\")\\nlabels=np.array(labels)\\n\\nface_recognizer=cv.face.LBPHFaceRecognizer_create()\\nface_recognizer.train(features,labels)\\n\\nface_recognizer.write(\\'train.yml\\')\\n\\nprint(\"training done\")\\n#save labels and features\\nnp.save(\\'features.npy\\',features)\\nnp.save(\\'labels.npy\\',labels)\\n\\n\\'\\'\\''),\n",
       " Document(metadata={'path': 'SECTION_3_Faces/21face_recognization.py', 'sha': 'd8f9756c81caf122c04ecb9e8388a96a4ae6348a', 'source': 'https://api.github.com/shafqatameen/opencv/blob/main/SECTION_3_Faces/21face_recognization.py'}, page_content=\"\\nimport numpy as np\\nimport cv2 as cv\\n\\nhaar_cascade = cv.CascadeClassifier('SECTION_3_Faces\\\\haar_face.xml')\\n\\npeople = ['Ben Afflek', 'Elton John', 'Jerry Seinfield', 'Madonna', 'Mindy Kaling']\\n# features = np.load('features.npy', allow_pickle=True)\\n# labels = np.load('labels.npy')\\n\\nface_recognizer = cv.face.LBPHFaceRecognizer_create()\\nface_recognizer.read('SECTION_3_Faces/face_trained.yml')\\n\\nimg = cv.imread(r'SECTION_3_Faces\\\\val\\\\mindy_kaling\\\\5.jpg')\\n\\ngray = cv.cvtColor(img, cv.COLOR_BGR2GRAY)\\ncv.imshow('Person', gray)\\n\\n# Detect the face in the image\\nfaces_rect = haar_cascade.detectMultiScale(gray, 1.1, 4)\\n\\nfor (x,y,w,h) in faces_rect:\\n    faces_roi = gray[y:y+h,x:x+w]\\n\\n    label, confidence = face_recognizer.predict(faces_roi)\\n    print(f'Label = {people[label]} with a confidence of {confidence}')\\n\\n    cv.putText(img, str(people[label]), (20,20), cv.FONT_HERSHEY_COMPLEX, 1.0, (0,255,0), thickness=2)\\n    cv.rectangle(img, (x,y), (x+w,y+h), (0,255,0), thickness=2)\\n\\ncv.imshow('Detected Face', img)\\n\\ncv.waitKey(0)\"),\n",
       " Document(metadata={'path': 'SECTION_3_Faces/21face_recognization2_multi img.py', 'sha': '2c21c8cea174d5c600ec3f6717ec5710b344f977', 'source': 'https://api.github.com/shafqatameen/opencv/blob/main/SECTION_3_Faces/21face_recognization2_multi img.py'}, page_content='from pathlib import Path\\nimport numpy as np\\nimport cv2 as cv\\n\\n# Load Haar cascade\\nhaar_cascade = cv.CascadeClassifier(str(Path(\\'SECTION_3_Faces/haar_face.xml\\')))\\n\\n# People names based on your training labels (same order!)\\npeople = [\\'Ben Afflek\\', \\'Elton John\\', \\'Jerry Seinfield\\', \\'Madonna\\', \\'Mindy Kaling\\']\\n\\n# Load trained face recognizer\\nface_recognizer = cv.face.LBPHFaceRecognizer_create()\\nface_recognizer.read(str(Path(\\'SECTION_3_Faces/face_trained.yml\\')))\\n\\n# Set the path to validation images\\nval_dir = Path(\\'SECTION_3_Faces/val\\')\\n\\n# Traverse all JPG images in all subfolders of val_dir\\nfor img_path in val_dir.glob(\\'*/*.jpg\\'):\\n    img = cv.imread(str(img_path))\\n    if img is None:\\n        print(f\"[!] Could not load image: {img_path}\")\\n        continue\\n\\n    gray = cv.cvtColor(img, cv.COLOR_BGR2GRAY)\\n    faces_rect = haar_cascade.detectMultiScale(gray, 1.1, 4)\\n\\n    print(f\"\\\\n📷 Image: {img_path.name}\")\\n    for (x, y, w, h) in faces_rect:\\n        faces_roi = gray[y:y + h, x:x + w]\\n\\n        label, confidence = face_recognizer.predict(faces_roi)\\n        print(f\\'    → Predicted: {people[label]} | Confidence: {round(confidence, 2)}\\')\\n\\n        cv.putText(img, people[label], (x, y - 10), cv.FONT_HERSHEY_COMPLEX, 0.9, (0, 255, 0), 2)\\n        cv.rectangle(img, (x, y), (x + w, y + h), (0, 255, 0), 2)\\n\\n    cv.imshow(\\'Detected Face\\', img)\\n    cv.waitKey(500)  # Show each image for 500ms\\n    cv.destroyAllWindows()'),\n",
       " Document(metadata={'path': 'SECTION_4_deep_CV/0demo.py', 'sha': 'c8c247ad3f77d97a505afa524a695c143dc3289d', 'source': 'https://api.github.com/shafqatameen/opencv/blob/main/SECTION_4_deep_CV/0demo.py'}, page_content='#pylint:disable=no-member (Removes linting problems with cv)\\n\\n# Installing `caer` and `canaro` since they don\\'t come pre-installed\\n# Uncomment the following line:\\n# !pip install --upgrade caer canaro\\n\\nimport os\\nimport caer\\nimport canaro\\nimport numpy as np\\nimport cv2 as cv\\nimport gc\\nimport matplotlib.pyplot as plt\\nfrom tensorflow.keras.utils import to_categorical\\nfrom tensorflow.keras.callbacks import LearningRateScheduler\\nfrom tensorflow.keras.models import Sequential\\nfrom tensorflow.keras.layers import Dense, Flatten, Dropout, Conv2D, MaxPooling2D\\nfrom tensorflow.keras.optimizers.legacy import SGD\\n\\nIMG_SIZE = (80,80)\\nchannels = 1\\nchar_path = r\\'SECTION_4_deep_CV\\\\archive\\\\simpsons_dataset\\'\\n\\n# Creating a character dictionary, sorting it in descending order\\nchar_dict = {}\\nfor char in os.listdir(char_path):\\n    char_dict[char] = len(os.listdir(os.path.join(char_path,char)))\\n\\n# Sort in descending order\\nchar_dict = caer.sort_dict(char_dict, descending=True)\\nchar_dict\\n\\n#  Getting the first 10 categories with the most number of images\\ncharacters = []\\ncount = 0\\nfor i in char_dict:\\n    characters.append(i[0])\\n    count += 1\\n    if count >= 10:\\n        break\\ncharacters\\n\\n# Create the training data\\ntrain = caer.preprocess_from_dir(char_path, characters, channels=channels, IMG_SIZE=IMG_SIZE, isShuffle=True)\\n\\n# Number of training samples\\nlen(train)\\n\\n# Visualizing the data (OpenCV doesn\\'t display well in Jupyter notebooks)\\nplt.figure(figsize=(30,30))\\nplt.imshow(train[0][0], cmap=\\'gray\\')\\nplt.show()\\n\\n# Separating the array and corresponding labels\\nfeatureSet, labels = caer.sep_train(train, IMG_SIZE=IMG_SIZE)\\n\\n\\n# Normalize the featureSet ==> (0,1)\\nfeatureSet = caer.normalize(featureSet)\\n# Converting numerical labels to binary class vectors\\nlabels = to_categorical(labels, len(characters))\\n\\n\\n# Creating train and validation data\\nx_train, x_val, y_train, y_val = caer.train_val_split(featureSet, labels, val_ratio=.2)\\n\\n# Deleting variables to save memory\\ndel train\\ndel featureSet\\ndel labels \\ngc.collect()\\n\\n# Useful variables when training\\nBATCH_SIZE = 32\\nEPOCHS = 10\\n\\n# Image data generator (introduces randomness in network ==> better accuracy)\\ndatagen = canaro.generators.imageDataGenerator()\\ntrain_gen = datagen.flow(x_train, y_train, batch_size=BATCH_SIZE)\\n\\n# Create our model (returns the compiled model)\\noutput_dim=10\\n\\nw, h = IMG_SIZE[:2]\\n\\nmodel = Sequential()\\nmodel.add(Conv2D(32, (3, 3), activation=\\'relu\\', padding=\\'same\\', input_shape=(w, h,channels)))\\nmodel.add(Conv2D(32, (3, 3), activation=\\'relu\\'))\\nmodel.add(MaxPooling2D(pool_size=(2, 2)))\\nmodel.add(Dropout(0.2))\\n\\nmodel.add(Conv2D(64, (3, 3), padding=\\'same\\', activation=\\'relu\\'))\\nmodel.add(Conv2D(64, (3, 3), activation=\\'relu\\'))\\nmodel.add(MaxPooling2D(pool_size=(2, 2)))\\nmodel.add(Dropout(0.2))\\n\\nmodel.add(Conv2D(256, (3, 3), padding=\\'same\\', activation=\\'relu\\')) \\nmodel.add(Conv2D(256, (3, 3), activation=\\'relu\\'))\\nmodel.add(MaxPooling2D(pool_size=(2, 2)))\\nmodel.add(Dropout(0.2))\\n\\nmodel.add(Flatten())\\nmodel.add(Dropout(0.5))\\nmodel.add(Dense(1024, activation=\\'relu\\'))\\n\\n# Output Layer\\nmodel.add(Dense(output_dim, activation=\\'softmax\\'))\\n\\nmodel.summary()\\n\\n# Training the model\\noptimizer = SGD(learning_rate=0.001, decay=1e-7, momentum=0.9, nesterov=True)\\nmodel.compile(loss=\\'binary_crossentropy\\', optimizer=optimizer, metrics=[\\'accuracy\\'])\\ncallbacks_list = [LearningRateScheduler(canaro.lr_schedule)]\\ntraining = model.fit(train_gen,\\n                    steps_per_epoch=len(x_train)//BATCH_SIZE,\\n                    epochs=EPOCHS,\\n                    validation_data=(x_val,y_val),\\n                    validation_steps=len(y_val)//BATCH_SIZE,\\n                    callbacks = callbacks_list)\\n\\nprint(characters)\\n\\n\\n\"\"\"## Testing\"\"\"\\n\\ntest_path = r\\'SECTION_4_deep_CV\\\\archive\\\\kaggle_simpson_testset/charles_montgomery_burns_0.jpg\\'\\n\\nimg = cv.imread(test_path)\\n\\nplt.imshow(img)\\nplt.show()\\n\\ndef prepare(image):\\n    image = cv.cvtColor(image, cv.COLOR_BGR2GRAY)\\n    image = cv.resize(image, IMG_SIZE)\\n    image = caer.reshape(image, IMG_SIZE, 1)\\n    return image\\n\\npredictions = model.predict(prepare(img))\\n\\n# Getting class with the highest probability\\nprint(characters[np.argmax(predictions[0])])'),\n",
       " Document(metadata={'path': 'lecture/cropping_tool.py', 'sha': '3a4116afec7202172cf195800a91abe4d2802d86', 'source': 'https://api.github.com/shafqatameen/opencv/blob/main/lecture/cropping_tool.py'}, page_content='import cv2\\nimport numpy as np\\n\\n# Global variables\\ncropping = False  # Flag to indicate cropping state\\nstart_x, start_y, end_x, end_y = -1, -1, -1, -1\\nimage = cv2.imread(\"img/fruits.jpg\")  # Load your image here\\nclone = image.copy()  # Keep a copy of the original image\\n\\n# Mouse callback function\\ndef crop_rectangle(event, x, y, flags, param):\\n    global start_x, start_y, end_x, end_y, cropping, image\\n    \\n    if event == cv2.EVENT_LBUTTONDOWN:  # Mouse button pressed\\n        start_x, start_y = x, y\\n        cropping = True\\n\\n    elif event == cv2.EVENT_MOUSEMOVE:  # Mouse movement\\n        if cropping:\\n            temp_image = clone.copy()  # Use a copy to avoid overwriting\\n            cv2.rectangle(temp_image, (start_x, start_y), (x, y), (0, 255, 0), 2)\\n            cv2.imshow(\"Image\", temp_image)\\n\\n    elif event == cv2.EVENT_LBUTTONUP:  # Mouse button released\\n        end_x, end_y = x, y\\n        cropping = False\\n        cv2.rectangle(image, (start_x, start_y), (end_x, end_y), (0, 255, 0), 2)\\n        cv2.imshow(\"Image\", image)\\n\\n        # Ensure coordinates are correct\\n        if start_x != end_x and start_y != end_y:\\n            cropped_image = clone[start_y:end_y, start_x:end_x]  # Extract ROI\\n            cv2.imshow(\"Cropped Image\", cropped_image)\\n            cv2.imwrite(\"cropped_output.jpg\", cropped_image)  # Save cropped image\\n\\n# Set up the OpenCV window\\ncv2.imshow(\"Image\", image)\\ncv2.setMouseCallback(\"Image\", crop_rectangle)\\n\\n# Keep the window open until \\'q\\' is pressed\\nwhile True:\\n    key = cv2.waitKey(1) & 0xFF\\n    if key == ord(\"q\"):\\n        break\\n\\ncv2.destroyAllWindows()\\n'),\n",
       " Document(metadata={'path': 'lecture/event.py', 'sha': '449315dde51a54bbaaa03f9e007be2936d2a20fd', 'source': 'https://api.github.com/shafqatameen/opencv/blob/main/lecture/event.py'}, page_content='import cv2\\nimport numpy as np\\n\\n# Initialize variables\\ndrawing = False  # True if mouse is pressed\\nstart_x, start_y = -1, -1  # Starting coordinates\\nimage = np.zeros((500, 500, 3), dtype=np.uint8)  # Black canvas\\n\\n# Mouse callback function\\ndef draw_rectangle(event, x, y, flags, param):\\n    global start_x, start_y, drawing, image\\n    \\n    if event == cv2.EVENT_LBUTTONDOWN:  # Mouse press event\\n        drawing = True\\n        start_x, start_y = x, y  # Store starting coordinates\\n    \\n    elif event == cv2.EVENT_MOUSEMOVE:  # Mouse movement event\\n        if drawing:  # Only draw if the left button is pressed\\n            temp_img = image.copy()  # Copy image to avoid overwriting\\n            cv2.rectangle(temp_img, (start_x, start_y), (x, y), (0, 255, 0), -1)\\n            cv2.imshow(\"Interactive Drawing\", temp_img)\\n    \\n    elif event == cv2.EVENT_LBUTTONUP:  # Mouse release event\\n        drawing = False\\n        cv2.rectangle(image, (start_x, start_y), (x, y), (0, 255, 0), -1)\\n        cv2.imshow(\"Interactive Drawing\", image)\\n\\n# Create OpenCV window and set callback\\ncv2.imshow(\"Interactive Drawing\", image)\\ncv2.setMouseCallback(\"Interactive Drawing\", draw_rectangle)\\n\\n# Keep window open until user presses \\'q\\'\\nwhile True:\\n    if cv2.waitKey(1) & 0xFF == ord(\\'q\\'):\\n        break\\n\\ncv2.destroyAllWindows()\\n\\n\\n\\'\\'\\'import cv2\\nimport numpy as np\\n\\nimg=np.zeros( (500,500,3) )\\n\\nflag=False\\nix=-1\\niy=-1\\ndef draw(event,x,y,flags,params):\\n    global ix,iy,flag\\n    if event==1:\\n        flag=True\\n        ix=x\\n        iy=y\\n    elif event==0:\\n        if flag==True:\\n            cv2.rectangle(img,(ix,iy),(x,y),(255,0,0),-1)\\n        \\n\\n    elif event==4:\\n            flag=False\\n            cv2.rectangle(img,(ix,iy),(x,y),(0,255,0),-1)\\n            \\n\\ncv2.namedWindow(winname=\"window\")\\ncv2.setMouseCallback(\\'window\\',draw)\\n\\nwhile True:\\n    cv2.imshow(\\'window\\',img)\\n    if (cv2.waitKey(1) & 0xFF)==ord(\\'x\\'):\\n        break\\n\\ncv2.destroyAllWindows()\\'\\'\\''),\n",
       " Document(metadata={'path': 'lecture/interractWithImage.py', 'sha': '2ceae0c2894c468c0cd01201a3f44a22c012d745', 'source': 'https://api.github.com/shafqatameen/opencv/blob/main/lecture/interractWithImage.py'}, page_content='import cv2\\nimport numpy as np\\n\\nimg=cv2.imread(\\'img/fruits.jpg\\')\\n#img=np.zeros((500,500,3))\\n\\ndef draw(event,x,y,flags,params):\\n    if event==1:\\n        cv2.circle(img,center=(x,y),radius=10,color=(255,0,0),thickness=-1)\\n\\'\\'\\'    print(event)\\n    if event==0:\\n        print(\\'mouse moved\\')\\n    if event==1:\\n        print(\\'left button clicked\\')\\n    if event==2:\\n        print(\\'right button clicked\\')\\n    if event==3:\\n        print(\\'middle button clicked\\')\\n    if event==4:\\n        print(\\'left button clicked released\\')\\n    if event==5:\\n        print(\\'right button clicked released\\')\\n    if event==6:\\n        print(\\'middle button clicked released\\')\\n    \\'\\'\\'\\n\\n\\ncv2.namedWindow(winname=\"window\")\\ncv2.setMouseCallback(\\'window\\',draw)\\n\\nwhile True:\\n    cv2.imshow(\\'window\\',img)\\n    if (cv2.waitKey(1) & 0xFF)==ord(\\'x\\'):\\n        break\\n\\ncv2.destroyAllWindows()\\n\\n\\n\\n\\n'),\n",
       " Document(metadata={'path': 'lecture/readimg.py', 'sha': '025e32241f0e1a61a1d53e87a37a4fa83ebdfaed', 'source': 'https://api.github.com/shafqatameen/opencv/blob/main/lecture/readimg.py'}, page_content=\"import cv2\\n\\nimg=cv2.imread('img/fruits.jpg')\\nprint(img.shape)\\ncv2.imshow('original image called color image',img)\\n##cv2.waitKey(4)\\n\\nimg_gray=cv2.cvtColor(img,cv2.COLOR_BGR2GRAY)\\nprint(img_gray.shape)\\ncv2.imshow('gray image called grayscale image',img_gray)\\n#cv2.waitKey(10)\\n\\ncv2.imshow('image without blue color',img[:,:,0])\\n#cv2.waitKey(100000)\\n\\ncv2.imshow('image without green color',img[:,:,1])\\n#cv2.waitKey(100000)\\n\\ncv2.imshow('image without red color',img[:,:,2])\\n#cv2.waitKey(100000)\\n\\n\\n\"),\n",
       " Document(metadata={'path': 'lecture/removeOneChannel.py', 'sha': '69a3e18cb281d489c0033961e4f642d8121e16ab', 'source': 'https://api.github.com/shafqatameen/opencv/blob/main/lecture/removeOneChannel.py'}, page_content=\"import cv2\\n#load iimage\\nimg=cv2.imread('img/fruits.jpg')\\nprint(img.shape)\\n\\n'''#show image\\ncv2.imshow('COLOR IMAGE',img)\\ncv2.waitKey(0)'''\\n\\n#remove the channel\\nimg[:,:,0]=0                 #remove blue channel\\ncv2.imshow('IMAGE WITHOUT BLUE CHANNEL',img)\\ncv2.waitKey(0)\\n\\nimg[:,:,1]=0\\ncv2.imshow('IMAGE WITHOUT GREEN CHANNEL',img)\\ncv2.waitKey(0)\\n\\nimg[:,:,2]=0\\ncv2.imshow('IMAGE WITHOUT RED CHANNEL',img)\\ncv2.waitKey(0)\\n\\n\"),\n",
       " Document(metadata={'path': 'lecture/removeonechannel2.py', 'sha': '0c547bcad79176dd996e49dfb793dda5d7aa88c6', 'source': 'https://api.github.com/shafqatameen/opencv/blob/main/lecture/removeonechannel2.py'}, page_content='import cv2\\nimport numpy as np\\n\\n# Load the image\\nimg = cv2.imread(\\'img/fruits.jpg\\')\\n\\n# Check if the image is loaded correctly\\nif img is None:\\n    print(\"Error: Image not found or path is incorrect.\")\\n    exit()\\n\\n# Resize the image to a manageable size (e.g., 600x400)\\nscale_percent = 50  # Reduce size by 50%\\nwidth = int(img.shape[1] * scale_percent / 100)\\nheight = int(img.shape[0] * scale_percent / 100)\\ndim = (width, height)\\nimg = cv2.resize(img, dim, interpolation=cv2.INTER_AREA)\\n\\n# Remove blue channel\\nimg_without_blue = img.copy()\\nimg_without_blue[:, :, 0] = 0\\n\\n# Remove green channel\\nimg_without_green = img.copy()\\nimg_without_green[:, :, 1] = 0\\n\\n# Remove red channel\\nimg_without_red = img.copy()\\nimg_without_red[:, :, 2] = 0\\n\\n# Stack images horizontally\\nnew_img = np.hstack((img_without_blue, img_without_green, img_without_red))\\n\\n# Show the final stacked image\\ncv2.imshow(\"Single Channel Removed\", new_img)\\ncv2.waitKey(0)\\ncv2.destroyAllWindows()\\n\\n\\n\\n\\n\\n\\'\\'\\'import cv2\\nimport numpy as np\\n\\n\\n# Load the image\\nimg = cv2.imread(\\'img/fruits.jpg\\')\\n\\n# Remove blue channel\\nimg_without_blue = img.copy()\\nimg_without_blue[:, :, 0] = 0\\ncv2.imshow(\\'IMAGE WITHOUT BLUE CHANNEL\\', img_without_blue)\\ncv2.waitKey(0)\\ncv2.destroyAllWindows()\\n\\n# Remove green channel\\nimg_without_green = img.copy()\\nimg_without_green[:, :, 1] = 0\\ncv2.imshow(\\'IMAGE WITHOUT GREEN CHANNEL\\', img_without_green)\\ncv2.waitKey(0)\\ncv2.destroyAllWindows()\\n\\n# Remove red channel\\nimg_without_red = img.copy()\\nimg_without_red[:, :, 2] = 0\\ncv2.imshow(\\'IMAGE WITHOUT RED CHANNEL\\', img_without_red)\\ncv2.waitKey(0)\\ncv2.destroyAllWindows()\\n\\nnew_img=np.hstack((img_without_blue,img_without_green,img_without_red))\\ncv2.imshow(\"single channel removed\",new_img)\\ncv2.waitKey(0)\\'\\'\\''),\n",
       " Document(metadata={'path': 'lecture/video.py', 'sha': '81cd1d5b2e230969760d53b84b6fd8132ee2bab2', 'source': 'https://api.github.com/shafqatameen/opencv/blob/main/lecture/video.py'}, page_content='import cv2\\n\\n# Open the video file (change \\'video.mp4\\' to 0 for webcam)\\nvideo_path = \"output_video.mp4\"  # Change this to your video file or use 0 for a webcam\\ncap = cv2.VideoCapture(video_path)\\n\\n# Check if the video file opened correctly\\nif not cap.isOpened():\\n    print(\"Error: Could not open video.\")\\n    exit()\\n\\n# Get video properties\\nframe_width = int(cap.get(3))  # Width of frames\\nframe_height = int(cap.get(4))  # Height of frames\\nfps = int(cap.get(cv2.CAP_PROP_FPS))  # Frames per second\\n\\n# Define the codec and create VideoWriter object\\noutput_path = \"output_video.mp4\"\\nfourcc = cv2.VideoWriter_fourcc(*\\'mp4v\\')  # Codec for MP4 format\\nout = cv2.VideoWriter(output_path, fourcc, fps, (frame_width, frame_height), isColor=False)\\n\\nwhile True:\\n    ret, frame = cap.read()  # Read a frame\\n    if not ret:\\n        break  # Exit loop if no frame is returned\\n\\n    # Convert frame to grayscale\\n    gray_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\\n\\n    # Show the frame\\n    cv2.imshow(\"Grayscale Video\", gray_frame)\\n\\n    # Write the processed frame to output file\\n    out.write(gray_frame)\\n\\n    # Handle playback controls\\n    key = cv2.waitKey(250) & 0xFF\\n    if key == ord(\\'q\\'):  # Press \\'q\\' to quit\\n        break\\n    elif key == ord(\\'p\\'):  # Press \\'p\\' to pause\\n        cv2.waitKey(-1)  # Wait until any key is pressed to resume\\n\\n# Release everything\\ncap.release()\\nout.release()\\ncv2.destroyAllWindows()\\n'),\n",
       " Document(metadata={'path': 'lecture/video2.py', 'sha': '146b57833e3f37a8395a6ed6a3aad3f294112ebd', 'source': 'https://api.github.com/shafqatameen/opencv/blob/main/lecture/video2.py'}, page_content='import cv2\\n\\n# Open webcam (0 for default camera)\\ncap = cv2.VideoCapture(0)\\n\\n# Get video properties\\nframe_width = int(cap.get(3))  # Width\\nframe_height = int(cap.get(4))  # Height\\nfps = 30  # Frame rate (adjust as needed)\\n\\n# Define codec and create VideoWriter object\\noutput_path = \"captured_video.mp4\"\\nfourcc = cv2.VideoWriter_fourcc(*\\'mp4v\\')  # Codec for MP4 format\\nout = cv2.VideoWriter(output_path, fourcc, fps, (frame_width, frame_height))\\n\\n# Check if webcam is opened\\nif not cap.isOpened():\\n    print(\"Error: Could not open webcam.\")\\n    exit()\\n\\nprint(\"Press \\'q\\' to stop recording...\")\\n\\nwhile True:\\n    ret, frame = cap.read()  # Read a frame from the webcam\\n    if not ret:\\n        break  # Exit loop if no frame is captured\\n\\n    # Display the video feed\\n    cv2.imshow(\"Webcam Live Feed\", frame)\\n\\n    # Save the frame to the output file\\n    out.write(frame)\\n\\n    # Press \\'q\\' to stop recording\\n    if cv2.waitKey(1) & 0xFF == ord(\\'q\\'):\\n        break\\n\\n# Release the camera and close all windows\\ncap.release()\\nout.release()\\ncv2.destroyAllWindows()\\nprint(\"Video saved as \\'captured_video.mp4\\'\")\\n')]"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "genai_evn",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
