{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "c9a7e59f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'url': 'https://github.com/shafqatameen/opencv/issues/3', 'title': 'its an annother issue', 'creator': 'shafqatameen', 'created_at': '2025-12-25T11:20:20Z', 'comments': 0, 'state': 'open', 'labels': [], 'assignee': None, 'milestone': None, 'locked': False, 'number': 3, 'is_pull_request': False}, page_content='i am adding the another issue so u can  use the document_loaders and find how i can extract these'),\n",
       " Document(metadata={'url': 'https://github.com/shafqatameen/opencv/issues/2', 'title': 'its my 2nd issue  i am going to solve', 'creator': 'shafqatameen', 'created_at': '2025-12-25T11:07:15Z', 'comments': 0, 'state': 'open', 'labels': [], 'assignee': None, 'milestone': None, 'locked': False, 'number': 2, 'is_pull_request': False}, page_content='ok i think its a 2nd issue i am putting on the test ')]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_community.document_loaders import GitHubIssuesLoader\n",
    "loader=GitHubIssuesLoader(\n",
    "    repo=\"shafqatameen/opencv\",\n",
    "    only_load_issues=False,\n",
    "\n",
    ")\n",
    "documents=loader.load()\n",
    "documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "76facc43",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'path': 'README.md', 'sha': 'd25fc863cd46a04e5abb8adf9b2e2d2ff82d59a7', 'source': 'https://api.github.com/shafqatameen/opencv/blob/main/README.md'}, page_content='hello its my first time doing the text on it \\n')]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#load the readme file from the github repo\n",
    "from langchain_community.document_loaders import GithubFileLoader\n",
    "loader=GithubFileLoader(\n",
    "    repo=\"shafqatameen/opencv\",\n",
    "    file_filter = lambda file_path: file_path.endswith(\".md\")\n",
    "    \n",
    ")\n",
    "readme_doc=loader.load()\n",
    "readme_doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "e7e51607",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'path': '0demo.py', 'sha': 'cec0601486bcbe8f86c2f7b6198702b51f533646', 'source': 'https://api.github.com/shafqatameen/opencv/blob/main/0demo.py'}, page_content='import cv2\\n\\n# Load pre-trained Haarcascade model\\nface_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + \"haarcascade_fullbody.xml\")\\n\\n# Read image\\nimg = cv2.imread(\"images/park.jpg\")\\ngray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\\n\\n# Detect faces\\nfaces = face_cascade.detectMultiScale(gray, scaleFactor=1.1, minNeighbors=5, minSize=(30, 30))\\n\\n# Draw rectangles around faces\\nfor (x, y, w, h) in faces:\\n    cv2.rectangle(img, (x, y), (x + w, y + h), (255, 0, 0), 2)\\n\\n# Show output\\ncv2.imshow(\"Detected Faces\", img)\\ncv2.waitKey(0)\\ncv2.destroyAllWindows()\\n'),\n",
       " Document(metadata={'path': 'SECTION_1_BASICS/0demo.py', 'sha': '751a6f201ba693717e30e6dd7a0985079c69c0d2', 'source': 'https://api.github.com/shafqatameen/opencv/blob/main/SECTION_1_BASICS/0demo.py'}, page_content='import cv2 as cv\\n\\n#resiszng the frame of live video, its working for all videos,live and images\\ndef resizedFrame(frame,scale=75):\\n    width=int(frame.shape[1]*scale/100)\\n    height=int(frame.shape[0]*scale/100)\\n    dim=(width,height)\\n    return cv.resize(frame,dim,interpolation=cv.INTER_AREA)\\n\\'\\'\\'def changeRes(width,height):\\n    #only for live video another method for live video\\n    capture.set(3,width)\\n    capture.set(4,height)\\'\\'\\'\\n# Open the default camera (0), or use 1, 2 for external cameras\\ncapture = cv.VideoCapture(0)\\n\\n\\n# Check if the camera opened successfully\\nif not capture.isOpened():\\n    print(\"Error: Could not open camera.\")\\n    exit()\\n\\nwhile True:\\n    isTrue, frame = capture.read()\\n    if not isTrue:  # Break if frame is not read (camera error)\\n        print(\"Error reading frame.\")\\n        break\\n    #change image to other type\\n    converted_color=cv.cvtColor(frame,cv.COLOR_BGR2YUV)\\n    # Resize the frame\\n    resized_frame=resizedFrame(converted_color,scale=110)\\n    cv.imshow(\\'Webcam\\', resized_frame)\\n\\n    # Press \\'d\\' to exit\\n    if cv.waitKey(20) & 0xFF == ord(\\'d\\'):\\n        break\\n\\n# Release resources\\ncapture.release()\\ncv.destroyAllWindows()\\n'),\n",
       " Document(metadata={'path': 'SECTION_1_BASICS/10contours.py', 'sha': '3c608c3d69a2601a85e4c9e0c64f2faf4f18038c', 'source': 'https://api.github.com/shafqatameen/opencv/blob/main/SECTION_1_BASICS/10contours.py'}, page_content=\"import cv2 as cv\\nimport numpy as np\\n\\n# Read the image\\nimg=cv.imread('images/cats.jpg')\\ncv.imshow('cats',img)\\n#create blank image\\nblank=np.zeros(img.shape, dtype='uint8')\\ncv.imshow('Blank',blank)\\n\\n# Convert to gray scale\\ngray=cv.cvtColor(img,cv.COLOR_BGR2GRAY)\\ncv.imshow('gray',gray)\\n\\n# Blur the image\\nblur=cv.GaussianBlur(gray,(5,5),cv.BORDER_DEFAULT)\\ncv.imshow('blur',blur)\\n\\n# Canny edge detection\\ncanny =cv.Canny(blur,125,175)\\ncv.imshow('canny',canny)\\n\\n# Thresholding\\nret,thresh = cv.threshold(gray,125,255,cv.THRESH_BINARY)\\ncv.imshow('thresh',thresh)\\n\\n#contouurs\\ncontours, hierarchies=cv.findContours(canny,cv.RETR_LIST,cv.CHAIN_APPROX_SIMPLE)\\nprint(f'{len(contours) } contours found')\\n\\n# Draw the contours\\ncv.drawContours(blank,contours,-1,(90,90,90),1)\\ncv.imshow('contours drawn',blank)\\n\\ncv.waitKey(0)\"),\n",
       " Document(metadata={'path': 'SECTION_1_BASICS/1create_img.py', 'sha': '650a3eee89d2b6309bf7271fedeb5f589effd0a4', 'source': 'https://api.github.com/shafqatameen/opencv/blob/main/SECTION_1_BASICS/1create_img.py'}, page_content=\"import cv2 as cv\\nimport numpy as np\\n\\n# Create the image using numpy\\nblack_img = np.zeros((500,500),dtype=np.uint8)\\nwhite_img = np.ones((500,500),dtype=np.uint8)*255\\n\\n#show the image on window\\ncv.imshow('Black Image',black_img)\\ncv.imshow('White Image',white_img)\\ncv.waitKey(0)\"),\n",
       " Document(metadata={'path': 'SECTION_1_BASICS/2readimg.py', 'sha': '1a9f6ab97cc1c3483c20598173aa0a142b19ae8a', 'source': 'https://api.github.com/shafqatameen/opencv/blob/main/SECTION_1_BASICS/2readimg.py'}, page_content='import cv2 as cv\\n\\nimg= cv.imread(\"images/cat.jpg\")\\n\\ncv.imshow(\\'window ka naam cat image\\',img)\\ncv.waitKey(0)\\ncv.destroyAllWindows()\\n\\n#reading an img greater than the screen size\\nlarge_img=cv.imread(\"images/cat_large.jpg\")\\ncv.imshow(\\'it a large image\\',large_img)\\ncv.waitKey(0)\\n\\n'),\n",
       " Document(metadata={'path': 'SECTION_1_BASICS/3readvideocam.py', 'sha': 'cb16c9a287f181175cc2f32ed985735ff15024ba', 'source': 'https://api.github.com/shafqatameen/opencv/blob/main/SECTION_1_BASICS/3readvideocam.py'}, page_content='import cv2 as cv\\n\\n# Open the default camera (0), or use 1, 2 for external cameras\\ncapture = cv.VideoCapture(0)\\n\\n# Check if the camera opened successfully\\nif not capture.isOpened():\\n    print(\"Error: Could not open camera.\")\\n    exit()\\n\\nwhile True:\\n    isTrue, frame = capture.read()\\n\\n    if not isTrue:  # Break if frame is not read (camera error)\\n        print(\"Error reading frame.\")\\n        break\\n\\n    cv.imshow(\\'Webcam\\', frame)\\n\\n    # Press \\'d\\' to exit\\n    if cv.waitKey(1) & 0xFF == ord(\\'d\\'):\\n        break\\n\\n# Release resources\\ncapture.release()\\ncv.destroyAllWindows()\\n'),\n",
       " Document(metadata={'path': 'SECTION_1_BASICS/4readvideo.py', 'sha': '91209bfe48f9f0bbac4a2d2ba3aba5af344df125', 'source': 'https://api.github.com/shafqatameen/opencv/blob/main/SECTION_1_BASICS/4readvideo.py'}, page_content=\"import cv2 as cv\\ncapture =cv.VideoCapture('videos/dog.mp4')\\nif not capture.isOpened():\\n    print('Error: Could not open video.')\\n    exit()\\nwhile True:\\n    isTrue, frame = capture.read()\\n    if not isTrue:\\n        print('End of video or Error reading video.')\\n        break\\n    cv.imshow('dog video', frame)\\n    if cv.waitKey(33)==ord('z'):\\n        break\\ncapture.release()\\ncv.destroyAllWindows()\"),\n",
       " Document(metadata={'path': 'SECTION_1_BASICS/5imgconversion.py', 'sha': '614bc99451a8caca86d33b043127e20de0fedac2', 'source': 'https://api.github.com/shafqatameen/opencv/blob/main/SECTION_1_BASICS/5imgconversion.py'}, page_content=\"import cv2 as cv\\nimport numpy as np\\n# Reading the image\\nimg= cv.imread('images/cat.jpg')\\ncv.imshow('Cat', img)\\n# Converting to grayscale\\ngrayimg= cv.cvtColor(img, cv.COLOR_BGR2GRAY)\\ncv.imshow('Gray cat', grayimg)\\n#converting to hsv\\nhsvimg= cv.cvtColor(img, cv.COLOR_BGR2HSV)\\ncv.imshow('hue saturation value', hsvimg)\\n#converting to lab\\nlabimg=cv.cvtColor(img,cv.COLOR_BGR2LAB)\\ncv.imshow('lab',labimg)\\n#converting to rgb\\nrgbimg=cv.cvtColor(img,cv.COLOR_BGR2RGB)\\ncv.imshow('rgb',rgbimg)\\n#converting to hls\\nhlsimg=cv.cvtColor(img,cv.COLOR_BGR2HLS)\\ncv.imshow('hUE lightness saturation',hlsimg)\\n#converting to xyz\\nxyzimg=cv.cvtColor(img,cv.COLOR_BGR2XYZ)\\ncv.imshow('x y z',xyzimg)\\n#displaying the single channels\\ncv.imshow('blue ',img[:,:,0])\\ncv.imshow('Green ',img[:,:,1])\\ncv.imshow('Red ',img[:,:,2])\\n\\ncv.waitKey(0)\"),\n",
       " Document(metadata={'path': 'SECTION_1_BASICS/6resizeframe.py', 'sha': 'dce1d2a565453e4e406dff4b9f2b6c114f8c4aa2', 'source': 'https://api.github.com/shafqatameen/opencv/blob/main/SECTION_1_BASICS/6resizeframe.py'}, page_content=\"import cv2 as cv\\n\\n# resizing frame of video or image\\ndef rescaleFrame(frame,scale=75):\\n    width=int(frame.shape[1]*scale//100)\\n    height=int(frame.shape[0]*scale//100)\\n    dimensions=(width,height)\\n    return cv.resize(frame,dimensions,interpolation=cv.INTER_AREA)\\nframe=cv.imread('images/cat_large.jpg')\\ncv.imshow('Cat',frame)\\nrescaled_frame=rescaleFrame(frame,25)\\ncv.imshow('Cat reduced to 75%',rescaled_frame)\\ncv.waitKey(0)\\n\\n'''import cv2 as cv\\n\\ndef rescaleFrame(frame, scale=0.75):\\n    # Calculate the new dimensions based on the scale\\n    width = int(frame.shape[1] * scale)\\n    height = int(frame.shape[0] * scale)\\n    dimensions = (width, height)\\n\\n    # Resize the frame using the calculated dimensions\\n    return cv.resize(frame, dimensions, interpolation=cv.INTER_AREA)\\n\\n# Example usage:\\n# Assuming 'frame' is an image or video frame loaded using OpenCV\\nframe = cv.imread('images/cat.jpg')\\nresized_frame = rescaleFrame(frame, scale=0.5)\\ncv.imshow('Resized Frame', resized_frame)\\ncv.waitKey(0)'''\"),\n",
       " Document(metadata={'path': 'SECTION_1_BASICS/6resizeframe2.py', 'sha': '4a791a67dba78480f8f302a1946408ee20d38b5d', 'source': 'https://api.github.com/shafqatameen/opencv/blob/main/SECTION_1_BASICS/6resizeframe2.py'}, page_content='import cv2 as cv\\n#resiszng the frame of video \\ndef resizedFrame(frame,scale=75):\\n    width=int(frame.shape[1]*scale/100)\\n    height=int(frame.shape[0]*scale/100)\\n    dim=(width,height)\\n    return cv.resize(frame,dim,interpolation=cv.INTER_AREA)\\ncapture= cv.VideoCapture(\"videos/dog.mp4\")\\nwhile True:\\n    isTrue,frame=capture.read()\\n    if not isTrue:\\n        print(\\'Can not read the video\\')\\n        break\\n    cv.imshow(\\'Video\\',frame)\\n    frame_resized=resizedFrame(frame,scale=50)\\n    cv.imshow(\\'Resized video\\',frame_resized)\\n\\n    if cv.waitKey(20)&0xFF==ord(\\'d\\'):\\n        break\\ncapture.release()\\ncv.destroyAllWindows()'),\n",
       " Document(metadata={'path': 'SECTION_1_BASICS/6resizeframeCam3.py', 'sha': '4ea05613d4a95098cb945acbeb9b44222fd2d34e', 'source': 'https://api.github.com/shafqatameen/opencv/blob/main/SECTION_1_BASICS/6resizeframeCam3.py'}, page_content='import cv2 as cv\\n\\n#resiszng the frame of live video, its working for all videos,live and images\\ndef resizedFrame(frame,scale=75):\\n    width=int(frame.shape[1]*scale/100)\\n    height=int(frame.shape[0]*scale/100)\\n    dim=(width,height)\\n    return cv.resize(frame,dim,interpolation=cv.INTER_AREA)\\n\\'\\'\\'def changeRes(width,height):\\n    #only for live video another method for live video\\n    capture.set(3,width)\\n    capture.set(4,height)\\'\\'\\'\\n# Open the default camera (0), or use 1, 2 for external cameras\\ncapture = cv.VideoCapture(0)\\n\\n\\n# Check if the camera opened successfully\\nif not capture.isOpened():\\n    print(\"Error: Could not open camera.\")\\n    exit()\\n\\nwhile True:\\n    isTrue, frame = capture.read()\\n    if not isTrue:  # Break if frame is not read (camera error)\\n        print(\"Error reading frame.\")\\n        break\\n    # Resize the frame\\n    resized_frame=resizedFrame(frame,scale=20)\\n    cv.imshow(\\'Webcam\\', resized_frame)\\n\\n    # Press \\'d\\' to exit\\n    if cv.waitKey(20) & 0xFF == ord(\\'d\\'):\\n        break\\n\\n# Release resources\\ncapture.release()\\ncv.destroyAllWindows()\\n'),\n",
       " Document(metadata={'path': 'SECTION_1_BASICS/7shapes.py', 'sha': '3eebf7dd8820be17895889725d8400366e4f629b', 'source': 'https://api.github.com/shafqatameen/opencv/blob/main/SECTION_1_BASICS/7shapes.py'}, page_content='import cv2 as cv\\nimport numpy as np\\n\\n# Create the image using numpy\\nblank=np.zeros((500,500,3),dtype=\"uint8\")\\ncv.imshow(\\'Blank Image\\',blank)\\n\\n# 1. Paint the image a certain color\\nblank[0:500,0:250]=100,25,10\\ncv.imshow(\"green\",blank)\\n\\n#draw the rectangle\\ncv.rectangle(blank,pt1=(0,0),pt2=(250,250),color=(0,0,200),thickness=-1)\\ncv.imshow(\"Rectangle\",blank)\\n\\n#draw the circle\\ncv.circle(blank,(blank.shape[1]//2,blank.shape[0]//2),100,(200,0,220),thickness=-1)\\ncv.imshow(\"Circle\",blank)\\n\\n#draw the text\\ncv.putText(blank,\"hello bhai\",(0,int(0.75*blank.shape[0])),cv.FONT_HERSHEY_TRIPLEX,2,(20,255,0),1)\\ncv.imshow(\"Text\",blank)\\n\\n#draw the line\\ncv.line(blank,(0,0),(500,500),(255,255,255),3)\\ncv.imshow(\"Line\",blank)\\ncv.waitKey(0)'),\n",
       " Document(metadata={'path': 'SECTION_1_BASICS/8basic_fxns.py', 'sha': '0415f51320b41c73908af12ae37862ff0e02ec65', 'source': 'https://api.github.com/shafqatameen/opencv/blob/main/SECTION_1_BASICS/8basic_fxns.py'}, page_content=\"import cv2 as cv\\n\\nimg=cv.imread('images/park.jpg')\\ncv.imshow('Park',img)\\n\\n# 1. Convert to grayscale\\ngray=cv.cvtColor(img,cv.COLOR_BGR2GRAY)\\ncv.imshow('Gray',gray)\\n\\n# 2. Blur the image\\nblur=cv.GaussianBlur(img,(3,3),cv.BORDER_DEFAULT)\\ncv.imshow('Blur',blur)\\n\\n# 3. Edge Cascade\\ncanny=cv.Canny(blur,125,175)\\ncv.imshow('Canny Edges',canny)\\n\\n# 4. Dilating the image\\ndilated=cv.dilate(canny,(3,3),iterations=1)\\ncv.imshow('Dilated',dilated)\\n\\n# 5. Eroding\\neroded=cv.erode(dilated,(3,3),iterations=1)\\ncv.imshow('Eroded',eroded)\\ncv.waitKey(0)\\n\"),\n",
       " Document(metadata={'path': 'SECTION_1_BASICS/9transformation.py', 'sha': 'b962cce136bcadd7060352eaee5732a73fc38c95', 'source': 'https://api.github.com/shafqatameen/opencv/blob/main/SECTION_1_BASICS/9transformation.py'}, page_content='import cv2 as cv\\nimport numpy as np\\n\\n#load and read the image\\nimg=cv.imread(\"images/park.jpg\")\\ncv.imshow(\\'Park\\',img)\\n\\n# transformation functions\\ndef translate(img,x,y):\\n    transMat=np.float32([[1,0,x],[0,1,y]])\\n    dim=(img.shape[1],img.shape[0])\\n    return cv.warpAffine(img,transMat,dim)\\n\\n\\'\\'\\'x: Moves the image horizontally.\\n+x â†’ Shifts right.\\n-x â†’ Shifts left.\\n   y: Moves the image vertically.\\n+y â†’ Shifts down.\\n-y â†’ Shifts up.           \\'\\'\\'\\n\\ntranslated=translate(img,-100,-100)\\ncv.imshow(\"window\",translated)\\n\\n# Rotation\\ndef rotate(img,angle,rotPoint=None):\\n    (height,width)=img.shape[:2]\\n    if rotPoint is None:\\n        rotPoint=(width//2,height//2)\\n    rotMat=cv.getRotationMatrix2D(rotPoint,angle,1.0)\\n    dim=(width,height)\\n    return cv.warpAffine(img,rotMat,dim)\\n\\nrotated=rotate(img,180)\\ncv.imshow(\\'rotated image\\',rotated)\\n\\n# Resizing\\nresized=cv.resize(img,(1000,1000),interpolation=cv.INTER_CUBIC)\\ncv.imshow(\\'Resized\\',resized)\\n\\n#flip\\n\\'\\'\\'flipCode: Defines the flipping direction:\\n0  â†’ Flip vertically (upside down).\\n1  â†’ Flip horizontally (mirror image).\\n-1 â†’ Flip both vertically & horizontally (180Â° rotation).\\'\\'\\'\\nfliped=cv.flip(img,-1)\\ncv.imshow(\\'Fliped\\',fliped)\\n\\n#cropping\\ncropped=img[0:img.shape[0]//2,0:img.shape[1]//2]\\ncv.imshow(\\'Cropped\\',cropped)\\n\\n\\ncv.waitKey(0)\\n'),\n",
       " Document(metadata={'path': 'SECTION_2_ADVANCE/0demo.py', 'sha': '3442eb609272aabe1d102a2210465626213d7f0f', 'source': 'https://api.github.com/shafqatameen/opencv/blob/main/SECTION_2_ADVANCE/0demo.py'}, page_content=\"import cv2 as cv\\nimport numpy as np\\n\\nimg = cv.imread('images/park.jpg')\\ngray = cv.cvtColor(img, cv.COLOR_BGR2GRAY)\\n\\nlap_float = cv.Laplacian(gray, cv.CV_64F)\\nlap_abs = np.absolute(lap_float)\\nlap_uint8 = np.uint8(lap_abs)\\n\\ncv.imshow('Laplacian Float64', lap_float / lap_float.max())  # Normalize for display\\ncv.imshow('Laplacian Absolute', lap_abs / lap_abs.max())      # Normalize for display\\ncv.imshow('Laplacian uint8', lap_uint8)\\n\\ncv.waitKey(0)\\ncv.destroyAllWindows()\\n\"),\n",
       " Document(metadata={'path': 'SECTION_2_ADVANCE/11matlab_cv2_imshow.py', 'sha': '24b94e29f5d266cf0de16bfe71446b7cea96cb90', 'source': 'https://api.github.com/shafqatameen/opencv/blob/main/SECTION_2_ADVANCE/11matlab_cv2_imshow.py'}, page_content='import cv2 as cv\\nimport matplotlib.pyplot as plt\\nfrom pathlib import Path as path\\nimport numpy as np\\n\\n#arranging the file using pathlib\\nimage=sorted(path(\"images\").glob(\\'*\\'))\\nprint(image)\\n#default bgr img\\nimg=cv.imread(image[3])\\n#cv.imshow(\"img->bgr\", img)\\n#converted into rgb img\\nimg_rgb=cv.cvtColor(img,cv.COLOR_BGR2RGB)\\n#cv.imshow(\"img->rbg\", img_rgb)\\n\\ncmbined_img=np.hstack((img_rgb,img))\\ncv.imshow(\"stack of img: img_rgb,img\", cmbined_img)\\n\\n#ushing stack display hsv, gray,lab,hls\\nhsv_img=cv.cvtColor(img,cv.COLOR_BGR2HSV)\\nlab_img=cv.cvtColor(img,cv.COLOR_BGR2LAB)\\nhls_img=cv.cvtColor(img,cv.COLOR_BGR2HLS)\\n\\nstack_img=np.hstack((hsv_img,lab_img,hls_img))\\n\\ncv.imshow(\"stack of imgs(cv)\", stack_img)\\n\\n# Display each image in a separate Matplotlib window\\n#using hstack in matplotlib for img_rgb and img_bgr\\nplt.figure(\"stack of img : img_rgb,img\")\\nplt.imshow(cmbined_img)\\nplt.title(\"stack of img : img_rgb,img\")\\n\\n\\n#using hstack in matplotlib for hsv,gray,lab,hls\\nplt.figure(\"stack of imgs(plt)\")\\nplt.imshow(stack_img)               #need image in rgb only(plt )\\nplt.title(\"stack of imgs(plt)\")\\nplt.show()\\n\\ncv.waitKey(0)\\nif cv.waitKey(0)& 0xFF ==ord(\\'q\\'):\\n    cv.destroyAllWindows()\\n\\n'),\n",
       " Document(metadata={'path': 'SECTION_2_ADVANCE/12split_merge.py', 'sha': '8d46385906e12866e01cd174b29bac6b9f546b32', 'source': 'https://api.github.com/shafqatameen/opencv/blob/main/SECTION_2_ADVANCE/12split_merge.py'}, page_content='import cv2 as  cv\\nimport numpy as np\\nfrom pathlib import Path\\n\\n#path of images\\nimage=sorted(Path(\"images\").glob(\\'*\\'))[2]\\n\\n#load img\\nimg=cv.imread(str(image))\\n#show img\\ncv.imshow(\"bgr img\",img)\\n\\nb , g , r=cv.split(img)\\nstack_rgb=np.hstack((b,g,r))\\ncv.imshow(\"stack of img\",stack_rgb,)\\n\\n#shape of img and  b g r imgs\\nprint(img.shape)\\nprint(b.shape)\\nprint(g.shape)\\nprint(r.shape)\\n\\n#merge \\nmerged=cv.merge([b,g,r])\\ncv.imshow(\"merged img\", merged)\\n\\nblank=np.zeros(img.shape[:2],dtype=\"uint8\")\\nblue=cv.merge([b,blank,blank])\\ngreen=cv.merge([blank,g,blank])\\nred=cv.merge([blank,blank,r])\\n\\nstack_colors=np.hstack((blue,green,red))\\ncv.imshow(\"stack of colors\",stack_colors)\\n\\ncv.waitKey(0)\\nif cv.waitkey(0) &0xFF==ord(\\'q\\'):\\n    cv.destroyAllWindows'),\n",
       " Document(metadata={'path': 'SECTION_2_ADVANCE/13smoothing.py', 'sha': '94978703569d53c2c2135916a44f4ec4285e60f0', 'source': 'https://api.github.com/shafqatameen/opencv/blob/main/SECTION_2_ADVANCE/13smoothing.py'}, page_content='import cv2 as cv\\nimport numpy as np\\n\\n#load img\\nimage=cv.imread(\"images/cats.jpg\")\\ncv.imshow(\"cat\",image)\\nimg=cv.resize(image,(300,300))\\ncv.imshow(\"resized cat\",img)\\n#average blur\\nimg_blur=cv.blur(img,(3,3))\\ncv.imshow(\"blur\",img_blur)\\n\\n#Gaussian blur\\nimg_gauss_blur=cv.GaussianBlur(img,(3,3),0)\\ncv.imshow(\"Gaussian blur\",img_gauss_blur)\\n\\n#median blur\\nimg_median=cv.medianBlur(img,3)\\ncv.imshow(\"median blur\",img_median)\\n\\n#bilateral blur\\nimg_bilateral=cv.bilateralFilter(img,3,75,75)\\ncv.imshow(\"bilateral blur\",img_bilateral)\\n\\n#hstack\\nstacked=np.hstack((img,img_blur,img_gauss_blur,img_median,img_bilateral))\\ncv.imshow(\"stacked (img,img_blur,img_gauss_blur,img_median,img_bilateral)\",stacked)\\n\\ncv.waitKey(0)'),\n",
       " Document(metadata={'path': 'SECTION_2_ADVANCE/14bitwise.py', 'sha': 'f15d8f75d26aeebbd1602141230e995fc3d49476', 'source': 'https://api.github.com/shafqatameen/opencv/blob/main/SECTION_2_ADVANCE/14bitwise.py'}, page_content='import cv2 as cv\\nimport numpy as np\\n\\n#create blank img\\nblank=np.zeros((400,400),dtype=\"uint8\")\\nrectangle=cv.rectangle(blank.copy(),(30,30),(370,370),255,-1)\\ncircle=cv.circle(blank.copy(), (200,200), 200,255,-1)\\n\\ncv.imshow(\"Rectangle\",rectangle)\\ncv.imshow(\"Circle\",circle)\\n\\n#bitwise AND\\nbitwise_and=cv.bitwise_and(rectangle,circle)\\ncv.imshow(\"Bitwise AND\",bitwise_and)\\n\\n#bitwise OR\\nbitwise_or=cv.bitwise_or(rectangle,circle)\\ncv.imshow(\"Bitwise OR\",bitwise_or)\\n\\n#bitwise XOR\\nbitwise_xor=cv.bitwise_xor(rectangle,circle)\\ncv.imshow(\"Bitwise XOR\",bitwise_xor)\\n\\n#bitwise NOT\\nbitwise_not=cv.bitwise_not(rectangle,circle)\\ncv.imshow(\"Bitwise NOT\",bitwise_not)\\n\\n\\ncv.waitKey(0)'),\n",
       " Document(metadata={'path': 'SECTION_2_ADVANCE/15masking.py', 'sha': '7f3be8c52e216d452ef1e303dc00bd70abe1d9bd', 'source': 'https://api.github.com/shafqatameen/opencv/blob/main/SECTION_2_ADVANCE/15masking.py'}, page_content='import cv2 as cv\\nimport numpy as np\\n\\n#load and read the image\\nimg=cv.imread(\"images/park.jpg\")\\ncv.imshow(\\'Park\\',img)\\n\\n#BLANK\\nblank=np.zeros(img.shape[:2],dtype=\"uint8\")\\ncv.imshow(\"Blank\",blank)\\nmask=cv.rectangle(blank.copy(),(0,0),(img.shape[1]//2,img.shape[0]//2),255,-1)\\ncv.imshow(\"mask\",mask)\\n\\nmasked=cv.bitwise_and(img,img,mask=mask)\\ncv.imshow(\"masked\",masked)\\ncv.waitKey(0)'),\n",
       " Document(metadata={'path': 'SECTION_2_ADVANCE/16histogram1_gray.py', 'sha': '83eb64b019bc85a206ccc1cb0313e62d1c671f5f', 'source': 'https://api.github.com/shafqatameen/opencv/blob/main/SECTION_2_ADVANCE/16histogram1_gray.py'}, page_content='import cv2 as cv\\nimport matplotlib.pyplot as plt\\nimport numpy as np\\n\\n#load and read the img\\nimg=cv.imread(\"images/park.jpg\")\\ncv.imshow(\\'Park\\',img)\\n\\n#conver the img into gray\\ngray=cv.cvtColor(img,cv.COLOR_BGR2GRAY)\\ncv.imshow(\\'gray\\',gray)\\n\\n\\n\\nblank=np.zeros(img.shape[:2],dtype=\"uint8\")\\nmask=cv.rectangle(blank.copy(),(0,0),(img.shape[1]//2,img.shape[0]//2),255,-1)\\nmasked=cv.bitwise_and(img,img,mask=mask)\\ncv.imshow(\"masked\",masked)\\n\\n#grayscale histogram\\ngray_hist=cv.calcHist([gray],[0],None,[256],[0,256])\\n#histogram of masked image\\nmasked_gray_hist=cv.calcHist([gray],[0],mask,[256],[0,256])\\n\\n\\n#plot the histogram\\nplt.figure()\\nplt.title(\"Grayscale Histogram\")\\nplt.xlabel(\"Bins\")\\nplt.ylabel(\"# of pixels\")\\nplt.plot(gray_hist)\\nplt.xlim([0,256])\\n\\n#plot the histogram for the masked image\\nplt.figure()\\nplt.title(\"Masked Grayscale Histogram\")\\nplt.xlabel(\"Bins\")\\nplt.ylabel(\"# of pixels\")\\nplt.plot(masked_gray_hist)\\nplt.xlim([0,256])\\nplt.show()\\n\\n\\n\\ncv.waitKey(0)'),\n",
       " Document(metadata={'path': 'SECTION_2_ADVANCE/16histogram2_color.py', 'sha': 'bff589bc14b5c7305d4ca867965dac874eca3845', 'source': 'https://api.github.com/shafqatameen/opencv/blob/main/SECTION_2_ADVANCE/16histogram2_color.py'}, page_content='import cv2 as cv\\nimport numpy as np\\nimport matplotlib.pyplot as plt\\n\\nimg=cv.imread(\\'images/cats.jpg\\')\\n\\n#masked image\\nblank=np.zeros(img.shape[:2],dtype=\"uint8\")\\nmask=cv.rectangle(blank.copy(),(0,0),(img.shape[1]//2,img.shape[0]//2),255,-1)\\nmasked=cv.bitwise_and(img,img,mask=mask)\\ncv.imshow(\"masked\",masked)\\n\\n#color histogram for color image\\ncolors=(\\'b\\',\\'g\\',\\'r\\')\\nfor i,col in enumerate(colors):\\n    hist_color=cv.calcHist([img],[i],None,[256],[0,256])\\n    plt.plot(hist_color,color=col)\\n    plt.xlim([0,256])    \\nplt.show()\\n\\n#color histogram for masked image\\ncolors=(\\'b\\',\\'g\\',\\'r\\')\\nfor i,col in enumerate(colors):\\n    hist_color=cv.calcHist([masked],[i],None,[256],[0,256])\\n    plt.plot(hist_color,color=col)\\n    plt.xlim([0,256])   \\nplt.show()\\n'),\n",
       " Document(metadata={'path': 'SECTION_2_ADVANCE/17threshold.py', 'sha': '07415a7925f2da2d5c363e2a8363ac54538419d6', 'source': 'https://api.github.com/shafqatameen/opencv/blob/main/SECTION_2_ADVANCE/17threshold.py'}, page_content=\"import cv2 as cv\\n\\nimg = cv.imread('images/park.jpg')\\ngray = cv.cvtColor(img, cv.COLOR_BGR2GRAY)\\n\\n# Simple threshold\\nthreshold , thresh = cv.threshold(gray, 150,255,cv.THRESH_BINARY)\\ncv.imshow('Simple Threshold', thresh)\\n\\n# Inverse threshold\\nthreshold,thresh= cv.threshold(gray,150,255,cv.THRESH_BINARY_INV)\\ncv.imshow('Inverse Threshold', thresh)\\n\\n\\n# Adaptive threshold\\nadp_thresh=cv.adaptiveThreshold(gray,255,cv.ADAPTIVE_THRESH_MEAN_C,cv.THRESH_BINARY,11,1)\\ncv.imshow('Adaptive Threshold', adp_thresh)\\n\\ncv.waitKey(0)\\n\"),\n",
       " Document(metadata={'path': 'SECTION_2_ADVANCE/18edge.py', 'sha': '5e07b2d67bec704bb710ab981b677df1763b2efb', 'source': 'https://api.github.com/shafqatameen/opencv/blob/main/SECTION_2_ADVANCE/18edge.py'}, page_content=\"import cv2 as cv\\nimport numpy as np\\n\\nimg=cv.imread('images/park.jpg')\\ncv.imshow('Park',img)\\ngray=cv.cvtColor(img,cv.COLOR_BGR2GRAY)\\ncv.imshow('Gray',gray)\\n\\n#laplacian edge detection\\nlap=cv.Laplacian(gray,cv.CV_64F)\\nabs_lap=np.absolute(lap)\\nlap_uint8=np.uint8(abs_lap)\\n#all_lap=np.hstack((lap_uint8,lap,abs_lap))#needs all in same data type\\ncv.imshow('Laplacian',lap_uint8)\\n\\n\\n#sobel edge detection\\nsobelx=cv.Sobel(gray,cv.CV_64F,1,0)\\nsobely=cv.Sobel(gray,cv.CV_64F,0,1)\\ncombined_sobel=cv.bitwise_and(sobelx,sobely)\\ncombined=np.hstack((combined_sobel,sobelx,sobely))\\ncv.imshow('Sobel',combined)\\n\\n#canny edge detection\\ncanny = cv.Canny(gray,125,175)\\ncv.imshow('Canny',canny)\\n\\ncv.waitKey(0)\"),\n",
       " Document(metadata={'path': 'SECTION_3_Faces/19haar_face_dection.py', 'sha': '3f79aeaa6ae1e29f684f895a8ef06124e47e3264', 'source': 'https://api.github.com/shafqatameen/opencv/blob/main/SECTION_3_Faces/19haar_face_dection.py'}, page_content='import cv2 as cv\\n#load and read the image\\nimg=cv.imread(\"images/lady.jpg\")\\ncv.imshow(\"lady\",img)\\n#conver into gray\\ngray_img=cv.cvtColor(img,cv.COLOR_BGR2GRAY)\\ncv.imshow(\"gray\",gray_img)\\n#haar cascade\\nhaar_cascade=cv.CascadeClassifier(\"SECTION_3_Faces/haar_face.xml\")\\n\\n#faces_rect=haar_cascade.detectMultiScale(gray_img,scaleFactor=1.1,minNeighbors=4)\\nfaces_rect=haar_cascade.detectMultiScale(gray_img,1.1,4)\\n#dectect the face\\nfor x,y,w,h in faces_rect:\\n    cv.rectangle(img,(x,y),(x+w,y+h),(0,255,0),3)\\ncv.imshow(\"face\",img)\\nprint(f\\'number of faces found={len(faces_rect)}\\')\\ncv.waitKey(0)'),\n",
       " Document(metadata={'path': 'SECTION_3_Faces/19haar_face_dection2.py', 'sha': '34efecdd740379bc15df72ad5d3281938a621fd3', 'source': 'https://api.github.com/shafqatameen/opencv/blob/main/SECTION_3_Faces/19haar_face_dection2.py'}, page_content='import cv2 as cv\\n#img_path=\\'\\'\\'C:\\\\xampp\\\\htdocs\\\\programming\\\\opencv\\\\images\\\\group2.jpg\\'\\'\\'\\n\\n#load and read the image\\nimg=cv.imread(\"images/group1.jpg\")\\n\\n# Check if image is loaded\\ncv.imshow(\"group2\",img)\\n\\n#conver into gray\\ngray_img=cv.cvtColor(img,cv.COLOR_BGR2GRAY)\\ncv.imshow(\"gray\",gray_img)\\n\\n#haar cascade\\nhaar_cascade=cv.CascadeClassifier(\"SECTION_3_Faces/haar_face.xml\")\\n\\n#faces_rect=haar_cascade.detectMultiScale(gray_img,scaleFactor=1.1,minNeighbors=4)\\nfaces_rect=haar_cascade.detectMultiScale(gray_img,1.1,1)\\n\\n#dectect the face\\nfor x,y,w,h in faces_rect:\\n    cv.rectangle(img,(x,y),(x+w,y+h),(0,255,0),3)\\ncv.imshow(\"face\",img)\\nprint(f\\'number of faces found={len(faces_rect)}\\')\\ncv.waitKey(0)'),\n",
       " Document(metadata={'path': 'SECTION_3_Faces/20face_dection.py', 'sha': '6e0475cd12fb36ff3127f5ff972644ee2d296e7e', 'source': 'https://api.github.com/shafqatameen/opencv/blob/main/SECTION_3_Faces/20face_dection.py'}, page_content='# pylint:disable=no-member\\nimport cv2 as cv\\nimport numpy as np\\nfrom pathlib import Path\\n\\npeople = [\\'Ben Afflek\\', \\'Elton John\\', \\'Jerry Seinfield\\', \\'Madonna\\', \\'Mindy Kaling\\']\\nDIR = Path(\"C:/xampp/htdocs/programming/opencv/SECTION_3_Faces/train\")\\nhaar_cascade = cv.CascadeClassifier(str(Path(\"SECTION_3_Faces/haar_face.xml\")))\\n\\nfeatures = []\\nlabels = []\\n\\ndef create_train():\\n    for person in people:\\n        path = DIR / person\\n        label = people.index(person)\\n\\n        for img_path in path.iterdir():\\n            img_array = cv.imread(str(img_path))\\n            \\n            if img_array is None:\\n                continue  # Skip unreadable images\\n\\n            gray = cv.cvtColor(img_array, cv.COLOR_BGR2GRAY)\\n            faces_rect = haar_cascade.detectMultiScale(gray, scaleFactor=1.1, minNeighbors=4)\\n\\n            for (x, y, w, h) in faces_rect:\\n                face_roi = gray[y:y+h, x:x+w]\\n                print(f\"face_roi: {face_roi.shape}\")\\n                features.append(face_roi)\\n                print(f\"features: {len(features)}\")\\n                labels.append(label)\\n                print(f\\'Label: {label}, {img_path}\\')\\n                \\n\\ncreate_train()\\n\\nprint(f\"Training complete âœ…\\\\nTotal faces: {len(features)}\\\\nTotal labels: {len(labels)}\")\\n\\nfeatures = np.array(features, dtype=\\'object\\')\\nlabels = np.array(labels)\\n\\nface_recognizer = cv.face.LBPHFaceRecognizer_create()\\nface_recognizer.train(features, labels)\\nface_recognizer.save(\"SECTION_3_Faces/face_trained.yml\")\\nnp.save(\"SECTION_3_Faces/features.npy\", features)\\nnp.save(\"SECTION_3_Faces/labels.npy\", labels)\\n\\n\\n\\n\\n\\'\\'\\'import os \\nimport cv2 as cv\\nimport numpy as np\\n\\npeople=[\\'Ben Afflek\\', \\'Elton John\\', \\'Jerry Seinfield\\', \\'Madonna\\', \\'Mindy Kaling\\']\\nDIC=r\"C:/xampp/htdocs/programming/opencv\\\\SECTION_3_Faces/train\"\\np=[]\\nfor i  in os.listdir(r\"C:/xampp/htdocs/programming/opencv\\\\SECTION_3_Faces/train\"):\\n    p.append(i)\\n\\nprint(p)\\n\\n\\nfeatures=[]\\nlabels=[]\\nhaar_cascade=cv.CascadeClassifier(\"SECTION_3_Faces/haar_face.xml\")\\n\\ndef create_train():\\n    for person in people:\\n        path=os.path.join(DIC,person)\\n        label=people.index(person)  \\n        for img in os.listdir(path):\\n            img_path=os.path.join(path,img)\\n            img_array=cv.imread(img_path)\\n            gray=cv.cvtColor(img_array,cv.COLOR_BGR2GRAY)\\n            faces_rect=haar_cascade.detectMultiScale(gray,1.1,1)\\n            for (x,y,w,h) in faces_rect:\\n                faces_roi=gray[y:y+h,x:x+w]\\n                features.append(faces_roi)\\n                labels.append(label)\\n\\ncreate_train()\\n\\nprint(f\\'length of features={len(features)}\\')\\nprint(f\\'length of labels={len(labels)}\\')\\n\\nfeatures=np.array(features,dtype=\"object\")\\nlabels=np.array(labels)\\n\\nface_recognizer=cv.face.LBPHFaceRecognizer_create()\\nface_recognizer.train(features,labels)\\n\\nface_recognizer.write(\\'train.yml\\')\\n\\nprint(\"training done\")\\n#save labels and features\\nnp.save(\\'features.npy\\',features)\\nnp.save(\\'labels.npy\\',labels)\\n\\n\\'\\'\\''),\n",
       " Document(metadata={'path': 'SECTION_3_Faces/21face_recognization.py', 'sha': 'd8f9756c81caf122c04ecb9e8388a96a4ae6348a', 'source': 'https://api.github.com/shafqatameen/opencv/blob/main/SECTION_3_Faces/21face_recognization.py'}, page_content=\"\\nimport numpy as np\\nimport cv2 as cv\\n\\nhaar_cascade = cv.CascadeClassifier('SECTION_3_Faces\\\\haar_face.xml')\\n\\npeople = ['Ben Afflek', 'Elton John', 'Jerry Seinfield', 'Madonna', 'Mindy Kaling']\\n# features = np.load('features.npy', allow_pickle=True)\\n# labels = np.load('labels.npy')\\n\\nface_recognizer = cv.face.LBPHFaceRecognizer_create()\\nface_recognizer.read('SECTION_3_Faces/face_trained.yml')\\n\\nimg = cv.imread(r'SECTION_3_Faces\\\\val\\\\mindy_kaling\\\\5.jpg')\\n\\ngray = cv.cvtColor(img, cv.COLOR_BGR2GRAY)\\ncv.imshow('Person', gray)\\n\\n# Detect the face in the image\\nfaces_rect = haar_cascade.detectMultiScale(gray, 1.1, 4)\\n\\nfor (x,y,w,h) in faces_rect:\\n    faces_roi = gray[y:y+h,x:x+w]\\n\\n    label, confidence = face_recognizer.predict(faces_roi)\\n    print(f'Label = {people[label]} with a confidence of {confidence}')\\n\\n    cv.putText(img, str(people[label]), (20,20), cv.FONT_HERSHEY_COMPLEX, 1.0, (0,255,0), thickness=2)\\n    cv.rectangle(img, (x,y), (x+w,y+h), (0,255,0), thickness=2)\\n\\ncv.imshow('Detected Face', img)\\n\\ncv.waitKey(0)\"),\n",
       " Document(metadata={'path': 'SECTION_3_Faces/21face_recognization2_multi img.py', 'sha': '2c21c8cea174d5c600ec3f6717ec5710b344f977', 'source': 'https://api.github.com/shafqatameen/opencv/blob/main/SECTION_3_Faces/21face_recognization2_multi img.py'}, page_content='from pathlib import Path\\nimport numpy as np\\nimport cv2 as cv\\n\\n# Load Haar cascade\\nhaar_cascade = cv.CascadeClassifier(str(Path(\\'SECTION_3_Faces/haar_face.xml\\')))\\n\\n# People names based on your training labels (same order!)\\npeople = [\\'Ben Afflek\\', \\'Elton John\\', \\'Jerry Seinfield\\', \\'Madonna\\', \\'Mindy Kaling\\']\\n\\n# Load trained face recognizer\\nface_recognizer = cv.face.LBPHFaceRecognizer_create()\\nface_recognizer.read(str(Path(\\'SECTION_3_Faces/face_trained.yml\\')))\\n\\n# Set the path to validation images\\nval_dir = Path(\\'SECTION_3_Faces/val\\')\\n\\n# Traverse all JPG images in all subfolders of val_dir\\nfor img_path in val_dir.glob(\\'*/*.jpg\\'):\\n    img = cv.imread(str(img_path))\\n    if img is None:\\n        print(f\"[!] Could not load image: {img_path}\")\\n        continue\\n\\n    gray = cv.cvtColor(img, cv.COLOR_BGR2GRAY)\\n    faces_rect = haar_cascade.detectMultiScale(gray, 1.1, 4)\\n\\n    print(f\"\\\\nðŸ“· Image: {img_path.name}\")\\n    for (x, y, w, h) in faces_rect:\\n        faces_roi = gray[y:y + h, x:x + w]\\n\\n        label, confidence = face_recognizer.predict(faces_roi)\\n        print(f\\'    â†’ Predicted: {people[label]} | Confidence: {round(confidence, 2)}\\')\\n\\n        cv.putText(img, people[label], (x, y - 10), cv.FONT_HERSHEY_COMPLEX, 0.9, (0, 255, 0), 2)\\n        cv.rectangle(img, (x, y), (x + w, y + h), (0, 255, 0), 2)\\n\\n    cv.imshow(\\'Detected Face\\', img)\\n    cv.waitKey(500)  # Show each image for 500ms\\n    cv.destroyAllWindows()'),\n",
       " Document(metadata={'path': 'SECTION_4_deep_CV/0demo.py', 'sha': 'c8c247ad3f77d97a505afa524a695c143dc3289d', 'source': 'https://api.github.com/shafqatameen/opencv/blob/main/SECTION_4_deep_CV/0demo.py'}, page_content='#pylint:disable=no-member (Removes linting problems with cv)\\n\\n# Installing `caer` and `canaro` since they don\\'t come pre-installed\\n# Uncomment the following line:\\n# !pip install --upgrade caer canaro\\n\\nimport os\\nimport caer\\nimport canaro\\nimport numpy as np\\nimport cv2 as cv\\nimport gc\\nimport matplotlib.pyplot as plt\\nfrom tensorflow.keras.utils import to_categorical\\nfrom tensorflow.keras.callbacks import LearningRateScheduler\\nfrom tensorflow.keras.models import Sequential\\nfrom tensorflow.keras.layers import Dense, Flatten, Dropout, Conv2D, MaxPooling2D\\nfrom tensorflow.keras.optimizers.legacy import SGD\\n\\nIMG_SIZE = (80,80)\\nchannels = 1\\nchar_path = r\\'SECTION_4_deep_CV\\\\archive\\\\simpsons_dataset\\'\\n\\n# Creating a character dictionary, sorting it in descending order\\nchar_dict = {}\\nfor char in os.listdir(char_path):\\n    char_dict[char] = len(os.listdir(os.path.join(char_path,char)))\\n\\n# Sort in descending order\\nchar_dict = caer.sort_dict(char_dict, descending=True)\\nchar_dict\\n\\n#  Getting the first 10 categories with the most number of images\\ncharacters = []\\ncount = 0\\nfor i in char_dict:\\n    characters.append(i[0])\\n    count += 1\\n    if count >= 10:\\n        break\\ncharacters\\n\\n# Create the training data\\ntrain = caer.preprocess_from_dir(char_path, characters, channels=channels, IMG_SIZE=IMG_SIZE, isShuffle=True)\\n\\n# Number of training samples\\nlen(train)\\n\\n# Visualizing the data (OpenCV doesn\\'t display well in Jupyter notebooks)\\nplt.figure(figsize=(30,30))\\nplt.imshow(train[0][0], cmap=\\'gray\\')\\nplt.show()\\n\\n# Separating the array and corresponding labels\\nfeatureSet, labels = caer.sep_train(train, IMG_SIZE=IMG_SIZE)\\n\\n\\n# Normalize the featureSet ==> (0,1)\\nfeatureSet = caer.normalize(featureSet)\\n# Converting numerical labels to binary class vectors\\nlabels = to_categorical(labels, len(characters))\\n\\n\\n# Creating train and validation data\\nx_train, x_val, y_train, y_val = caer.train_val_split(featureSet, labels, val_ratio=.2)\\n\\n# Deleting variables to save memory\\ndel train\\ndel featureSet\\ndel labels \\ngc.collect()\\n\\n# Useful variables when training\\nBATCH_SIZE = 32\\nEPOCHS = 10\\n\\n# Image data generator (introduces randomness in network ==> better accuracy)\\ndatagen = canaro.generators.imageDataGenerator()\\ntrain_gen = datagen.flow(x_train, y_train, batch_size=BATCH_SIZE)\\n\\n# Create our model (returns the compiled model)\\noutput_dim=10\\n\\nw, h = IMG_SIZE[:2]\\n\\nmodel = Sequential()\\nmodel.add(Conv2D(32, (3, 3), activation=\\'relu\\', padding=\\'same\\', input_shape=(w, h,channels)))\\nmodel.add(Conv2D(32, (3, 3), activation=\\'relu\\'))\\nmodel.add(MaxPooling2D(pool_size=(2, 2)))\\nmodel.add(Dropout(0.2))\\n\\nmodel.add(Conv2D(64, (3, 3), padding=\\'same\\', activation=\\'relu\\'))\\nmodel.add(Conv2D(64, (3, 3), activation=\\'relu\\'))\\nmodel.add(MaxPooling2D(pool_size=(2, 2)))\\nmodel.add(Dropout(0.2))\\n\\nmodel.add(Conv2D(256, (3, 3), padding=\\'same\\', activation=\\'relu\\')) \\nmodel.add(Conv2D(256, (3, 3), activation=\\'relu\\'))\\nmodel.add(MaxPooling2D(pool_size=(2, 2)))\\nmodel.add(Dropout(0.2))\\n\\nmodel.add(Flatten())\\nmodel.add(Dropout(0.5))\\nmodel.add(Dense(1024, activation=\\'relu\\'))\\n\\n# Output Layer\\nmodel.add(Dense(output_dim, activation=\\'softmax\\'))\\n\\nmodel.summary()\\n\\n# Training the model\\noptimizer = SGD(learning_rate=0.001, decay=1e-7, momentum=0.9, nesterov=True)\\nmodel.compile(loss=\\'binary_crossentropy\\', optimizer=optimizer, metrics=[\\'accuracy\\'])\\ncallbacks_list = [LearningRateScheduler(canaro.lr_schedule)]\\ntraining = model.fit(train_gen,\\n                    steps_per_epoch=len(x_train)//BATCH_SIZE,\\n                    epochs=EPOCHS,\\n                    validation_data=(x_val,y_val),\\n                    validation_steps=len(y_val)//BATCH_SIZE,\\n                    callbacks = callbacks_list)\\n\\nprint(characters)\\n\\n\\n\"\"\"## Testing\"\"\"\\n\\ntest_path = r\\'SECTION_4_deep_CV\\\\archive\\\\kaggle_simpson_testset/charles_montgomery_burns_0.jpg\\'\\n\\nimg = cv.imread(test_path)\\n\\nplt.imshow(img)\\nplt.show()\\n\\ndef prepare(image):\\n    image = cv.cvtColor(image, cv.COLOR_BGR2GRAY)\\n    image = cv.resize(image, IMG_SIZE)\\n    image = caer.reshape(image, IMG_SIZE, 1)\\n    return image\\n\\npredictions = model.predict(prepare(img))\\n\\n# Getting class with the highest probability\\nprint(characters[np.argmax(predictions[0])])'),\n",
       " Document(metadata={'path': 'lecture/cropping_tool.py', 'sha': '3a4116afec7202172cf195800a91abe4d2802d86', 'source': 'https://api.github.com/shafqatameen/opencv/blob/main/lecture/cropping_tool.py'}, page_content='import cv2\\nimport numpy as np\\n\\n# Global variables\\ncropping = False  # Flag to indicate cropping state\\nstart_x, start_y, end_x, end_y = -1, -1, -1, -1\\nimage = cv2.imread(\"img/fruits.jpg\")  # Load your image here\\nclone = image.copy()  # Keep a copy of the original image\\n\\n# Mouse callback function\\ndef crop_rectangle(event, x, y, flags, param):\\n    global start_x, start_y, end_x, end_y, cropping, image\\n    \\n    if event == cv2.EVENT_LBUTTONDOWN:  # Mouse button pressed\\n        start_x, start_y = x, y\\n        cropping = True\\n\\n    elif event == cv2.EVENT_MOUSEMOVE:  # Mouse movement\\n        if cropping:\\n            temp_image = clone.copy()  # Use a copy to avoid overwriting\\n            cv2.rectangle(temp_image, (start_x, start_y), (x, y), (0, 255, 0), 2)\\n            cv2.imshow(\"Image\", temp_image)\\n\\n    elif event == cv2.EVENT_LBUTTONUP:  # Mouse button released\\n        end_x, end_y = x, y\\n        cropping = False\\n        cv2.rectangle(image, (start_x, start_y), (end_x, end_y), (0, 255, 0), 2)\\n        cv2.imshow(\"Image\", image)\\n\\n        # Ensure coordinates are correct\\n        if start_x != end_x and start_y != end_y:\\n            cropped_image = clone[start_y:end_y, start_x:end_x]  # Extract ROI\\n            cv2.imshow(\"Cropped Image\", cropped_image)\\n            cv2.imwrite(\"cropped_output.jpg\", cropped_image)  # Save cropped image\\n\\n# Set up the OpenCV window\\ncv2.imshow(\"Image\", image)\\ncv2.setMouseCallback(\"Image\", crop_rectangle)\\n\\n# Keep the window open until \\'q\\' is pressed\\nwhile True:\\n    key = cv2.waitKey(1) & 0xFF\\n    if key == ord(\"q\"):\\n        break\\n\\ncv2.destroyAllWindows()\\n'),\n",
       " Document(metadata={'path': 'lecture/event.py', 'sha': '449315dde51a54bbaaa03f9e007be2936d2a20fd', 'source': 'https://api.github.com/shafqatameen/opencv/blob/main/lecture/event.py'}, page_content='import cv2\\nimport numpy as np\\n\\n# Initialize variables\\ndrawing = False  # True if mouse is pressed\\nstart_x, start_y = -1, -1  # Starting coordinates\\nimage = np.zeros((500, 500, 3), dtype=np.uint8)  # Black canvas\\n\\n# Mouse callback function\\ndef draw_rectangle(event, x, y, flags, param):\\n    global start_x, start_y, drawing, image\\n    \\n    if event == cv2.EVENT_LBUTTONDOWN:  # Mouse press event\\n        drawing = True\\n        start_x, start_y = x, y  # Store starting coordinates\\n    \\n    elif event == cv2.EVENT_MOUSEMOVE:  # Mouse movement event\\n        if drawing:  # Only draw if the left button is pressed\\n            temp_img = image.copy()  # Copy image to avoid overwriting\\n            cv2.rectangle(temp_img, (start_x, start_y), (x, y), (0, 255, 0), -1)\\n            cv2.imshow(\"Interactive Drawing\", temp_img)\\n    \\n    elif event == cv2.EVENT_LBUTTONUP:  # Mouse release event\\n        drawing = False\\n        cv2.rectangle(image, (start_x, start_y), (x, y), (0, 255, 0), -1)\\n        cv2.imshow(\"Interactive Drawing\", image)\\n\\n# Create OpenCV window and set callback\\ncv2.imshow(\"Interactive Drawing\", image)\\ncv2.setMouseCallback(\"Interactive Drawing\", draw_rectangle)\\n\\n# Keep window open until user presses \\'q\\'\\nwhile True:\\n    if cv2.waitKey(1) & 0xFF == ord(\\'q\\'):\\n        break\\n\\ncv2.destroyAllWindows()\\n\\n\\n\\'\\'\\'import cv2\\nimport numpy as np\\n\\nimg=np.zeros( (500,500,3) )\\n\\nflag=False\\nix=-1\\niy=-1\\ndef draw(event,x,y,flags,params):\\n    global ix,iy,flag\\n    if event==1:\\n        flag=True\\n        ix=x\\n        iy=y\\n    elif event==0:\\n        if flag==True:\\n            cv2.rectangle(img,(ix,iy),(x,y),(255,0,0),-1)\\n        \\n\\n    elif event==4:\\n            flag=False\\n            cv2.rectangle(img,(ix,iy),(x,y),(0,255,0),-1)\\n            \\n\\ncv2.namedWindow(winname=\"window\")\\ncv2.setMouseCallback(\\'window\\',draw)\\n\\nwhile True:\\n    cv2.imshow(\\'window\\',img)\\n    if (cv2.waitKey(1) & 0xFF)==ord(\\'x\\'):\\n        break\\n\\ncv2.destroyAllWindows()\\'\\'\\''),\n",
       " Document(metadata={'path': 'lecture/interractWithImage.py', 'sha': '2ceae0c2894c468c0cd01201a3f44a22c012d745', 'source': 'https://api.github.com/shafqatameen/opencv/blob/main/lecture/interractWithImage.py'}, page_content='import cv2\\nimport numpy as np\\n\\nimg=cv2.imread(\\'img/fruits.jpg\\')\\n#img=np.zeros((500,500,3))\\n\\ndef draw(event,x,y,flags,params):\\n    if event==1:\\n        cv2.circle(img,center=(x,y),radius=10,color=(255,0,0),thickness=-1)\\n\\'\\'\\'    print(event)\\n    if event==0:\\n        print(\\'mouse moved\\')\\n    if event==1:\\n        print(\\'left button clicked\\')\\n    if event==2:\\n        print(\\'right button clicked\\')\\n    if event==3:\\n        print(\\'middle button clicked\\')\\n    if event==4:\\n        print(\\'left button clicked released\\')\\n    if event==5:\\n        print(\\'right button clicked released\\')\\n    if event==6:\\n        print(\\'middle button clicked released\\')\\n    \\'\\'\\'\\n\\n\\ncv2.namedWindow(winname=\"window\")\\ncv2.setMouseCallback(\\'window\\',draw)\\n\\nwhile True:\\n    cv2.imshow(\\'window\\',img)\\n    if (cv2.waitKey(1) & 0xFF)==ord(\\'x\\'):\\n        break\\n\\ncv2.destroyAllWindows()\\n\\n\\n\\n\\n'),\n",
       " Document(metadata={'path': 'lecture/readimg.py', 'sha': '025e32241f0e1a61a1d53e87a37a4fa83ebdfaed', 'source': 'https://api.github.com/shafqatameen/opencv/blob/main/lecture/readimg.py'}, page_content=\"import cv2\\n\\nimg=cv2.imread('img/fruits.jpg')\\nprint(img.shape)\\ncv2.imshow('original image called color image',img)\\n##cv2.waitKey(4)\\n\\nimg_gray=cv2.cvtColor(img,cv2.COLOR_BGR2GRAY)\\nprint(img_gray.shape)\\ncv2.imshow('gray image called grayscale image',img_gray)\\n#cv2.waitKey(10)\\n\\ncv2.imshow('image without blue color',img[:,:,0])\\n#cv2.waitKey(100000)\\n\\ncv2.imshow('image without green color',img[:,:,1])\\n#cv2.waitKey(100000)\\n\\ncv2.imshow('image without red color',img[:,:,2])\\n#cv2.waitKey(100000)\\n\\n\\n\"),\n",
       " Document(metadata={'path': 'lecture/removeOneChannel.py', 'sha': '69a3e18cb281d489c0033961e4f642d8121e16ab', 'source': 'https://api.github.com/shafqatameen/opencv/blob/main/lecture/removeOneChannel.py'}, page_content=\"import cv2\\n#load iimage\\nimg=cv2.imread('img/fruits.jpg')\\nprint(img.shape)\\n\\n'''#show image\\ncv2.imshow('COLOR IMAGE',img)\\ncv2.waitKey(0)'''\\n\\n#remove the channel\\nimg[:,:,0]=0                 #remove blue channel\\ncv2.imshow('IMAGE WITHOUT BLUE CHANNEL',img)\\ncv2.waitKey(0)\\n\\nimg[:,:,1]=0\\ncv2.imshow('IMAGE WITHOUT GREEN CHANNEL',img)\\ncv2.waitKey(0)\\n\\nimg[:,:,2]=0\\ncv2.imshow('IMAGE WITHOUT RED CHANNEL',img)\\ncv2.waitKey(0)\\n\\n\"),\n",
       " Document(metadata={'path': 'lecture/removeonechannel2.py', 'sha': '0c547bcad79176dd996e49dfb793dda5d7aa88c6', 'source': 'https://api.github.com/shafqatameen/opencv/blob/main/lecture/removeonechannel2.py'}, page_content='import cv2\\nimport numpy as np\\n\\n# Load the image\\nimg = cv2.imread(\\'img/fruits.jpg\\')\\n\\n# Check if the image is loaded correctly\\nif img is None:\\n    print(\"Error: Image not found or path is incorrect.\")\\n    exit()\\n\\n# Resize the image to a manageable size (e.g., 600x400)\\nscale_percent = 50  # Reduce size by 50%\\nwidth = int(img.shape[1] * scale_percent / 100)\\nheight = int(img.shape[0] * scale_percent / 100)\\ndim = (width, height)\\nimg = cv2.resize(img, dim, interpolation=cv2.INTER_AREA)\\n\\n# Remove blue channel\\nimg_without_blue = img.copy()\\nimg_without_blue[:, :, 0] = 0\\n\\n# Remove green channel\\nimg_without_green = img.copy()\\nimg_without_green[:, :, 1] = 0\\n\\n# Remove red channel\\nimg_without_red = img.copy()\\nimg_without_red[:, :, 2] = 0\\n\\n# Stack images horizontally\\nnew_img = np.hstack((img_without_blue, img_without_green, img_without_red))\\n\\n# Show the final stacked image\\ncv2.imshow(\"Single Channel Removed\", new_img)\\ncv2.waitKey(0)\\ncv2.destroyAllWindows()\\n\\n\\n\\n\\n\\n\\'\\'\\'import cv2\\nimport numpy as np\\n\\n\\n# Load the image\\nimg = cv2.imread(\\'img/fruits.jpg\\')\\n\\n# Remove blue channel\\nimg_without_blue = img.copy()\\nimg_without_blue[:, :, 0] = 0\\ncv2.imshow(\\'IMAGE WITHOUT BLUE CHANNEL\\', img_without_blue)\\ncv2.waitKey(0)\\ncv2.destroyAllWindows()\\n\\n# Remove green channel\\nimg_without_green = img.copy()\\nimg_without_green[:, :, 1] = 0\\ncv2.imshow(\\'IMAGE WITHOUT GREEN CHANNEL\\', img_without_green)\\ncv2.waitKey(0)\\ncv2.destroyAllWindows()\\n\\n# Remove red channel\\nimg_without_red = img.copy()\\nimg_without_red[:, :, 2] = 0\\ncv2.imshow(\\'IMAGE WITHOUT RED CHANNEL\\', img_without_red)\\ncv2.waitKey(0)\\ncv2.destroyAllWindows()\\n\\nnew_img=np.hstack((img_without_blue,img_without_green,img_without_red))\\ncv2.imshow(\"single channel removed\",new_img)\\ncv2.waitKey(0)\\'\\'\\''),\n",
       " Document(metadata={'path': 'lecture/video.py', 'sha': '81cd1d5b2e230969760d53b84b6fd8132ee2bab2', 'source': 'https://api.github.com/shafqatameen/opencv/blob/main/lecture/video.py'}, page_content='import cv2\\n\\n# Open the video file (change \\'video.mp4\\' to 0 for webcam)\\nvideo_path = \"output_video.mp4\"  # Change this to your video file or use 0 for a webcam\\ncap = cv2.VideoCapture(video_path)\\n\\n# Check if the video file opened correctly\\nif not cap.isOpened():\\n    print(\"Error: Could not open video.\")\\n    exit()\\n\\n# Get video properties\\nframe_width = int(cap.get(3))  # Width of frames\\nframe_height = int(cap.get(4))  # Height of frames\\nfps = int(cap.get(cv2.CAP_PROP_FPS))  # Frames per second\\n\\n# Define the codec and create VideoWriter object\\noutput_path = \"output_video.mp4\"\\nfourcc = cv2.VideoWriter_fourcc(*\\'mp4v\\')  # Codec for MP4 format\\nout = cv2.VideoWriter(output_path, fourcc, fps, (frame_width, frame_height), isColor=False)\\n\\nwhile True:\\n    ret, frame = cap.read()  # Read a frame\\n    if not ret:\\n        break  # Exit loop if no frame is returned\\n\\n    # Convert frame to grayscale\\n    gray_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\\n\\n    # Show the frame\\n    cv2.imshow(\"Grayscale Video\", gray_frame)\\n\\n    # Write the processed frame to output file\\n    out.write(gray_frame)\\n\\n    # Handle playback controls\\n    key = cv2.waitKey(250) & 0xFF\\n    if key == ord(\\'q\\'):  # Press \\'q\\' to quit\\n        break\\n    elif key == ord(\\'p\\'):  # Press \\'p\\' to pause\\n        cv2.waitKey(-1)  # Wait until any key is pressed to resume\\n\\n# Release everything\\ncap.release()\\nout.release()\\ncv2.destroyAllWindows()\\n'),\n",
       " Document(metadata={'path': 'lecture/video2.py', 'sha': '146b57833e3f37a8395a6ed6a3aad3f294112ebd', 'source': 'https://api.github.com/shafqatameen/opencv/blob/main/lecture/video2.py'}, page_content='import cv2\\n\\n# Open webcam (0 for default camera)\\ncap = cv2.VideoCapture(0)\\n\\n# Get video properties\\nframe_width = int(cap.get(3))  # Width\\nframe_height = int(cap.get(4))  # Height\\nfps = 30  # Frame rate (adjust as needed)\\n\\n# Define codec and create VideoWriter object\\noutput_path = \"captured_video.mp4\"\\nfourcc = cv2.VideoWriter_fourcc(*\\'mp4v\\')  # Codec for MP4 format\\nout = cv2.VideoWriter(output_path, fourcc, fps, (frame_width, frame_height))\\n\\n# Check if webcam is opened\\nif not cap.isOpened():\\n    print(\"Error: Could not open webcam.\")\\n    exit()\\n\\nprint(\"Press \\'q\\' to stop recording...\")\\n\\nwhile True:\\n    ret, frame = cap.read()  # Read a frame from the webcam\\n    if not ret:\\n        break  # Exit loop if no frame is captured\\n\\n    # Display the video feed\\n    cv2.imshow(\"Webcam Live Feed\", frame)\\n\\n    # Save the frame to the output file\\n    out.write(frame)\\n\\n    # Press \\'q\\' to stop recording\\n    if cv2.waitKey(1) & 0xFF == ord(\\'q\\'):\\n        break\\n\\n# Release the camera and close all windows\\ncap.release()\\nout.release()\\ncv2.destroyAllWindows()\\nprint(\"Video saved as \\'captured_video.mp4\\'\")\\n')]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#load the python files from the github repo\n",
    "#from langchain_community.document_laoders import GithubFileLoader\n",
    "loader=GithubFileLoader(\n",
    "    repo=\"shafqatameen/opencv\",\n",
    "    file_filter= lambda file_path:file_path.endswith(\".py\")\n",
    ")\n",
    "python_doc=loader.load()\n",
    "python_doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5b1c791",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
